{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings\n",
    "\n",
    "\n",
    "Word embedding is any of a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with many dimensions per word to a continuous vector space with a much lower dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec \n",
    "\n",
    "\n",
    "### Statistical Language Model\n",
    "A statistical language model is a probability distribution over sequences of words. Given such a sequence, say of length m, it assigns a probability. to the whole sequence. The language model provides context to distinguish between words and phrases that sound similar.\n",
    "\n",
    "\n",
    "### CBOW\n",
    "Given context, setup a neural net to predict next word\n",
    "\n",
    "### SkipGram \n",
    "Given a word in the text sequence, setup a neural net to predict the sequence.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/word2vec.jpg\"> \n",
    "\n",
    "\n",
    "Context size is fixed (hyperparameter) and the input to the neural net is the k words of the context in 1-hot representation.\n",
    "\n",
    "So if we have 1000 words in the vocab and context 4, the input in CBOW will be 4 stacked 1000 dimensional 1-hot vectors (one vector for each word in the context) and the target vector a 1000 dimensional target vector of the probabilities of the next word.\n",
    "\n",
    "The hidden layer would be of dimensionality D (again hyperparameter). The byproduct of the training then are two matrices, the input-hidden layer weights and the hidden-output layer weights.\n",
    "\n",
    "Both matrices have dimensions $Dx|V|$, where D the hidden layer dimension and $|V|$ the size of the vocabulary.\n",
    "\n",
    "\n",
    "Similarily, the configuration of the skip-gram architecture is similar:\n",
    "\n",
    "Input vector 1-hot encoding of the input word, output vectors k softmax vectors of dimension $|V|$. \n",
    "\n",
    "\n",
    "\n",
    "## word2vec breakthrough\n",
    "\n",
    "- Very fast to train \n",
    "    - (async sgd)\n",
    "    - (negative sampling)\n",
    "- trained on larger corpora \n",
    "    - better embedding quality \n",
    "\n",
    "\n",
    "\n",
    "## fastText \n",
    "\n",
    "Fasttext is essentially very similar to word2vec. The main differentiation is that it produces embeddings on subword level (ngrams) and then combines the word representation as the sum of its components.\n",
    "\n",
    "Because of this property it is really useful in datasets with typos/misplaced or missing characters, as it will produce very similar embeddings. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Problems with embeddings\n",
    "\n",
    "- No contextual semantic sensitivity : bank vs bank \n",
    "- inherited bias from existing corpora (so we need to be very careful)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus(object):\n",
    "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
    "    \n",
    "    def __init__(self, lines):\n",
    "        self.lines = lines\n",
    "            \n",
    "    def __iter__(self):\n",
    "        for line in self.lines:\n",
    "            \n",
    "            text = line.lower()\n",
    "            \n",
    "            yield gensim.utils.simple_preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open(\"./data/amazon_reviews.txt\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bought', 'this', 'album', 'because', 'loved', 'the', 'title', 'song', 'it', 'such', 'great', 'song', 'how', 'bad', 'can', 'the', 'rest', 'of', 'the', 'album', 'be', 'right', 'well', 'the', 'rest', 'of', 'the', 'songs', 'are', 'just', 'filler', 'and', 'are', 'worth', 'the', 'money', 'paid', 'for', 'this', 'it', 'either', 'shameless', 'bubblegum', 'or', 'depressing', 'tripe', 'kenny', 'chesney', 'is', 'popular', 'artist', 'and', 'as', 'result', 'he', 'is', 'in', 'the', 'cookie', 'cutter', 'category', 'of', 'the', 'nashville', 'music', 'scene', 'he', 'gotta', 'pump', 'out', 'the', 'albums', 'so', 'the', 'record', 'company', 'can', 'keep', 'lining', 'their', 'pockets', 'while', 'the', 'suckers', 'out', 'there', 'keep', 'buying', 'this', 'garbage', 'to', 'perpetuate', 'more', 'garbage', 'coming', 'out', 'of', 'that', 'town', 'll', 'get', 'down', 'off', 'my', 'soapbox', 'now', 'but', 'country', 'music', 'really', 'needs', 'to', 'get', 'back', 'to', 'it', 'roots', 'and', 'stop', 'this', 'pop', 'nonsense', 'what', 'country', 'music', 'really', 'is', 'and', 'what', 'it', 'is', 'considered', 'to', 'be', 'by', 'mainstream', 'are', 'two', 'different', 'things']\n"
     ]
    }
   ],
   "source": [
    "sentences = MyCorpus(data)\n",
    "for s in sentences:\n",
    "    print(s)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-28 13:44:38,798 : INFO : collecting all words and their counts\n",
      "2021-05-28 13:44:38,799 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-05-28 13:44:41,004 : INFO : PROGRESS: at sentence #10000, processed 1242775 words, keeping 41603 word types\n",
      "2021-05-28 13:44:41,419 : INFO : collected 45128 word types from a corpus of 1480752 raw words and 11914 sentences\n",
      "2021-05-28 13:44:41,420 : INFO : Loading a fresh vocabulary\n",
      "2021-05-28 13:44:41,495 : INFO : effective_min_count=5 retains 13926 unique words (30% of original 45128, drops 31202)\n",
      "2021-05-28 13:44:41,496 : INFO : effective_min_count=5 leaves 1429573 word corpus (96% of original 1480752, drops 51179)\n",
      "2021-05-28 13:44:41,549 : INFO : deleting the raw counts dictionary of 45128 items\n",
      "2021-05-28 13:44:41,551 : INFO : sample=0.001 downsamples 50 most-common words\n",
      "2021-05-28 13:44:41,552 : INFO : downsampling leaves estimated 1091047 word corpus (76.3% of prior 1429573)\n",
      "2021-05-28 13:44:41,586 : INFO : estimated required memory for 13926 words and 200 dimensions: 29244600 bytes\n",
      "2021-05-28 13:44:41,587 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec( min_count=5, workers=5, size=200) \n",
    "model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-28 13:44:45,067 : INFO : training model with 5 workers on 13926 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2021-05-28 13:44:46,087 : INFO : EPOCH 1 - PROGRESS: at 29.82% examples, 322884 words/s, in_qsize 0, out_qsize 0\n",
      "2021-05-28 13:44:47,132 : INFO : EPOCH 1 - PROGRESS: at 57.23% examples, 299487 words/s, in_qsize 9, out_qsize 0\n",
      "2021-05-28 13:44:48,167 : INFO : EPOCH 1 - PROGRESS: at 90.01% examples, 317020 words/s, in_qsize 9, out_qsize 0\n",
      "2021-05-28 13:44:48,259 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-05-28 13:44:48,262 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-05-28 13:44:48,266 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-28 13:44:48,269 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-28 13:44:48,274 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-28 13:44:48,274 : INFO : EPOCH - 1 : training on 1480752 raw words (1091414 effective words) took 3.2s, 341316 effective words/s\n",
      "2021-05-28 13:44:49,332 : INFO : EPOCH 2 - PROGRESS: at 24.78% examples, 263891 words/s, in_qsize 9, out_qsize 0\n",
      "2021-05-28 13:44:50,368 : INFO : EPOCH 2 - PROGRESS: at 59.09% examples, 306515 words/s, in_qsize 9, out_qsize 0\n",
      "2021-05-28 13:44:51,412 : INFO : EPOCH 2 - PROGRESS: at 91.95% examples, 320682 words/s, in_qsize 9, out_qsize 0\n",
      "2021-05-28 13:44:51,467 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-05-28 13:44:51,468 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-05-28 13:44:51,475 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-28 13:44:51,476 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-28 13:44:51,485 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-28 13:44:51,485 : INFO : EPOCH - 2 : training on 1480752 raw words (1091119 effective words) took 3.2s, 341486 effective words/s\n",
      "2021-05-28 13:44:52,513 : INFO : EPOCH 3 - PROGRESS: at 29.82% examples, 320449 words/s, in_qsize 0, out_qsize 0\n",
      "2021-05-28 13:44:53,541 : INFO : EPOCH 3 - PROGRESS: at 56.55% examples, 297495 words/s, in_qsize 9, out_qsize 0\n",
      "2021-05-28 13:44:54,552 : INFO : EPOCH 3 - PROGRESS: at 87.93% examples, 313232 words/s, in_qsize 8, out_qsize 1\n",
      "2021-05-28 13:44:54,692 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-05-28 13:44:54,695 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-05-28 13:44:54,701 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-28 13:44:54,703 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-28 13:44:54,709 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-28 13:44:54,709 : INFO : EPOCH - 3 : training on 1480752 raw words (1090484 effective words) took 3.2s, 339371 effective words/s\n",
      "2021-05-28 13:44:55,721 : INFO : EPOCH 4 - PROGRESS: at 24.15% examples, 266881 words/s, in_qsize 9, out_qsize 0\n",
      "2021-05-28 13:44:56,755 : INFO : EPOCH 4 - PROGRESS: at 57.23% examples, 301934 words/s, in_qsize 9, out_qsize 0\n",
      "2021-05-28 13:44:57,781 : INFO : EPOCH 4 - PROGRESS: at 86.48% examples, 307790 words/s, in_qsize 9, out_qsize 1\n",
      "2021-05-28 13:44:57,962 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-05-28 13:44:57,965 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-05-28 13:44:57,970 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-28 13:44:57,974 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-28 13:44:57,978 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-28 13:44:57,979 : INFO : EPOCH - 4 : training on 1480752 raw words (1090427 effective words) took 3.3s, 334455 effective words/s\n",
      "2021-05-28 13:44:58,993 : INFO : EPOCH 5 - PROGRESS: at 24.15% examples, 266975 words/s, in_qsize 9, out_qsize 0\n",
      "2021-05-28 13:45:00,000 : INFO : EPOCH 5 - PROGRESS: at 59.09% examples, 317210 words/s, in_qsize 7, out_qsize 0\n",
      "2021-05-28 13:45:01,063 : INFO : EPOCH 5 - PROGRESS: at 89.26% examples, 316446 words/s, in_qsize 9, out_qsize 0\n",
      "2021-05-28 13:45:01,203 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-05-28 13:45:01,205 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-05-28 13:45:01,208 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-28 13:45:01,215 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-28 13:45:01,219 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-28 13:45:01,219 : INFO : EPOCH - 5 : training on 1480752 raw words (1091199 effective words) took 3.2s, 337857 effective words/s\n",
      "2021-05-28 13:45:01,220 : INFO : training on a 7403760 raw words (5454643 effective words) took 16.2s, 337721 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5454643, 7403760)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sentences = MyCorpus(data)\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13926"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = model.wv[\"awful\"]\n",
    "v2 = model.wv[\"ridiculous\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8366808"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1.dot(v2) / np.sqrt(v1.dot(v1) * v2.dot(v2))\n",
    "#[('ridiculous', 0.8427592515945435)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"this is an awful product\"\n",
    "\n",
    "ret = []\n",
    "for word in sentence.split():\n",
    "    v = model.wv[word]\n",
    "    ret.append(v)\n",
    "    \n",
    "ret = np.array(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = np.median( ret, axis = 0)#.shape\n",
    "r2 = np.mean( ret, axis = 0)#.shape\n",
    "r3 = np.max( ret, axis = 0)#.shape\n",
    "r4 = np.min( ret, axis = 0)#.shape\n",
    "\n",
    "r1.reshape(-1,1).shape\n",
    "\n",
    "final = np.concatenate( (r1.reshape(-1,1), r2.reshape(-1,1), r3.reshape(-1,1), r4.reshape(-1,1)))\n",
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-28 13:45:23,344 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ridiculous', 0.8366807699203491),\n",
       " ('terrible', 0.8213960528373718),\n",
       " ('okay', 0.8132612705230713),\n",
       " ('scary', 0.7956511974334717),\n",
       " ('kinda', 0.794615626335144),\n",
       " ('sad', 0.7933586239814758),\n",
       " ('joke', 0.7898640632629395),\n",
       " ('dissappointing', 0.7880542278289795),\n",
       " ('weird', 0.7867347598075867),\n",
       " ('subpar', 0.7781393527984619)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lens', 0.8993487358093262),\n",
       " ('bag', 0.8668299317359924),\n",
       " ('camcorder', 0.8383852243423462),\n",
       " ('unit', 0.8286342620849609),\n",
       " ('battery', 0.8257143497467041),\n",
       " ('case', 0.8062664270401001),\n",
       " ('canon', 0.8035275936126709),\n",
       " ('tripod', 0.7975265979766846),\n",
       " ('charger', 0.7946109175682068),\n",
       " ('scale', 0.7940206527709961)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"camera\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('novel', 0.7867717742919922),\n",
       " ('author', 0.7369700074195862),\n",
       " ('story', 0.7038761377334595),\n",
       " ('books', 0.6955527663230896),\n",
       " ('movie', 0.6905809640884399),\n",
       " ('read', 0.6675878167152405),\n",
       " ('writing', 0.6507339477539062),\n",
       " ('review', 0.6300108432769775),\n",
       " ('film', 0.626803994178772),\n",
       " ('language', 0.5969916582107544)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 0.9149080514907837),\n",
       " ('story', 0.8254029154777527),\n",
       " ('novel', 0.7964406609535217),\n",
       " ('show', 0.7392091751098633),\n",
       " ('ending', 0.7327175140380859),\n",
       " ('album', 0.7141045928001404),\n",
       " ('acting', 0.6943376064300537),\n",
       " ('book', 0.6905809640884399),\n",
       " ('stuff', 0.6872981190681458),\n",
       " ('song', 0.6854572296142578)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"movie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=10)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = KMeans(10)\n",
    "\n",
    "kmeans.fit(model.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(model.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quiet upfront capture determine meeting\n",
      "\n",
      "\n",
      "chesney popular in while ll\n",
      "\n",
      "\n",
      "album money result category gotta\n",
      "\n",
      "\n",
      "bought such how bad shameless\n",
      "\n",
      "\n",
      "loved great rest be are\n",
      "\n",
      "\n",
      "kenny keep town my brilliant\n",
      "\n",
      "\n",
      "title it of just filler\n",
      "\n",
      "\n",
      "out mainstream cd many they\n",
      "\n",
      "\n",
      "can well either artist as\n",
      "\n",
      "\n",
      "this because the song right\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    ret = np.where( labels==i)[0]\n",
    "    \n",
    "    words = [vocab[ret[j]] for j in range( min(5, len(ret)) )]\n",
    "    print( \" \".join(words))\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
