{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings and sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-03 11:05:02--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘./data/dataset.tar.gz’\n",
      "\n",
      "./data/dataset.tar. 100%[===================>]  80.23M   744KB/s    in 1m 52s  \n",
      "\n",
      "2020-12-03 11:06:55 (736 KB/s) - ‘./data/dataset.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz -O ./data/dataset.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "from gensim.utils import tokenize, deaccent, simple_preprocess\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus(object):\n",
    "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
    "    \n",
    "    def __init__(self, files):\n",
    "        self.files = files\n",
    "            \n",
    "    def __iter__(self):\n",
    "        for file in self.files:\n",
    "            \n",
    "            text = open( file ).read().lower()\n",
    "            \n",
    "            yield simple_preprocess(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12500, 12500)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_train = glob.glob(\"./data/aclImdb/train/pos/*.txt\")\n",
    "negative_train = glob.glob(\"./data/aclImdb/train/neg/*.txt\")\n",
    "#negative_train[0:5], positive_train[0:5]\n",
    "\n",
    "positive_test = glob.glob(\"./data/aclImdb/test/pos/*.txt\")\n",
    "negative_test = glob.glob(\"./data/aclImdb/test/neg/*.txt\")\n",
    "len(negative_test), len(positive_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for', 'movie', 'that', 'gets', 'no', 'respect', 'there', 'sure', 'are', 'lot', 'of', 'memorable', 'quotes', 'listed', 'for', 'this', 'gem', 'imagine', 'movie', 'where', 'joe', 'piscopo', 'is', 'actually', 'funny', 'maureen', 'stapleton', 'is', 'scene', 'stealer', 'the', 'moroni', 'character', 'is', 'an', 'absolute', 'scream', 'watch', 'for', 'alan', 'the', 'skipper', 'hale', 'jr', 'as', 'police', 'sgt']\n"
     ]
    }
   ],
   "source": [
    "sentences = MyCorpus(positive_train + negative_train)\n",
    "for s in sentences:\n",
    "    print(s)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec( min_count=5, workers=5, size=200) \n",
    "model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_vector( text, model, D,  nwords=1000 ):\n",
    "    words = simple_preprocess(open(text).read())[0:nwords]\n",
    "    \n",
    "    c = 0 \n",
    "    v = np.zeros(D)\n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            c +=1 \n",
    "            v+= model.wv[word]\n",
    "        \n",
    "      \n",
    "    return v/c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 200\n",
    "nwords = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos = np.zeros( (len(positive_train), D))\n",
    "y_pos = np.ones( len(positive_train) )\n",
    "\n",
    "for idx,f in enumerate(positive_train):\n",
    "    X_pos[idx,:] = file_to_vector(f, model, D=D, nwords=nwords )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "X_neg = np.zeros( (len(negative_train), D))\n",
    "y_neg = np.zeros( len(negative_train) )\n",
    "\n",
    "for idx,f in enumerate(negative_train):\n",
    "    X_neg[idx,:] = file_to_vector(f, model, D=D, nwords=nwords )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "X = np.concatenate( (X_pos, X_neg) , axis=0)\n",
    "y= np.concatenate(  (y_pos, y_neg) , axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessing.scale(X, axis=0) # zero mean, unit variance for each vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18750, 200), (6250, 200))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train , y_test = train_test_split( X, y, random_state =42)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1500, random_state=42)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter = 1500, random_state = 42,fit_intercept=True) \n",
    "clf.fit( X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7232"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = clf.predict(X_test)\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNClassifier(nn.Module):\n",
    "    def __init__(self, D, n_hidden):\n",
    "        super(NNClassifier, self).__init__()\n",
    "        \n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout()\n",
    "        self.fc1 = nn.Linear( D, n_hidden )\n",
    "        self.fc2 = nn.Linear( n_hidden, 1)\n",
    "        \n",
    "    \n",
    "    def forward(self, X):\n",
    "        batch_size = X.shape[0]\n",
    "        out = self.fc1(X)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.Size([50, 200]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "net = NNClassifier(D=200, n_hidden = 100)\n",
    "\n",
    "x_in = torch.rand( size=(50, 200))\n",
    "x_in.dtype, x_in.shape\n",
    "#net(x_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx= { w:i for i, w in enumerate(model.wv.vocab.keys()) }\n",
    "idx2word = {i:w for w,i in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/aclImdb/train/pos/4715_9.txt'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 4.49688989e-04, -4.01483645e-04, -2.59369175e-04, -4.35122609e-04,\n",
       "         3.08975403e-04,  5.63730173e-05,  2.92383687e-04, -2.82398891e-04,\n",
       "         4.41822747e-04, -6.36014374e-06,  6.39296559e-05, -3.48708818e-05,\n",
       "        -1.64607816e-04,  3.76173120e-05, -1.02190672e-04,  4.68529470e-04,\n",
       "        -4.02406877e-04,  7.83308715e-05, -1.93617045e-04, -2.47308082e-04,\n",
       "        -2.07903184e-04, -6.85326086e-05, -3.91543435e-04, -2.35021886e-04,\n",
       "        -6.11755240e-05, -3.52100789e-04,  1.34786669e-05,  1.35929600e-04,\n",
       "        -3.67138360e-04, -2.85615912e-04, -4.19135700e-04, -3.44096552e-05,\n",
       "         1.75669207e-04,  2.70006276e-04,  3.66209897e-05, -1.51392465e-04,\n",
       "        -3.03276582e-04, -3.85350577e-05, -3.07914743e-04, -4.32111963e-04,\n",
       "        -2.55237159e-04,  6.79154255e-05, -1.91538493e-04,  2.19883204e-05,\n",
       "         5.11856197e-05,  2.13449719e-04, -2.10035534e-04, -2.43975839e-04,\n",
       "         2.46141804e-04,  3.90571979e-04,  3.24069610e-04, -3.62127117e-04,\n",
       "         3.88056214e-05,  7.98021138e-05,  1.19471813e-04,  1.47851970e-04,\n",
       "        -9.76971351e-05, -1.76695452e-04,  1.55136440e-04,  1.59454983e-04,\n",
       "         1.94074484e-04, -4.14646027e-04,  2.41567992e-04, -2.69809301e-04,\n",
       "        -3.49824724e-04, -2.20954098e-04,  4.95958491e-04,  3.17906146e-04,\n",
       "         2.57977284e-04,  1.59901057e-04, -2.01993011e-04,  5.90859599e-05,\n",
       "        -1.31330680e-05,  2.39700807e-04,  4.55296162e-04,  8.51128498e-05,\n",
       "        -1.95529632e-04,  1.97290690e-04, -2.54390907e-05, -1.08757282e-04,\n",
       "        -5.55261911e-04,  8.62603920e-05,  4.92287334e-04, -8.97020436e-05,\n",
       "         4.49552928e-04,  2.99337727e-04,  1.58739305e-04,  1.47000072e-04,\n",
       "         1.54243840e-04,  2.79573025e-04,  7.64148062e-05,  1.02269796e-05,\n",
       "         7.80630558e-07,  4.85559285e-05, -1.94167631e-04, -2.54536553e-05,\n",
       "        -2.56320956e-04, -4.78470574e-05,  1.40171003e-04,  2.02503797e-04,\n",
       "         5.40957146e-04, -1.95397515e-04, -6.29602233e-04, -2.33191880e-04,\n",
       "        -4.78288654e-04, -8.24119779e-05, -5.74744081e-05,  5.91411072e-06,\n",
       "        -3.82602098e-04, -1.17872194e-04, -4.29152191e-04, -5.12844184e-04,\n",
       "        -3.03062610e-04,  5.34996565e-04, -2.08125464e-04, -1.31102293e-04,\n",
       "        -3.29000468e-04,  2.02420983e-04,  3.56586883e-04, -1.07955511e-05,\n",
       "        -1.52431428e-04,  2.41264745e-04,  2.09234291e-04, -1.90659237e-04,\n",
       "         8.42039735e-05,  9.49690366e-05,  1.07297246e-05,  2.52014695e-04,\n",
       "         2.23402909e-04,  1.14287432e-04,  2.15946002e-05,  6.29531569e-04,\n",
       "         1.17058713e-04,  1.01096688e-04,  1.51760862e-04,  3.16064456e-04,\n",
       "        -4.47705999e-04, -2.96312384e-04,  3.20317573e-04,  1.22256286e-04,\n",
       "         5.42521229e-06,  1.63567936e-04,  3.04403860e-04, -3.03786539e-04,\n",
       "        -2.92727840e-04, -3.50721297e-04, -5.18034096e-04, -3.47720692e-04,\n",
       "        -2.94579833e-04, -1.02768736e-05, -2.85190763e-04,  7.84086078e-05,\n",
       "         3.69066925e-04,  2.16958695e-04, -2.35470361e-04, -4.36655537e-05,\n",
       "         1.98292146e-05, -2.42214897e-04, -9.41016333e-05, -2.06634257e-04,\n",
       "         1.07899999e-04,  1.02000260e-04, -3.45950742e-04, -7.23136982e-05,\n",
       "        -3.25064728e-04, -9.56694130e-05,  3.12279299e-04,  1.81919706e-04,\n",
       "         1.54389578e-04,  2.78821943e-04, -3.40029190e-04, -5.16963592e-05,\n",
       "         3.00236221e-04,  5.83218680e-05,  4.75281529e-04,  4.71403735e-04,\n",
       "        -1.43693775e-04, -4.40166157e-04, -1.47384751e-04,  7.02271063e-04,\n",
       "        -5.27916069e-04, -1.90297986e-04,  3.39678169e-04, -1.67777995e-04,\n",
       "        -1.74018078e-05,  8.06542230e-05,  1.51109431e-04,  3.57034238e-04,\n",
       "         2.31395330e-04,  3.16342688e-04,  2.38667308e-05, -1.67171587e-04,\n",
       "        -4.68026446e-05, -5.93826962e-06,  5.98262763e-04,  4.55366651e-04,\n",
       "        -2.61545123e-04,  3.73657269e-04, -1.95923465e-04, -2.73764046e-04],\n",
       "       dtype=float32),\n",
       " tensor(1.))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"Imbdb dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, positives, negatives, word2idx, wv_model):\n",
    "\n",
    "        self.dataset = positives + negatives\n",
    "        self.word2idx = word2idx\n",
    "     \n",
    "        self.labels = [1 for _ in range(len(positives))] + [1 for _ in range(len(negatives))]\n",
    "        self.w2v = wv_model\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        text = open(self.dataset[idx]).read()\n",
    "        s = simple_preprocess(text)\n",
    "              \n",
    "        vec = [self.w2v.wv[v] for v in s if v in word2idx]\n",
    "        \n",
    "        vec = np.mean( vec, axis =0)\n",
    "        assert( vec.shape[0] == 200)\n",
    "        \n",
    "            \n",
    "        return vec, torch.tensor( self.labels[idx], dtype=torch.float32)\n",
    "            \n",
    "train_dataset = TextDataset(positive_train, negative_train, word2idx, wv_model=model)\n",
    "test_dataset = TextDataset(positive_test, negative_test, word2idx,  wv_model=model)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=0)\n",
    "\n",
    "test_dataloader = DataLoader(train_dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "dataloaders = { \"train\": train_dataloader, \"val\":test_dataloader}\n",
    "\n",
    "dataset_sizes = { \"train\": len(train_dataset), \"val\":len(test_dataset)}\n",
    "train_dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.Size([16, 200]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x_in = torch.rand( size=(16,200))\n",
    "\n",
    "x_in.dtype, x_in.shape#net(x_in).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NNClassifier(D=200, n_hidden = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam( net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, dataloaders, scheduler,  num_epochs=10):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                \n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.reshape(-1,1).to(device)\n",
    "                \n",
    "                print(\"inputs shape=>\", inputs.shape)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                if scheduler:\n",
    "                    scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(net, criterion, optimizer, dataloaders, scheduler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
