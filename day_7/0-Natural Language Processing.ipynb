{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "tl;dr: Make computers understand language\n",
    "\n",
    "\n",
    "- Text classification\n",
    "- Text clustering\n",
    "- Representations\n",
    "\n",
    "- Syntax analysis \n",
    "- Part of Speech Tagging\n",
    "\n",
    "\n",
    "- Also:\n",
    "    - Speech Recognition\n",
    "    - Speech Synthesis\n",
    "    - Machine Translation \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf representation\n",
    "\n",
    "tf–idf (frequency–inverse document frequency) comes from Information Retrieval and is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n",
    "\n",
    "It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes used in practice.\n",
    "\n",
    "\n",
    "Variations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields, including text summarization and classification.\n",
    "\n",
    "One of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$tf$ = number of times that term t occurs in document d. \n",
    "\n",
    "\n",
    "$tf(d,t) = \\frac{ \\#term\\_in\\_doc }{ \\#total\\_terms\\_in\\_doc}  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverse document frequency is a measure of how much information the word provides,\n",
    "i.e., if it is common or rare across all documents. It is the logarithmically scaled inverse fraction of the documents that contain the word (obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ idf(t, D) = log( \\frac{ N }{ | \\{ d \\in D: t \\in D \\} | }  ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N: total number of documents in the corpus, N = {|D|}\n",
    "\n",
    "    \n",
    "$\\{ {d \\in D: t \\in d \\} }$: number of documents where the term t appears    \n",
    "    \n",
    "If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the denominator to $ 1 + |\\{d \\in D: t \\in d \\} | $\n",
    "\n",
    "and overall \n",
    "\n",
    "$$ tfidf(t,d) = tf(t,d) * idf(t,D) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of speech tagging \n",
    "\n",
    "POS tagging is the process of marking up a word in a corpus to a corresponding part of a speech tag, based on its context and definition.\n",
    "\n",
    "Part of speech tags are very useful for intent classification and web query optimisation.\n",
    "\n",
    "\n",
    "There are several libraries that support POS tagging such as NLTK, spacy, FlairNLP\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-28 13:38:24,208 https://nlp.informatik.hu-berlin.de/resources/models/pos/en-pos-ontonotes-v0.5.pt not found in cache, downloading to /var/folders/g6/y0xdcdss4pn4llzglgct_yvm0000gn/T/tmpbwvkz685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249072763/249072763 [01:16<00:00, 3257116.67B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-28 13:39:40,942 copying /var/folders/g6/y0xdcdss4pn4llzglgct_yvm0000gn/T/tmpbwvkz685 to cache at /Users/kostas/.flair/models/en-pos-ontonotes-v0.5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-28 13:39:41,732 removing temp file /var/folders/g6/y0xdcdss4pn4llzglgct_yvm0000gn/T/tmpbwvkz685\n",
      "2021-05-28 13:39:41,788 loading file /Users/kostas/.flair/models/en-pos-ontonotes-v0.5.pt\n",
      "\n",
      "\n",
      "Sentence: \"Arteta wants temporary head injury substitutes after David Luiz incident\"   [− Tokens: 10  − Token-Labels: \"Arteta <NNP> wants <VBZ> temporary <JJ> head <NN> injury <NN> substitutes <NNS> after <IN> David <NNP> Luiz <NNP> incident <NN>\"]\n",
      "The following POS tags are found:\n",
      "\n",
      "\n",
      "Span [1]: \"Arteta\"   [− Labels: NNP (1.0)]\n",
      "Span [2]: \"wants\"   [− Labels: VBZ (1.0)]\n",
      "Span [3]: \"temporary\"   [− Labels: JJ (1.0)]\n",
      "Span [4]: \"head\"   [− Labels: NN (0.9996)]\n",
      "Span [5]: \"injury\"   [− Labels: NN (1.0)]\n",
      "Span [6]: \"substitutes\"   [− Labels: NNS (1.0)]\n",
      "Span [7]: \"after\"   [− Labels: IN (0.9999)]\n",
      "Span [8]: \"David\"   [− Labels: NNP (1.0)]\n",
      "Span [9]: \"Luiz\"   [− Labels: NNP (1.0)]\n",
      "Span [10]: \"incident\"   [− Labels: NN (0.9991)]\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "\n",
    "# make a sentence\n",
    "sentence = Sentence('Arteta wants temporary head injury substitutes after David Luiz incident')\n",
    "\n",
    "# load the POS tagger\n",
    "tagger = SequenceTagger.load('pos')\n",
    "\n",
    "# run NER over sentence\n",
    "tagger.predict(sentence)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(sentence)\n",
    "print('The following POS tags are found:')\n",
    "print(\"\\n\")\n",
    "\n",
    "# iterate over entities and print\n",
    "for entity in sentence.get_spans('pos'):\n",
    "    print(entity)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NNP : Proper noun, singular\n",
    "- VB  : Verb, base form\n",
    "- VBZ : Verb, 3rd person singular present\n",
    "- IN  : Preposition or subordinating conjunction\n",
    "- JJ  : Adjective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "Chunking (shallow parsing) it  the identification of parts of speech and short phrases (like noun phrases). POS tagging identifies labels of words such as verbs, adjectives, etc.\n",
    "\n",
    "Chunking essentially groups text into blocks semantic boundaries such as noun phrases, verbal phrases etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-28 13:40:50,079 https://nlp.informatik.hu-berlin.de/resources/models/chunk/en-chunk-conll2000-v0.4.pt not found in cache, downloading to /var/folders/g6/y0xdcdss4pn4llzglgct_yvm0000gn/T/tmphc7aiyuy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249034168/249034168 [01:17<00:00, 3233880.74B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-28 13:42:07,355 copying /var/folders/g6/y0xdcdss4pn4llzglgct_yvm0000gn/T/tmphc7aiyuy to cache at /Users/kostas/.flair/models/en-chunk-conll2000-v0.4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-28 13:42:08,211 removing temp file /var/folders/g6/y0xdcdss4pn4llzglgct_yvm0000gn/T/tmphc7aiyuy\n",
      "2021-05-28 13:42:08,246 loading file /Users/kostas/.flair/models/en-chunk-conll2000-v0.4.pt\n",
      "\n",
      "\n",
      "Sentence: \"Arteta wants temporary head injury substitutes after David Luiz incident\"   [− Tokens: 10  − Token-Labels: \"Arteta <S-NP> wants <S-VP> temporary <B-NP> head <I-NP> injury <I-NP> substitutes <E-NP> after <S-PP> David <B-NP> Luiz <I-NP> incident <E-NP>\"]\n",
      "The following chunks are found:\n",
      "\n",
      "\n",
      "Span [1]: \"Arteta\"   [− Labels: NP (0.9991)]\n",
      "Span [2]: \"wants\"   [− Labels: VP (0.9998)]\n",
      "Span [3,4,5,6]: \"temporary head injury substitutes\"   [− Labels: NP (0.8966)]\n",
      "Span [7]: \"after\"   [− Labels: PP (0.4565)]\n",
      "Span [8,9,10]: \"David Luiz incident\"   [− Labels: NP (0.8614)]\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "\n",
    "# make a sentence\n",
    "sentence = Sentence('Arteta wants temporary head injury substitutes after David Luiz incident')\n",
    "\n",
    "# load the Chunk tagger\n",
    "tagger = SequenceTagger.load('chunk')\n",
    "\n",
    "# run Chunk over sentence\n",
    "tagger.predict(sentence)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(sentence)\n",
    "print('The following chunks are found:')\n",
    "print(\"\\n\")\n",
    "\n",
    "# iterate over entities and print\n",
    "for entity in sentence.get_spans('np'):\n",
    "    print(entity)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NP: Noun Phrase\n",
    "VP: Verb Phrase\n",
    "PP: Prepositional Phrase \n",
    "ADJP: Adjective phrase\n",
    "ADVP: Adverb phrase "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER)\n",
    "\n",
    "With NER, we aim to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-28 13:42:47,069 https://nlp.informatik.hu-berlin.de/resources/models/ner/en-ner-conll03-v0.4.pt not found in cache, downloading to /var/folders/g6/y0xdcdss4pn4llzglgct_yvm0000gn/T/tmp0tc8cpjt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 432197603/432197603 [02:11<00:00, 3293317.49B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-28 13:44:58,548 copying /var/folders/g6/y0xdcdss4pn4llzglgct_yvm0000gn/T/tmp0tc8cpjt to cache at /Users/kostas/.flair/models/en-ner-conll03-v0.4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-28 13:45:00,388 removing temp file /var/folders/g6/y0xdcdss4pn4llzglgct_yvm0000gn/T/tmp0tc8cpjt\n",
      "2021-05-28 13:45:00,490 loading file /Users/kostas/.flair/models/en-ner-conll03-v0.4.pt\n",
      "\n",
      "\n",
      "Sentence: \"Arteta wants temporary head injury substitutes after David Luiz incident\"   [− Tokens: 10  − Token-Labels: \"Arteta <S-PER> wants temporary head injury substitutes after David <B-PER> Luiz <E-PER> incident\"]\n",
      "The following NER tags are found:\n",
      "\n",
      "\n",
      "Span [1]: \"Arteta\"   [− Labels: PER (1.0)]\n",
      "Span [8,9]: \"David Luiz\"   [− Labels: PER (0.9987)]\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "\n",
    "# make a sentence\n",
    "sentence = Sentence('Arteta wants temporary head injury substitutes after David Luiz incident')\n",
    "\n",
    "# load the NER tagger\n",
    "tagger = SequenceTagger.load('ner')\n",
    "\n",
    "# run NER over sentence\n",
    "tagger.predict(sentence)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(sentence)\n",
    "print('The following NER tags are found:')\n",
    "print(\"\\n\")\n",
    "\n",
    "# iterate over entities and print\n",
    "for entity in sentence.get_spans('ner'):\n",
    "    print(entity)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text clustering \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kmeans\n",
    "\n",
    "Let's see how we can do text clustering with k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import numpy as np\n",
    "\n",
    "data = open('./data/amazon_reviews.txt', \"r\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 30\n",
    "\n",
    "vec = CountVectorizer(max_df=2, max_features=30000)\n",
    "\n",
    "svd = TruncatedSVD(n_components)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(vec, svd, normalizer)\n",
    "X = lsa.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=10, random_state=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_clusters = 10\n",
    "model = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4599,)\n",
      "(872,)\n",
      "(959,)\n",
      "(818,)\n",
      "(757,)\n",
      "(694,)\n",
      "(927,)\n",
      "(608,)\n",
      "(756,)\n",
      "(924,)\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_clusters):\n",
    "    print(labels[labels==i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[1 3 5]\n",
      "i was misled and thought i was buying the entire cd and it contains one song \n",
      "\n",
      "anything you purchase in the left behind series is an excellent read . these books are great and ver\n",
      "in my experience , this camera takes great pictures , but the zoom lens is so delicate that it break\n",
      "\n",
      "\n",
      "[11 19 24]\n",
      "as a certified information systems security professional , i can definitively state that this book d\n",
      "i believe this is one of the best lens for about $1000. forget about zoom lenses even if they are l \n",
      "i bought bead fantasies and bead fantasies ii at the same time after reading the positive reviews ; \n",
      "\n",
      "\n",
      "[ 4  9 21]\n",
      "i loved these movies , and i cant wiat for the third one ! very funny , not suitable for chilren \n",
      "\n",
      "if you 're intimidated by tolstoy or dostoevsky 's long masterpieces , consider starting instead wit\n",
      "easily the worst textbook i encountered during my undergraduate years . unfocused , sparse , and alm\n",
      "\n",
      "\n",
      "[61 68 71]\n",
      "the major flaw with this product is that it 's not clear which version of office you can upgrade fro\n",
      "1. you ca n't print on anything over 8.5x14 paper . useless if you want to review your house to any \n",
      "one of the movies i like to watch over and over . jessica tandy played a great character in this mov\n",
      "\n",
      "\n",
      "[ 2 33 63]\n",
      "i have introduced many of my ell , high school students to lois lowery and the depth of her characte\n",
      "before using colorvision spyder2 my monitor was calibrated with the free adobe gamma program install\n",
      "i was assigned this book for a college course and was greatly surprised at its quality . blamires ha\n",
      "\n",
      "\n",
      "[22 36 80]\n",
      "our photo and video editing softwares were both very basic and \" old \" , so we decided to upgrade a \n",
      "word space for the other language is not work performed or designed . for english ok ! work 's good \n",
      "i feel some of the other reviews here are unjust and over exagerated . there were two places in the \n",
      "\n",
      "\n",
      "[0 7 8]\n",
      "i bought this album because i loved the title song . it 's such a great song , how bad can the rest \n",
      "this is a very good shaver for the private area . however , the key to getting the best results is t\n",
      "not even worth finishing this book ! this author does not know enough to even write this book ! i ha\n",
      "\n",
      "\n",
      "[12 15 17]\n",
      "i just got my first case of this fabric softener last week and yesterday , i was walking downstairs \n",
      "the s9000 fits perfectly in the case . too bad the case has no strap and no belt loop . apparently ,\n",
      "my cannon died the day before my daughters birthday so i ran out and tried a bunch of cameras in the\n",
      "\n",
      "\n",
      "[16 20 23]\n",
      "outstanding camera in every respect . i 've owned several digital cameras in recent years ( sony & n\n",
      "low tech and brilliant , music that seems to have channeled zappa , winchester cathedral , and god k\n",
      "i just got this camera a week ago and thought it was great . but today i tried to turn it on and it \n",
      "\n",
      "\n",
      "[38 49 65]\n",
      "i 'm holding on the phone right now trying to get support for this product . there is no 800 number \n",
      "having visited st. petersburg , russia a year ago , i could not pass up the chance to watch this mov\n",
      "i used this software to do a scrapbook for my mother for her birthday . first of all the software ta\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_clusters):\n",
    "    print( \"\\n\")\n",
    "    \n",
    "    idx  = np.where(labels==i)\n",
    "    idxs = idx[0][0:3]\n",
    "    \n",
    "    print(idxs)\n",
    "    \n",
    "    \n",
    "    print(data[idxs[0]][0:100] )\n",
    "    print(data[idxs[1]][0:100] )\n",
    "    print(data[idxs[2]][0:100] )\n",
    "    \n",
    "    \n",
    "    #print(data[idxs[0]][0:200].strip())\n",
    "    #print(100*'-')\n",
    "    #print(data[idxs[2]][0:200].strip())\n",
    "    \n",
    "    #print(100*\"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA (Latent Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "corpus = gensim.corpora.textcorpus.TextCorpus('./data/amazon_reviews.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i bought this album because i loved the title song . it 's such a great song , how bad can the rest of the album be , right ? well , the rest of the songs are just filler and are n't worth the money i paid for this . it 's either shameless bubblegum or oversentimentalized depressing tripe . kenny chesney is a popular artist and as a result he is in the cookie cutter category of the nashville music scene . he 's gotta pump out the albums so the record company can keep lining their pockets while the suckers out there keep buying this garbage to perpetuate more garbage coming out of that town . i 'll get down off my soapbox now . but country music really needs to get back to it 's roots and stop this pop nonsense . what country music really is and what it is considered to be by mainstream are two different things . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines = open(\"./data/amazon_reviews.txt\").readlines()\n",
    "print(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i have introduced many of my ell , high school students to lois lowery and the depth of her characters . she is a brilliant writer and capable of inspiring fierce passion in her readers as they encounter shocking details of her utopian worlds . i was anxious to read this companion novel and had planned to share it with my class this january . although the series is written for 6th graders and older , this book 's simplicity , in its message , language and writing style will inspire no one . i am sadly disappointed \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(lines[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bought', 'album', 'loved', 'title', 'song', 'great', 'song', 'bad', 'rest', 'album', 'right', 'rest', 'songs', 'filler', 'worth', 'money', 'paid', 'shameless', 'bubblegum', 'oversentimentalized', 'depressing', 'tripe', 'kenny', 'chesney', 'popular', 'artist', 'result', 'cookie', 'cutter', 'category', 'nashville', 'music', 'scene', 'gotta', 'pump', 'albums', 'record', 'company', 'lining', 'pockets', 'suckers', 'buying', 'garbage', 'perpetuate', 'garbage', 'coming', 'town', 'soapbox', 'country', 'music', 'needs', 'roots', 'stop', 'pop', 'nonsense', 'country', 'music', 'considered', 'mainstream', 'different', 'things']\n",
      "\n",
      "\n",
      "['misled', 'thought', 'buying', 'entire', 'contains', 'song']\n",
      "\n",
      "\n",
      "['introduced', 'ell', 'high', 'school', 'students', 'lois', 'lowery', 'depth', 'characters', 'brilliant', 'writer', 'capable', 'inspiring', 'fierce', 'passion', 'readers', 'encounter', 'shocking', 'details', 'utopian', 'worlds', 'anxious', 'read', 'companion', 'novel', 'planned', 'share', 'class', 'january', 'series', 'written', 'graders', 'older', 'book', 'simplicity', 'message', 'language', 'writing', 'style', 'inspire', 'sadly', 'disappointed']\n"
     ]
    }
   ],
   "source": [
    "seq = corpus.get_texts()\n",
    "print(next(seq))\n",
    "print(\"\\n\")\n",
    "print(next(seq))\n",
    "print(\"\\n\")\n",
    "print(next(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.LdaModel(corpus, id2word=corpus.dictionary,\n",
    "                               alpha='auto',\n",
    "                               num_topics=10,\n",
    "                               passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: season glasses head mcafee teeth pair projects like pinnacle widescreen\n",
      "1: game like kids good way lot body think games play\n",
      "2: product use time software like work program great good version\n",
      "3: quot bar flaxseed flavor autofocus que bars flag contains taste\n",
      "4: dvd video videos capabilities image transfer dvds nero player audio\n",
      "5: skin oil like water day face eat energy food body\n",
      "6: book read story people life books like time author world\n",
      "7: camera lens pictures quality canon good battery great use digital\n",
      "8: movie film like great good dvd love time best movies\n",
      "9: album music like songs song good sound band great rock\n"
     ]
    }
   ],
   "source": [
    "for topic_id in range(model.num_topics):\n",
    "    topk = model.show_topic(topic_id, 10)\n",
    "    topk_words = [ w for w, _ in topk ]\n",
    "    \n",
    "    print('{}: {}'.format(topic_id, ' '.join(topk_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
