{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "tl;dr: Make computers understand language\n",
    "\n",
    "\n",
    "- Text classification\n",
    "- Text clustering\n",
    "- Representations\n",
    "\n",
    "- Syntax analysis \n",
    "- Part of Speech Tagging\n",
    "\n",
    "\n",
    "- Also:\n",
    "    - Speech Recognition\n",
    "    - Speech Synthesis\n",
    "    - Machine Translation \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf representation\n",
    "\n",
    "tf–idf (frequency–inverse document frequency) comes from Information Retrieval and is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n",
    "\n",
    "It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes used in practice.\n",
    "\n",
    "\n",
    "Variations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields, including text summarization and classification.\n",
    "\n",
    "One of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$tf$ = number of times that term t occurs in document d. \n",
    "\n",
    "\n",
    "$tf(d,t) = \\frac{ \\#term\\_in\\_doc }{ \\#total\\_terms\\_in\\_doc}  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverse document frequency is a measure of how much information the word provides,\n",
    "i.e., if it is common or rare across all documents. It is the logarithmically scaled inverse fraction of the documents that contain the word (obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ idf(t, D) = log( \\frac{ N }{ | \\{ d \\in D: t \\in D \\} | }  ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N: total number of documents in the corpus, N = {|D|}\n",
    "\n",
    "    \n",
    "$\\{ {d \\in D: t \\in d \\} }$: number of documents where the term t appears    \n",
    "    \n",
    "If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the denominator to $ 1 + |\\{d \\in D: t \\in d \\} | $\n",
    "\n",
    "and overall \n",
    "\n",
    "$$ tfidf(t,d) = tf(t,d) x idf(t,D) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of speech tagging \n",
    "\n",
    "POS tagging is the process of marking up a word in a corpus to a corresponding part of a speech tag, based on its context and definition.\n",
    "\n",
    "Part of speech tags are very useful for intent classification and web query optimisation.\n",
    "\n",
    "\n",
    "There are several libraries that support POS tagging such as NLTK, spacy, FlairNLP\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-02 20:56:35,961 https://nlp.informatik.hu-berlin.de/resources/models/pos/en-pos-ontonotes-v0.5.pt not found in cache, downloading to /tmp/tmpt09zowwq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249072763/249072763 [10:40<00:00, 388665.79B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-02 21:07:17,098 copying /tmp/tmpt09zowwq to cache at /home/kostas/.flair/models/en-pos-ontonotes-v0.5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-02 21:07:17,320 removing temp file /tmp/tmpt09zowwq\n",
      "2020-12-02 21:07:17,351 loading file /home/kostas/.flair/models/en-pos-ontonotes-v0.5.pt\n",
      "\n",
      "\n",
      "Sentence: \"Arteta wants temporary head injury substitutes after David Luiz incident\"   [− Tokens: 10  − Token-Labels: \"Arteta <NNP> wants <VBZ> temporary <JJ> head <NN> injury <NN> substitutes <NNS> after <IN> David <NNP> Luiz <NNP> incident <NN>\"]\n",
      "The following POS tags are found:\n",
      "\n",
      "\n",
      "Span [1]: \"Arteta\"   [− Labels: NNP (1.0)]\n",
      "Span [2]: \"wants\"   [− Labels: VBZ (1.0)]\n",
      "Span [3]: \"temporary\"   [− Labels: JJ (1.0)]\n",
      "Span [4]: \"head\"   [− Labels: NN (0.9996)]\n",
      "Span [5]: \"injury\"   [− Labels: NN (1.0)]\n",
      "Span [6]: \"substitutes\"   [− Labels: NNS (1.0)]\n",
      "Span [7]: \"after\"   [− Labels: IN (0.9999)]\n",
      "Span [8]: \"David\"   [− Labels: NNP (1.0)]\n",
      "Span [9]: \"Luiz\"   [− Labels: NNP (1.0)]\n",
      "Span [10]: \"incident\"   [− Labels: NN (0.9991)]\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "\n",
    "# make a sentence\n",
    "sentence = Sentence('Arteta wants temporary head injury substitutes after David Luiz incident')\n",
    "\n",
    "# load the NER tagger\n",
    "tagger = SequenceTagger.load('pos')\n",
    "\n",
    "# run NER over sentence\n",
    "tagger.predict(sentence)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(sentence)\n",
    "print('The following POS tags are found:')\n",
    "print(\"\\n\")\n",
    "\n",
    "# iterate over entities and print\n",
    "for entity in sentence.get_spans('pos'):\n",
    "    print(entity)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNP : Proper noun, singular\n",
    "VB  : Verb, base form\n",
    "VBZ : Verb, 3rd person singular present\n",
    "IN  : Preposition or subordinating conjunction\n",
    "JJ  : Adjective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "Chunking (shallow parsing) it  the identification of parts of speech and short phrases (like noun phrases). POS tagging identifies labels of words such as verbs, adjectives, etc.\n",
    "\n",
    "Chunking essentially groups text into blocks semantic boundaries such as noun phrases, verbal phrases etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-02 21:29:48,444 loading file /home/kostas/.flair/models/en-chunk-conll2000-v0.4.pt\n",
      "\n",
      "\n",
      "Sentence: \"Arteta wants temporary head injury substitutes after David Luiz incident\"   [− Tokens: 10  − Token-Labels: \"Arteta <S-NP> wants <S-VP> temporary <B-NP> head <I-NP> injury <I-NP> substitutes <E-NP> after <S-PP> David <B-NP> Luiz <I-NP> incident <E-NP>\"]\n",
      "The following chunks are found:\n",
      "\n",
      "\n",
      "Span [1]: \"Arteta\"   [− Labels: NP (0.9991)]\n",
      "Span [2]: \"wants\"   [− Labels: VP (0.9998)]\n",
      "Span [3,4,5,6]: \"temporary head injury substitutes\"   [− Labels: NP (0.8966)]\n",
      "Span [7]: \"after\"   [− Labels: PP (0.4565)]\n",
      "Span [8,9,10]: \"David Luiz incident\"   [− Labels: NP (0.8614)]\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "\n",
    "# make a sentence\n",
    "sentence = Sentence('Arteta wants temporary head injury substitutes after David Luiz incident')\n",
    "\n",
    "# load the NER tagger\n",
    "tagger = SequenceTagger.load('chunk')\n",
    "\n",
    "# run NER over sentence\n",
    "tagger.predict(sentence)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(sentence)\n",
    "print('The following chunks are found:')\n",
    "print(\"\\n\")\n",
    "\n",
    "# iterate over entities and print\n",
    "for entity in sentence.get_spans('np'):\n",
    "    print(entity)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NP: Noun Phrase\n",
    "VP: Verb Phrase\n",
    "PP: Prepositional Phrase \n",
    "ADJP: Adjective phrase\n",
    "ADVP: Adverb phrase "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER)\n",
    "\n",
    "With NER, we aim to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-02 20:40:08,102 loading file /home/kostas/.flair/models/en-ner-conll03-v0.4.pt\n",
      "\n",
      "\n",
      "Sentence: \"Arteta wants temporary head injury substitutes after David Luiz incident\"   [− Tokens: 10  − Token-Labels: \"Arteta <S-PER> wants temporary head injury substitutes after David <B-PER> Luiz <E-PER> incident\"]\n",
      "The following NER tags are found:\n",
      "\n",
      "\n",
      "Span [1]: \"Arteta\"   [− Labels: PER (1.0)]\n",
      "Span [8,9]: \"David Luiz\"   [− Labels: PER (0.9987)]\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "\n",
    "# make a sentence\n",
    "sentence = Sentence('Arteta wants temporary head injury substitutes after David Luiz incident')\n",
    "\n",
    "# load the NER tagger\n",
    "tagger = SequenceTagger.load('ner')\n",
    "\n",
    "# run NER over sentence\n",
    "tagger.predict(sentence)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(sentence)\n",
    "print('The following NER tags are found:')\n",
    "print(\"\\n\")\n",
    "\n",
    "# iterate over entities and print\n",
    "for entity in sentence.get_spans('ner'):\n",
    "    print(entity)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text clustering \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kmeans\n",
    "\n",
    "Let's see how we can do text clustering with k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import numpy as np\n",
    "\n",
    "data = open('./data/amazon_reviews.txt', \"r\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 30\n",
    "\n",
    "vec = CountVectorizer(max_df=2, max_features=30000)\n",
    "\n",
    "svd = TruncatedSVD(n_components)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(vec, svd, normalizer)\n",
    "X = lsa.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=10, random_state=0)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_clusters = 10\n",
    "model = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878,)\n",
      "(788,)\n",
      "(4620,)\n",
      "(698,)\n",
      "(720,)\n",
      "(988,)\n",
      "(1027,)\n",
      "(757,)\n",
      "(745,)\n",
      "(693,)\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_clusters):\n",
    "    print(labels[labels==i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[12 21 23]\n",
      "i just got my first case of this fabric softener last week and yesterday , i was walking downstairs \n",
      "easily the worst textbook i encountered during my undergraduate years . unfocused , sparse , and alm\n",
      "i just got this camera a week ago and thought it was great . but today i tried to turn it on and it \n",
      "\n",
      "\n",
      "[ 4 24 68]\n",
      "i loved these movies , and i cant wiat for the third one ! very funny , not suitable for chilren \n",
      "\n",
      "i bought bead fantasies and bead fantasies ii at the same time after reading the positive reviews ; \n",
      "1. you ca n't print on anything over 8.5x14 paper . useless if you want to review your house to any \n",
      "\n",
      "\n",
      "[1 3 5]\n",
      "i was misled and thought i was buying the entire cd and it contains one song \n",
      "\n",
      "anything you purchase in the left behind series is an excellent read . these books are great and ver\n",
      "in my experience , this camera takes great pictures , but the zoom lens is so delicate that it break\n",
      "\n",
      "\n",
      "[18 26 44]\n",
      "i got this for my mom when i got the digital frame for my husband it is very easy to download pictur\n",
      "i am suppose to be receiving a refund because i have done sent it back but i have not heard anything\n",
      "zonealarm by checkpoint is one the best programs for protecting yourself . it is better than norton \n",
      "\n",
      "\n",
      "[ 9 15 17]\n",
      "if you 're intimidated by tolstoy or dostoevsky 's long masterpieces , consider starting instead wit\n",
      "the s9000 fits perfectly in the case . too bad the case has no strap and no belt loop . apparently ,\n",
      "my cannon died the day before my daughters birthday so i ran out and tried a bunch of cameras in the\n",
      "\n",
      "\n",
      "[ 0 19 32]\n",
      "i bought this album because i loved the title song . it 's such a great song , how bad can the rest \n",
      "i believe this is one of the best lens for about $1000. forget about zoom lenses even if they are l \n",
      "this is the best invention yet . i could not believe how warm and smooth the cream felt . what a dif\n",
      "\n",
      "\n",
      "[11 16 20]\n",
      "as a certified information systems security professional , i can definitively state that this book d\n",
      "outstanding camera in every respect . i 've owned several digital cameras in recent years ( sony & n\n",
      "low tech and brilliant , music that seems to have channeled zappa , winchester cathedral , and god k\n",
      "\n",
      "\n",
      "[ 2  7 22]\n",
      "i have introduced many of my ell , high school students to lois lowery and the depth of her characte\n",
      "this is a very good shaver for the private area . however , the key to getting the best results is t\n",
      "our photo and video editing softwares were both very basic and \" old \" , so we decided to upgrade a \n",
      "\n",
      "\n",
      "[52 66 69]\n",
      "\" i have shooting pains in my upper left arm . \" said the pilot to brian . brian is the main charact\n",
      "this movie is perfect for people in their 40ies and / or 50ies . it just hits the bullseye . lean ba\n",
      "the brush completely feel apart prior to using it . i sent a review to the company with no response \n",
      "\n",
      "\n",
      "[ 8 47 50]\n",
      "not even worth finishing this book ! this author does not know enough to even write this book ! i ha\n",
      "i am a busy architect and this is the 2nd pair of clics i own . if you are active and need reading g\n",
      "just like in my other review i ca n't figure out the madness over these guys . i will ask in this po\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_clusters):\n",
    "    print( \"\\n\")\n",
    "    \n",
    "    idx  = np.where(labels==i)\n",
    "    idxs = idx[0][0:3]\n",
    "    \n",
    "    print(idxs)\n",
    "    \n",
    "    \n",
    "    print(data[idxs[0]][0:100] )\n",
    "    print(data[idxs[1]][0:100] )\n",
    "    print(data[idxs[2]][0:100] )\n",
    "    \n",
    "    \n",
    "    #print(data[idxs[0]][0:200].strip())\n",
    "    #print(100*'-')\n",
    "    #print(data[idxs[2]][0:200].strip())\n",
    "    \n",
    "    #print(100*\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "corpus = gensim.corpora.textcorpus.TextCorpus('./data/amazon_reviews.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bought', 'album', 'loved', 'title', 'song', 'great', 'song', 'bad', 'rest', 'album', 'right', 'rest', 'songs', 'filler', 'worth', 'money', 'paid', 'shameless', 'bubblegum', 'oversentimentalized', 'depressing', 'tripe', 'kenny', 'chesney', 'popular', 'artist', 'result', 'cookie', 'cutter', 'category', 'nashville', 'music', 'scene', 'gotta', 'pump', 'albums', 'record', 'company', 'lining', 'pockets', 'suckers', 'buying', 'garbage', 'perpetuate', 'garbage', 'coming', 'town', 'soapbox', 'country', 'music', 'needs', 'roots', 'stop', 'pop', 'nonsense', 'country', 'music', 'considered', 'mainstream', 'different', 'things']\n",
      "\n",
      "\n",
      "['misled', 'thought', 'buying', 'entire', 'contains', 'song']\n",
      "\n",
      "\n",
      "['introduced', 'ell', 'high', 'school', 'students', 'lois', 'lowery', 'depth', 'characters', 'brilliant', 'writer', 'capable', 'inspiring', 'fierce', 'passion', 'readers', 'encounter', 'shocking', 'details', 'utopian', 'worlds', 'anxious', 'read', 'companion', 'novel', 'planned', 'share', 'class', 'january', 'series', 'written', 'graders', 'older', 'book', 'simplicity', 'message', 'language', 'writing', 'style', 'inspire', 'sadly', 'disappointed']\n"
     ]
    }
   ],
   "source": [
    "seq = corpus.get_texts()\n",
    "print(next(seq))\n",
    "print(\"\\n\")\n",
    "print(next(seq))\n",
    "print(\"\\n\")\n",
    "print(next(seq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.LdaModel(corpus, id2word=corpus.dictionary,\n",
    "                               alpha='auto',\n",
    "                               num_topics=10,\n",
    "                               passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: book read books author reading people life written world way\n",
      "1: album music like songs song quot great good sound best\n",
      "2: lens canon light image use camera lenses digital wide focus\n",
      "3: season scale body episodes fat workout weight episode dvd pressure\n",
      "4: movie film like good story time people love great characters\n",
      "5: hair product skin like razor use great shave head time\n",
      "6: recipes match christ poems jesus bowl duo cookbook proud god\n",
      "7: camera use good pictures great quality bought time battery like\n",
      "8: product software use program version time new work like support\n",
      "9: norton game year tax time bar program internet years virus\n"
     ]
    }
   ],
   "source": [
    "for topic_id in range(model.num_topics):\n",
    "    topk = model.show_topic(topic_id, 10)\n",
    "    topk_words = [ w for w, _ in topk ]\n",
    "    \n",
    "    print('{}: {}'.format(topic_id, ' '.join(topk_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
