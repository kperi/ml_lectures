{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - An introduction\n",
    "\n",
    "\n",
    "## Structure of  the class  \n",
    "___\n",
    "\n",
    "### Part I: Machine Learning\n",
    "  - Basics & Theory\n",
    "    - What's this all about?\n",
    "    - Supervised, Unsupervised Learning\n",
    "    - Classification, Regression, Clustering\n",
    "  - Python Basics for Data Science / ML\n",
    "    - numpy, scipy\n",
    "    - scikit learn\n",
    "    - pytorch\n",
    "  - Hands-on examples\n",
    "    - Classification\n",
    "        * Churn prediction\n",
    "        * Digit classification\n",
    "    - Regression example\n",
    "        * Estimating House Prices\n",
    "    - Clustering\n",
    "        - kmeans document clustering\n",
    "\n",
    "\n",
    "  - What is Machine Learning anyway?\n",
    "  - Some examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "\n",
    "A (very informal) definition: Build machines that learn through examples (data) and generalize to unseen data\n",
    "\n",
    "\n",
    "Examples we are all using in a daily basis:\n",
    "\n",
    "      - Car plate detection systems\n",
    "      - spam email classifier  \n",
    "      - voice recognition in smartphones\n",
    "      - face recognition @ facebook \n",
    "      - amazon recomendations  \n",
    "      \n",
    "\n",
    "Aim of this series is to build the foundations and principles to be able to implement some of this systems from scratch. \n",
    "\n",
    "\n",
    "### Introduction and definitions \n",
    "\n",
    "\n",
    "\n",
    "#### Supervised Learning\n",
    "   \n",
    "    Here, the data comes with (a fixed number of) labels  \n",
    "    \n",
    "      \n",
    "      - a dataset of emails and for every email a flag if it is spam or not spam \n",
    "      - photos of animals, where for each photo we have a label if it is a cat or a dog (assume for a moment there is only one animal per photo) \n",
    "      - fraud detection: a dataset with valid and fraudulent transation examples\n",
    "      \n",
    "#### Unsupervised Learning\n",
    "    \n",
    "     In the unsupervised learning case, data comes without labels \n",
    "     \n",
    "     - a collection of documents\n",
    "     - a collection of photographs\n",
    "     - timeseries data\n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "#### Regression (Παλλινδρόμηση)\n",
    "\n",
    "      In regression, the outcome we are trying to predict is or can be thought as real number (for example house prices, demand etc)    \n",
    "\n",
    "#### Classification (Ταξινόμηση)\n",
    "    \n",
    "    In classification the target variable is one of many categories. For example (cat, dog, bird), (spam, non-spam), (fraud, non-fraud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification \n",
    "\n",
    "The repeating pattern in supervised learing is that data comes with labels and we wish to build a system to be able to predict the label given a new data example. For example, if we are building a pet detection system, we would like the system to answer that this is a photo of a cat with very high accuracy, eg predict the class of of the new image to be cat and not dog or bird.\n",
    "\n",
    "\n",
    "<b><span style=\"color:red\">WARNING: MATHS AHEAD</span></b>\n",
    "\n",
    "Without being too formal, we would like to learn from the data a function $f(x) \\to D $, where $x \\in X$ is the data and D the set of the labels to be predicted.\n",
    "\n",
    "This function must have the property to be able to produce correct results in new, unseen data. That is, if we take a photo of a random cat somewhere in the world and we feed this cat in to our system, the system should be able to respond that this is a cat although it has never seen this cat before. We call this fuction property \"Gereralisation\" and is the most important aspect of machine learning systems: to be able to perform well in unseen data.\n",
    "\n",
    "\n",
    "Given the above, we need the following ingredients to build a machine learning system\n",
    "- The function we are going to be using to model our data (usually called hypothesis) \n",
    "- A way to measure how \"wrong\" is this function and a configuration of its parameters, given our data\n",
    "- A way to train this function, eg to modify the parameters in such a way that the error is minimised \n",
    "- A way to test this function to new, unseen data and make a claim of how well we expect this function to behave in unseen data. \n",
    "\n",
    "Similarly, for the regression part, instead of trying to predict a label we now try to predict a real value number. \n",
    "\n",
    "Again we need a hypothesis, a loss function, a way to tune the hypothesis and a way to see how well we are generalizing to unseen data.\n",
    "\n",
    "\n",
    "Let's see that in practice. \n",
    "\n",
    "\n",
    "#### Linear Regression\n",
    "\n",
    "One of  the simplest parametric models in statistics and machine learning is Linear Regression. \n",
    "\n",
    "Linear Regression tries to find a best straight line to fit the data. \n",
    "\n",
    "\n",
    "##### Linear Regression Example\n",
    "\n",
    "![alt text](./images/lr.png \"Linear Regression Example\")\n",
    "\n",
    "\n",
    "\n",
    "We are given a data set D of values  $\\{ (x_i,y_i) \\}$ and our hypothesis is that there is a straight line \n",
    "$y = h(x) = w_0 + w_1 * x$ that fits the data. \n",
    "\n",
    "We are now asked to find the \"best\" weights w that fit our data.\n",
    "\n",
    "A typicall way to define \"best\" is to try to minimize the square difference between our targets y and what the model predicts, $\\hat{y} = f(x)$,  eg minimize the total loss \n",
    "\n",
    "$$ L = \\sum_i{  ( y_i - \\hat{y_i})^2 } = \\sum_i{  ( y_i - \\ w_0 + w_1*x_i )^2 }$$ \n",
    "\n",
    "There a few ways to do that, either look for a closed form analytic expression or use an optimisation algorithm such as gradient descent. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generallization, model performance \n",
    "\n",
    "\n",
    "When we are solving a supervised learning problem, we are given data and labels and we want to build a predictive model which will be good enough to generalize to unseen data.\n",
    "\n",
    "\n",
    "A common way to proceed is to split the data in 3 parts, assuming we have enough data. Sometimes this is a reasonable assumption, some times it is not.\n",
    "\n",
    "\n",
    "We split the data in train, validation and test parts. (80/10/10 typically or something similar)\n",
    "\n",
    "We then train the model in the 80% of the data and we validate/optimise the model in the validate data. When we are happy, then ** and only then ** we are testing the model's performance on the test data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
