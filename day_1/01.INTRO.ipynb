{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - An introduction\n",
    "\n",
    "\n",
    "## Structure of  the class  \n",
    "___\n",
    "\n",
    "### Part I: Machine Learning\n",
    "  - Basics & Theory\n",
    "    - What's this all about?\n",
    "    - Supervised, Unsupervised Learning\n",
    "    - Classification, Regression, Clustering\n",
    "  - Python Basics for Data Science / ML\n",
    "    - numpy, scipy\n",
    "    - scikit learn\n",
    "    - pytorch\n",
    "  - Hands-on examples\n",
    "    - Classification\n",
    "        * Churn prediction\n",
    "        * Digit classification\n",
    "    - Regression example\n",
    "        * Estimating House Prices\n",
    "    - Clustering\n",
    "        - kmeans document clustering\n",
    "\n",
    "\n",
    "  - What is Machine Learning anyway?\n",
    "  - Some examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "\n",
    "A (very informal) definition: Build machines that learn through examples (data) and generalize to unseen data\n",
    "\n",
    "\n",
    "Examples we are all using in a daily basis:\n",
    "\n",
    "      - Car plate detection systems\n",
    "      - spam email classifier  \n",
    "      - voice recognition in smartphones\n",
    "      - face recognition @ facebook \n",
    "      - amazon recomendations  \n",
    "      \n",
    "\n",
    "Aim of this series is to build the foundations and principles to be able to implement some of this systems from scratch. \n",
    "\n",
    "\n",
    "### Introduction and definitions \n",
    "\n",
    "\n",
    "Machine learning problems are categorized in 3 broad categories as below, based on the nature/form of the available data.\n",
    "\n",
    "\n",
    "\n",
    "#### Supervised Learning\n",
    "   \n",
    "    Here, the data comes with (a fixed number of) labels  \n",
    "    \n",
    "      \n",
    "      - a dataset of emails and for every email a flag if it is spam or not spam \n",
    "      - photos of animals, where for each photo we have a label if it is a cat or a dog (assume for a moment there is only one animal per photo) \n",
    "      - fraud detection: a dataset with valid and fraudulent transation examples\n",
    "      \n",
    "#### Unsupervised Learning\n",
    "    \n",
    "     In the unsupervised learning case, data comes without labels \n",
    "     \n",
    "     - a collection of documents\n",
    "     - a collection of photographs\n",
    "     - timeseries data\n",
    "   \n",
    "#### Reinforcement Learning\n",
    "    \n",
    "    Here, the goal is to train an AI agent to learn how to navigate and take actions in a completely unknown environment. We won't be covering RL in this course.  \n",
    "\n",
    "\n",
    "### Supervised Learning Problems\n",
    "   \n",
    "   \n",
    "#### Regression (Παλλινδρόμηση)\n",
    "\n",
    "      In regression, the outcome we are trying to predict is or can be thought as real number (for example house prices, demand etc)    \n",
    "\n",
    "#### Classification (Ταξινόμηση)\n",
    "    \n",
    "    In classification the target variable is one of many categories. For example (cat, dog, bird), (spam, non-spam), (fraud, non-fraud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification \n",
    "\n",
    "The repeating pattern in supervised learing is that data comes with labels and we wish to build a system to be able to predict the label given a new data example. For example, if we are building a pet detection system, we would like the system to answer that this is a photo of a cat with very high accuracy, eg predict the class of of the new image to be a cat and not a dog or a bird.\n",
    "\n",
    "\n",
    "<b><span style=\"color:red\">WARNING: MATHS AHEAD</span></b>\n",
    "\n",
    "Without being too formal, we would like to learn from the data a function $f(x) \\to D $, where $x \\in X$ is a data instance and D the set of the labels to be predicted.\n",
    "\n",
    "This function must have the property to be able to produce correct results in new, unseen data. That is, if we take a photo of a random cat somewhere in the world and we feed this cat in to our system, the system should be able to respond that this is a cat although it has never seen this cat before. We call this fuction property **Generalisation** and is the most important aspect of machine learning systems: to be able to perform well in unseen data.\n",
    "\n",
    "\n",
    "Given the above, we need the following ingredients to build a machine learning system\n",
    "- The function we are going to be using to model our data (usually called hypothesis) \n",
    "- A way to measure how \"wrong\" this function is and a configuration of its parameters, given our data. This is called the loss function.\n",
    "- A way to train this function, eg to modify the parameters in such a way that the error is minimised \n",
    "- A way to test this function to new, unseen data and make a claim of how well we expect this function to behave in unseen data. \n",
    "\n",
    "Similarly, for the regression part, instead of trying to predict a label we now try to predict a real value number. \n",
    "\n",
    "Again we need a hypothesis, a loss function, a way to tune the hypothesis and a way to see how well we are generalizing to unseen data.\n",
    "\n",
    "\n",
    "Let's see that in practice. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametrics vs Non-Parametric learning algorithms\n",
    "\n",
    "\n",
    "Based on the nature of the hypothesis function, an ML model can be categorized as `parametric` or `non-parametric`\n",
    "\n",
    "A parametric algorithm is one where the hypothesis function is controlled by parameters (or weights).\n",
    "An example of such an algorithm is Linear Regression or a Neural Network, we are trying to find the weights/parameters that optimize the loss function. As long as we have these values, the training data can then be discarded.\n",
    "\n",
    "\n",
    "\n",
    "Non-parametric models differ from parametric models in that the model structure is not specified a priori but is instead determined from data. The term non-parametric is not meant to imply that such models completely lack parameters but that the number and nature of the parameters are flexible and not fixed in advance.\n",
    "\n",
    "\n",
    "A typicall example of non-parametric model is a very simple model, known as the kNN algorithm as we will see bellow. One thing to keep in mind with non-parametric algoriths is that we need to keep the entire training dataset when we are moving the model into production\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression algorithms\n",
    "\n",
    "#### Linear Regression\n",
    "\n",
    "One of  the simplest parametric models in statistics and machine learning is Linear Regression. \n",
    "\n",
    "Linear Regression tries to find a best straight line to fit the data. \n",
    "\n",
    "\n",
    "##### Linear Regression Example\n",
    "\n",
    "\n",
    "<img src=\"./images/lr2.jpg\"> \n",
    "\n",
    "\n",
    "\n",
    "We are given a data set D of values  $\\{ (x_i,y_i) \\}$ and our hypothesis is that there is a straight line \n",
    "$y = h(x) = w_0 + w_1  x$ that fits the data. \n",
    "\n",
    "We are now asked to find the \"best\" weights w that fit our data.\n",
    "\n",
    "A typicall way to define \"best\" is to try to minimize the square difference between our targets y and what the model predicts, $\\hat{y} = f(x)$,  eg minimize the total loss \n",
    "\n",
    "$$ L = \\sum_i{  ( y_i - \\hat{y_i})^2 } = \\sum_i{  ( y_i - \\ w_0 + w_1 x_i )^2 }$$ \n",
    "\n",
    "There a few ways to do that, either look for a closed form analytic expression or use an optimisation algorithm such as gradient descent. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification algorithms\n",
    "\n",
    "### k-nearest neighbors algorithm\n",
    "\n",
    "\n",
    "Imagine we are given a dataset of handwritten digits 0-9. Each digit image has fixed dimensions (for example 28x28, the MNIST dataset).\n",
    "\n",
    "We would like to build a classification system which will classify new incoming digits. One straightforwad way to do this is to compare the given new digit with every other digit we have in our dataset, pick k most similar digits to our new digit and report as classification decision the majority of the most similar votes.\n",
    "\n",
    "\n",
    "\n",
    "For example, let's assume that k=3 and the most similar images to our new image are images with labels 0,0,8.\n",
    "The class \"0\" appears 2 times so we will report that this image is a `0`. \n",
    "This is the k-NN algorithm, where $k$ is what we call a `hyperparameter`: \n",
    "\n",
    "A parameter that controls the behaviour of the algorithm and its optimal value is typically a \"function\" of the data.\n",
    "\n",
    "\n",
    "\n",
    "Question 1: What is missing here for the algorithm to work?\n",
    "Question 2: What is the obvious problem with this algorithm?\n",
    "\n",
    "\n",
    "### Logistic regression\n",
    "\n",
    "Consider a binary classification problem, with two classes, say emails labeld `spam` and `non-spam`.\n",
    "\n",
    "\n",
    "One of the most common models in Statistics and Machine Learning is a model called Logistic Regression. \n",
    "\n",
    "(Beware: it is **not** a regression model, but rather a classification model).\n",
    "\n",
    "\n",
    "\n",
    "Let's denote $p = Pr( label=spam | x )$ where x is a training example. Then, 1-p is the probability of x is non spam. Logistic Regression makes the assumption that the log-odd ratio of the probabilities is a linear model:\n",
    "\n",
    "$$ \\frac{p}{1-p} = w_0 + w_1  x_1  + ... + w_n x_n  $$\n",
    "\n",
    "\n",
    "after doing some math, we obtain that\n",
    "\n",
    "$$ p(x) =  \\frac{1}{1+e^{-(w_0 + w_1  x_1  + ... + w_n x_n )}} $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "the function $s(x) = \\frac{1}{ 1 + e^{-x} } $ is called the `sigmoid` function and is a fundamendal building block in Neural Networks and classification problems.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "\n",
    "X = np.linspace( -10, 10)\n",
    "Y = sigmoid(X)\n",
    "\n",
    "plt.plot( X, Y )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent and stochastic gradient descent \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Say we want to find a -local- minimum of a differentiable function. \n",
    "One way to do it is the following:\n",
    "\n",
    "\n",
    "We start at a random point, calculate the gradient at this point and take a step proportional to the the gradient's negative direction.\n",
    "\n",
    "\n",
    "Let's illustrate this \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/gd2d.jpg\"> \n",
    "<img src=\"images/gd3d.jpg\"> \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In Machine Learning, the function we are trying to minimize is the loss function. What we get back is the optimal values (weights, parameters) of the model, minimizing loss. \n",
    "\n",
    "Typically, the loss is a function of the entire training set. We can calculate loss the in the entire training set and perform gradient descent. This can be very slow however, especially when the training data set is large, so what we typically do is we shuffle the training data and we calculate the loss in small batches (mini batches) of 16,32 etc examples. And then we move towards the negative gradient of this mini-batch.\n",
    "\n",
    "This is called Stochastic Gradient Descent (SGD) and it is the main method when we are training Neural Nets and Deep Learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generallization, model performance \n",
    "\n",
    "\n",
    "When we are solving a supervised learning problem, we are given data and labels and we want to build a predictive model which will be good enough to generalize to unseen data.\n",
    "\n",
    "\n",
    "\n",
    "### Train/Validation/Test\n",
    "A common way to proceed is to split the data in 3 parts, assuming we have enough data. Sometimes this is a reasonable assumption, some times it is not.\n",
    "\n",
    "\n",
    "We split the data in train, validation and test parts. (80/10/10 typically or something similar)\n",
    "\n",
    "We then train the model in the 80% of the data and we validate/optimise the model in the validate data. When we are happy, then ** and only then ** we are testing the model's performance on the test data. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Cross Validation\n",
    "Another strategy used in practice is Cross Validation.\n",
    "\n",
    "\n",
    "Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample.\n",
    "\n",
    "The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation. When a specific value for k is chosen, it may be used in place of k in the reference to the model, such as k=10 becoming 10-fold cross-validation.\n",
    "\n",
    "Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.\n",
    "\n",
    "It is a popular method because it is simple to understand and because it generally results in a less biased or less optimistic estimate of the model skill than other methods, such as a simple train/test split.\n",
    "\n",
    "The general procedure is as follows:\n",
    "\n",
    "- Shuffle the dataset randomly.\n",
    "- Split the dataset into k groups\n",
    "- For each unique group:\n",
    "       - Take the group as a hold out or test data set\n",
    "        - Take the remaining groups as a training data set\n",
    "        - Fit a model on the training set and evaluate it on the test set\n",
    "        - Retain the evaluation score and discard the model\n",
    "- Summarize the skill of the model using the sample of model evaluation scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimisation \n",
    "\n",
    "\n",
    "In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned.\n",
    "\n",
    "The same kind of machine learning model can require different constraints, weights or learning rates to generalize different data patterns. These measures are called hyperparameters, and have to be tuned so that the model can optimally solve the machine learning problem. Hyperparameter optimization finds a tuple of hyperparameters that yields an optimal model which minimizes a predefined loss function on given independent data.[1] The objective function takes a tuple of hyperparameters and returns the associated loss.\n",
    "\n",
    "Cross-validation is often used to estimate this generalization performance.\n",
    "\n",
    "\n",
    "\n",
    "#### Hyperparameter optimisation strategies\n",
    "\n",
    "- Grid Search \n",
    "- Randomised Search \n",
    "- Advanced search methods (Gaussian Processes/Bayesian Optimisation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot products and cosine similarity \n",
    "\n",
    "\n",
    "####  dot product\n",
    "\n",
    "The dot (inner) product is a crusial operation in Machine Learning as it used to quantify vector similarity\n",
    "\n",
    "$ <x,y> = \\sum{x_i \\cdot y_i} $\n",
    "\n",
    "Generally, when we need to compare vectors their length is not important. Rather, we are mostly interested in vectors that have the same direction. A standard way to quantify similarity of vectors then is their angle: the smaller the angle the most similar the vectos are. If the angle is zero, the vectors are \"identical\" since they are pointing to the same direction.\n",
    "\n",
    "Casting this to a similarity measure is typically done by using the cosine of the angle between the two vectors. \n",
    "\n",
    "\n",
    "####  cosine similarity\n",
    "\n",
    "\n",
    "the cosine of two vectors is their dot product, normalized by the vector lengths:\n",
    "\n",
    "$$ cos( u , v ) = \\frac{ < u , v > } { ||u|| ||v|| }  =  \\frac{ u^T v }{ ||u|| ||v|| }  $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
