{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisers and learning rates\n",
    "\n",
    "\n",
    "There's a vast number of optimisers in the literature and most of them are offered in pytorch. \n",
    "Different problems tend to work best with certain optimisers, although recent research shows that SGD with proper architecture works effectively most of the times.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Momentum\n",
    "\n",
    "Momentum is a method that helps accelerate SGD in the relevant direction.\n",
    "It uses a fraction of the past time step to update the current gradient\n",
    "\n",
    "\n",
    "$$ u_t = \\gamma u_{t-1} + \\eta \\nabla_{\\theta}{J(\\theta))} $$\n",
    "\n",
    "$$\\theta \\leftarrow \\theta - u_{t}$$\n",
    "\n",
    "$$ u_{0} = 0$$\n",
    "\n",
    "\n",
    "### Adagrad\n",
    "\n",
    "In the vanilla SGD we are using the same learning rate for all the parameters.\n",
    "\n",
    "Adagrad is an algorithm for gradient-based optimization that adapts the learning rate to the parameters, performing smaller updates (i.e. low learning rates) for parameters associated with frequently occurring features, and larger updates (i.e. high learning rates) for parameters associated with infrequent features. For this reason, it is well-suited for dealing with sparse data. \n",
    " \n",
    " \n",
    "### Adadelta \n",
    "\n",
    "\n",
    "Adadelta is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size \n",
    "\n",
    "\n",
    "### Adam\n",
    "\n",
    "Adaptive Moment Estimation (Adam) is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients like Adadelta, Adam also keeps an exponentially decaying average of past gradients  similar to momentum.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Choosing learning rate\n",
    "\n",
    "Adagram, Adadelta and Adam in pytorch have a default learning rate:\n",
    "\n",
    "```python\n",
    "torch.optim.Adam(params, lr=0.001)\n",
    "torch.optim.Adagrad(params, lr=0.01) \n",
    "torch.optim.Adadelta(params, lr=1.0)\n",
    "```\n",
    "\n",
    "SGD does not have a default value for learning rate. Different problems may require different learning rates.\n",
    "\n",
    "Typical values for a neural network with standardized inputs (or inputs mapped to the (0,1) interval) are less than 1 and greater than $10^{-6}$\n",
    "\n",
    "A traditional default value for the learning rate is 0.01, and this may represent a good starting point.\n",
    "\n",
    "Personally, I would do two runs, one with a rate around $10^{-4}$ and one with $10^{-2}$, observe the loss function and decide if I need to move closer to $10^{-4}$ or to $10^{-2}$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "\n",
    "\n",
    "A very common paradigm followed in Machine Vision/Deep Learning is what we call data augmentation.\n",
    "Here, depending on the nature of the problem, we are transforming the input images and thus augmenting the training data set.\n",
    "\n",
    "Augmentation techniques include random noise injection to the input images, to make the network more robust in perturbations. Also, depending on the nature of the problem we can use rotation, translation, mirroring etc.\n",
    "\n",
    "Data augmentation is essentially an almost free way to increase the training set and allow the network to learn a more robust representation of the input domain, and it is hugely used in practice.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
