{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir ./data\n",
    "#!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz -O ./data/dataset.tar.gz\n",
    "#%cd data\n",
    "#!tar xvfz dataset.tar.gz\n",
    "#%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextCNN for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "from gensim.utils import tokenize, deaccent, simple_preprocess\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12500, 12500, 12500, 12500)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_train = glob.glob(\"./data/aclImdb/train/pos/*.txt\")\n",
    "negative_train = glob.glob(\"./data/aclImdb/train/neg/*.txt\")\n",
    "\n",
    "positive_test = glob.glob(\"./data/aclImdb/test/pos/*.txt\")\n",
    "negative_test = glob.glob(\"./data/aclImdb/test/neg/*.txt\")\n",
    "len(negative_test), len(positive_test), len(positive_train), len(negative_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter()\n",
    "\n",
    "for file in positive_train+negative_train+positive_test+negative_test:\n",
    "    c.update(simple_preprocess(open(file).read()))\n",
    "vocab = c.most_common(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus(object):\n",
    "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
    "    \n",
    "    def __init__(self, files):\n",
    "        self.files = files\n",
    "            \n",
    "    def __iter__(self):\n",
    "        for file in self.files:\n",
    "            \n",
    "            text = open( file ).read().lower()\n",
    "            \n",
    "            yield simple_preprocess(text)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5002"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx= { w[0]:i+2 for i, w in enumerate(vocab) }\n",
    "word2idx['<PAD>'] = 0\n",
    "word2idx['<UNK>'] = 1 \n",
    "idx2word = {i:w for w,i in word2idx.items()}\n",
    "SENTENCE_LENGTH = 80\n",
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index( word, vocab):\n",
    "    if word in vocab:\n",
    "        return vocab[word]\n",
    "    else:\n",
    "        return vocab['<UNK>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"Imbdb dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, positives, negatives, word2idx, sentence_length):\n",
    "\n",
    "        self.dataset = positives + negatives\n",
    "        self.word2idx = word2idx\n",
    "        self.labels = [1 for _ in range(len(positives))] + [0 for _ in range(len(negatives))]\n",
    "        self.sentence_length = sentence_length\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        \n",
    "        ret = torch.zeros( self.sentence_length, dtype=torch.int64)\n",
    "\n",
    "        text = open(self.dataset[idx]).read().lower()\n",
    "        s = simple_preprocess(text)\n",
    "           \n",
    "        for i in range(self.sentence_length):\n",
    "            #print(s[i] in self.word2idx)\n",
    "            ret[i]=0\n",
    "            if i<len(s):\n",
    "                ret[i] = get_index( s[i], self.word2idx)\n",
    "            \n",
    "        return ret, torch.tensor( self.labels[idx], dtype=torch.float32)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1765,  617,   42,    2,    1,    1,    4,    2,  354,    1,    1,    1,\n",
       "            4,    1,    9,    1,    1,    9,    1,    1,    2, 2598,    4,  354,\n",
       "            3,  403,   35, 1370,    4,    1,  554,    9,   10,    1, 1121,  113,\n",
       "          331,   25,  208,   74,    1,  119,  354, 1816,    5,    1, 1075,   26,\n",
       "           48,   67,  798,    2,    1,    4,    1,    1, 4504, 2598,   15,  354,\n",
       "            3,  403,   98,  152,    4, 1181, 1880,    1,    4,    2,    1, 1645,\n",
       "          993,    5,  552,  169, 2700,    3,    2, 1238]),\n",
       " tensor(1.))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_dataset = TextDataset(positive_train, \n",
    "                            negative_train,\n",
    "                            word2idx,  sentence_length = SENTENCE_LENGTH)\n",
    "\n",
    "test_dataset = TextDataset(positive_test,\n",
    "                           negative_test, \n",
    "                           word2idx,   sentence_length=SENTENCE_LENGTH)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32,\n",
    "                        shuffle=True, num_workers=0)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32,\n",
    "                        shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "dataloaders = { \"train\": train_dataloader, \"val\":test_dataloader}\n",
    "\n",
    "dataset_sizes = { \"train\": len(train_dataset), \"val\":len(test_dataset)}\n",
    "\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  10,   18,    1,  ...,  615,  827, 2667],\n",
       "         [ 332,    2,  263,  ...,    3,    1,    3],\n",
       "         [   2, 1134,  109,  ...,   81,  704,    3],\n",
       "         ...,\n",
       "         [   1,   17, 2492,  ...,  252, 3038, 2802],\n",
       "         [1509,  147,   36,  ...,    4,    1, 1299],\n",
       "         [  45,   16,    2,  ...,  148,  595,    3]]),\n",
       " tensor([0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "         1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0.]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_in, label = next(iter(train_dataloader))\n",
    "x_in, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 80])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim , n_hidden, \n",
    "                 num_embeddings, sentence_length, wv_model):\n",
    "        \n",
    "        super(TextClassifier, self).__init__()\n",
    "        \n",
    "        self.sentence_length = sentence_length\n",
    "        \n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout()\n",
    "\n",
    "        \n",
    "        weights = torch.FloatTensor(wv_model.wv.vectors)\n",
    "        self.embedding = nn.Embedding.from_pretrained(weights,freeze=True)\n",
    "        #print(\"req grad?=>\", self.embedding.requires_grad)\n",
    "        \n",
    "        #self.embedding.requires_grad = False\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 4, kernel_size=3, padding=1)\n",
    "        \n",
    "        \n",
    "        self.mp = nn.MaxPool2d(8, 3)\n",
    "        \n",
    "        #self.conv2 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n",
    "        #self.conv3 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        \n",
    "        \n",
    "        #self.bn1 = nn.BatchNorm2d(32)\n",
    "        #self.bn2 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.fc1 = nn.Linear( 80, n_hidden )\n",
    "        self.fc2 = nn.Linear( n_hidden, 1)\n",
    "        \n",
    "    \n",
    "    def forward(self, X):\n",
    "        batch_size = X.shape[0]\n",
    "        \n",
    "        out = self.embedding(X)\n",
    "        out = out.unsqueeze(1)\n",
    " \n",
    "        \n",
    "        out = self.conv1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.mp(out)\n",
    "       \n",
    "    \n",
    "        out = self.conv2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.mp(out)\n",
    "        \n",
    "        #print(out.shape)\n",
    "       \n",
    "        \n",
    "        #return out\n",
    "        #out = self.mp(out)\n",
    "        #out = out.view(batch_size, -1)\n",
    "        \n",
    "        \"\"\"\n",
    "        return out\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "    \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "    \n",
    "     \n",
    "        out = self.conv3(out)\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        \n",
    "        return out \n",
    "\n",
    "\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, embed_num,  embed_dim=200, class_num=1, ):\n",
    "        super(TextCNN, self).__init__()\n",
    "        \n",
    "       \n",
    "\n",
    "        V = embed_num\n",
    "        D = embed_dim\n",
    "        C = class_num\n",
    "        Ci = 1\n",
    "        Co = 3 \n",
    "        Ks = [3,4,5]\n",
    "        \n",
    "        self.embedding = nn.Embedding(V, D)\n",
    "        #weights = torch.FloatTensor(wv_model.wv.vectors)\n",
    "        #self.embedding = nn.Embedding.from_pretrained(weights,freeze=False)\n",
    "        \n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(len(Ks) * Co, C)\n",
    "\n",
    "    def conv_and_pool(self, x, conv):\n",
    "        x = F.relu(conv(x)).squeeze(3) \n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  \n",
    "        x = x.unsqueeze(1) \n",
    "\n",
    "        x = [F.relu(conv(x)).squeeze(3)\n",
    "             for conv in self.convs1]  \n",
    "\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2)\n",
    "             for i in x]  \n",
    "\n",
    "        x = torch.cat(x, 1)\n",
    " \n",
    "        x = self.dropout(x)   \n",
    "        out = self.fc1(x)  \n",
    "        return torch.sigmoid(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "net = TextCNN(embed_num=len(word2idx)+2)\n",
    "net(x_in).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnet = TextClassifier( n_hidden = 100, embedding_dim = 200, \\n                     num_embeddings=len(word2idx), sentence_length=SENTENCE_LENGTH,\\n                    wv_model=model)\\n\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "net = TextClassifier( n_hidden = 100, embedding_dim = 200, \n",
    "                     num_embeddings=len(word2idx), sentence_length=SENTENCE_LENGTH,\n",
    "                    wv_model=model)\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, dataloaders, scheduler,  num_epochs=10):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        model.train()\n",
    "        # Iterate over data.\n",
    "        epoch_loss = []\n",
    "        for inputs, labels in dataloaders[\"train\"]:\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.reshape(-1,1).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "                    \n",
    "            loss = criterion(outputs, labels)\n",
    "             \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss.append( loss.item( ))\n",
    "        \n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        targets = [] \n",
    "        predicted  = []\n",
    "        for inputs, labels in dataloaders[\"val\"]:\n",
    "        \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.reshape(-1,1).to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels) \n",
    "            \n",
    "            preds = outputs.clone().detach()\n",
    "          \n",
    "       \n",
    "            preds[preds<0.5]  = 0\n",
    "            preds[preds>=0.5] = 1 \n",
    "            \n",
    "          \n",
    "            targets.extend( list(labels.view(-1).numpy()))\n",
    "            predicted.extend( list(preds.view(-1).numpy()))\n",
    "            \n",
    "            \n",
    "            val_losses.append(  loss.item() )\n",
    "    \n",
    "    \n",
    "    \n",
    "        epoch_acc = accuracy_score( targets, predicted)\n",
    "        \n",
    "        if epoch_acc> best_acc:\n",
    "            best_acc = epoch_acc\n",
    "        print( f\"epoch {epoch}, mean loss = {np.mean(epoch_loss):.3}, validation loss={np.mean(val_losses):.3}, epoch acc={epoch_acc:.3}\")\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    #model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextCNN(\n",
       "  (embedding): Embedding(5002, 200)\n",
       "  (convs1): ModuleList(\n",
       "    (0): Conv2d(1, 3, kernel_size=(3, 200), stride=(1, 1))\n",
       "    (1): Conv2d(1, 3, kernel_size=(4, 200), stride=(1, 1))\n",
       "    (2): Conv2d(1, 3, kernel_size=(5, 200), stride=(1, 1))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (fc1): Linear(in_features=9, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = TextCNN(embed_num=len(word2idx))\n",
    "optimizer = torch.optim.Adam(  net.parameters())\n",
    "criterion = torch.nn.BCELoss()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/39\n",
      "----------\n",
      "epoch 0, mean loss = 0.672, validation loss=0.584, epoch acc=0.708\n",
      "Epoch 1/39\n",
      "----------\n",
      "epoch 1, mean loss = 0.591, validation loss=0.539, epoch acc=0.737\n",
      "Epoch 2/39\n",
      "----------\n",
      "epoch 2, mean loss = 0.547, validation loss=0.51, epoch acc=0.75\n",
      "Epoch 3/39\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "m = train_model(net, criterion, optimizer, dataloaders, scheduler=None, num_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in net.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
