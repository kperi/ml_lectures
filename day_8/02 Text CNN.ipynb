{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir ./data\n",
    "#!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz -O ./data/dataset.tar.gz\n",
    "#%cd data\n",
    "#!tar xvfz dataset.tar.gz\n",
    "#%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextCNN for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "from gensim.utils import tokenize, deaccent, simple_preprocess\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_train = glob.glob(\"./data/aclImdb/train/pos/*.txt\")\n",
    "negative_train = glob.glob(\"./data/aclImdb/train/neg/*.txt\")\n",
    "\n",
    "positive_test = glob.glob(\"./data/aclImdb/test/pos/*.txt\")\n",
    "negative_test = glob.glob(\"./data/aclImdb/test/neg/*.txt\")\n",
    "len(negative_test), len(positive_test), len(positive_train), len(negative_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter()\n",
    "\n",
    "for file in positive_train+negative_train+positive_test+negative_test:\n",
    "    c.update(simple_preprocess(open(file).read()))\n",
    "vocab = c.most_common(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus(object):\n",
    "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
    "    \n",
    "    def __init__(self, files):\n",
    "        self.files = files\n",
    "            \n",
    "    def __iter__(self):\n",
    "        for file in self.files:\n",
    "            \n",
    "            text = open( file ).read().lower()\n",
    "            \n",
    "            yield simple_preprocess(text)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx= { w[0]:i+2 for i, w in enumerate(vocab) }\n",
    "word2idx['<PAD>'] = 0\n",
    "word2idx['<UNK>'] = 1 \n",
    "idx2word = {i:w for w,i in word2idx.items()}\n",
    "SENTENCE_LENGTH = 80\n",
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index( word, vocab):\n",
    "    if word in vocab:\n",
    "        return vocab[word]\n",
    "    else:\n",
    "        return vocab['<UNK>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"Imbdb dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, positives, negatives, word2idx, sentence_length):\n",
    "\n",
    "        self.dataset = positives + negatives\n",
    "        self.word2idx = word2idx\n",
    "        self.labels = [1 for _ in range(len(positives))] + [0 for _ in range(len(negatives))]\n",
    "        self.sentence_length = sentence_length\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        \n",
    "        ret = torch.zeros( self.sentence_length, dtype=torch.int64)\n",
    "\n",
    "        text = open(self.dataset[idx]).read().lower()\n",
    "        s = simple_preprocess(text)\n",
    "           \n",
    "        for i in range(self.sentence_length):\n",
    "            #print(s[i] in self.word2idx)\n",
    "            ret[i]=0\n",
    "            if i<len(s):\n",
    "                ret[i] = get_index( s[i], self.word2idx)\n",
    "            \n",
    "        return ret, torch.tensor( self.labels[idx], dtype=torch.float32)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = TextDataset(positive_train, \n",
    "                            negative_train,\n",
    "                            word2idx,  sentence_length = SENTENCE_LENGTH)\n",
    "\n",
    "test_dataset = TextDataset(positive_test,\n",
    "                           negative_test, \n",
    "                           word2idx,   sentence_length=SENTENCE_LENGTH)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32,\n",
    "                        shuffle=True, num_workers=0)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32,\n",
    "                        shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "dataloaders = { \"train\": train_dataloader, \"val\":test_dataloader}\n",
    "\n",
    "dataset_sizes = { \"train\": len(train_dataset), \"val\":len(test_dataset)}\n",
    "\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_in, label = next(iter(train_dataloader))\n",
    "x_in, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim , n_hidden, \n",
    "                 num_embeddings, sentence_length, wv_model):\n",
    "        \n",
    "        super(TextClassifier, self).__init__()\n",
    "        \n",
    "        self.sentence_length = sentence_length\n",
    "        \n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout()\n",
    "\n",
    "        \n",
    "        weights = torch.FloatTensor(wv_model.wv.vectors)\n",
    "        self.embedding = nn.Embedding.from_pretrained(weights,freeze=True)\n",
    "        #print(\"req grad?=>\", self.embedding.requires_grad)\n",
    "        \n",
    "        #self.embedding.requires_grad = False\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 4, kernel_size=3, padding=1)\n",
    "        \n",
    "        \n",
    "        self.mp = nn.MaxPool2d(8, 3)\n",
    "        \n",
    "        #self.conv2 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n",
    "        #self.conv3 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        \n",
    "        \n",
    "        #self.bn1 = nn.BatchNorm2d(32)\n",
    "        #self.bn2 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.fc1 = nn.Linear( 80, n_hidden )\n",
    "        self.fc2 = nn.Linear( n_hidden, 1)\n",
    "        \n",
    "    \n",
    "    def forward(self, X):\n",
    "        batch_size = X.shape[0]\n",
    "        \n",
    "        out = self.embedding(X)\n",
    "        out = out.unsqueeze(1)\n",
    " \n",
    "        \n",
    "        out = self.conv1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.mp(out)\n",
    "       \n",
    "    \n",
    "        out = self.conv2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.mp(out)\n",
    "        \n",
    "        #print(out.shape)\n",
    "       \n",
    "        \n",
    "        #return out\n",
    "        #out = self.mp(out)\n",
    "        #out = out.view(batch_size, -1)\n",
    "        \n",
    "        \"\"\"\n",
    "        return out\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "    \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "    \n",
    "     \n",
    "        out = self.conv3(out)\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        \n",
    "        return out \n",
    "\n",
    "\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, embed_num,  embed_dim=200, class_num=1, pretrained_vectors=None, freeze=True ):\n",
    "        super(TextCNN, self).__init__()\n",
    "        \n",
    "       \n",
    "\n",
    "        V = embed_num\n",
    "        D = embed_dim\n",
    "        C = class_num\n",
    "        Ci = 1\n",
    "        Co = 3 \n",
    "        Ks = [3,4,5]\n",
    "        \n",
    "        if pretrained_vectors is not None:\n",
    "            weights = torch.FloatTensor(pretrained_vectors)\n",
    "            self.embedding = nn.Embedding.from_pretrained(weights,freeze=freeze)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(V, D)\n",
    "        \n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(len(Ks) * Co, C)\n",
    "\n",
    "    def conv_and_pool(self, x, conv):\n",
    "        x = F.relu(conv(x)).squeeze(3) \n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  \n",
    "        x = x.unsqueeze(1) \n",
    "\n",
    "        x = [F.relu(conv(x)).squeeze(3)\n",
    "             for conv in self.convs1]  \n",
    "\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2)\n",
    "             for i in x]  \n",
    "\n",
    "        x = torch.cat(x, 1)\n",
    " \n",
    "        x = self.dropout(x)   \n",
    "        out = self.fc1(x)  \n",
    "        return torch.sigmoid(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "net = TextCNN(embed_num=len(word2idx)+2)\n",
    "net(x_in).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnet = TextClassifier( n_hidden = 100, embedding_dim = 200, \\n                     num_embeddings=len(word2idx), sentence_length=SENTENCE_LENGTH,\\n                    wv_model=model)\\n\\n'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "net = TextClassifier( n_hidden = 100, embedding_dim = 200, \n",
    "                     num_embeddings=len(word2idx), sentence_length=SENTENCE_LENGTH,\n",
    "                    wv_model=model)\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, dataloaders, scheduler,  num_epochs=10):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        model.train()\n",
    "        # Iterate over data.\n",
    "        epoch_loss = []\n",
    "        for inputs, labels in dataloaders[\"train\"]:\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.reshape(-1,1).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "                    \n",
    "            loss = criterion(outputs, labels)\n",
    "             \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss.append( loss.item( ))\n",
    "        \n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        targets = [] \n",
    "        predicted  = []\n",
    "        for inputs, labels in dataloaders[\"val\"]:\n",
    "        \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.reshape(-1,1).to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels) \n",
    "            \n",
    "            preds = outputs.clone().detach()\n",
    "          \n",
    "       \n",
    "            preds[preds<0.5]  = 0\n",
    "            preds[preds>=0.5] = 1 \n",
    "            \n",
    "          \n",
    "            targets.extend( list(labels.view(-1).numpy()))\n",
    "            predicted.extend( list(preds.view(-1).numpy()))\n",
    "            \n",
    "            \n",
    "            val_losses.append(  loss.item() )\n",
    "    \n",
    "    \n",
    "    \n",
    "        epoch_acc = accuracy_score( targets, predicted)\n",
    "        \n",
    "        if epoch_acc> best_acc:\n",
    "            best_acc = epoch_acc\n",
    "        print( f\"epoch {epoch}, mean loss = {np.mean(epoch_loss):.3}, validation loss={np.mean(val_losses):.3}, epoch acc={epoch_acc:.3}\")\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    #model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = TextCNN(embed_num=len(word2idx))\n",
    "optimizer = torch.optim.Adam(  net.parameters())\n",
    "criterion = torch.nn.BCELoss()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m = train_model(net, criterion, optimizer, dataloaders, scheduler=None, num_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5002, 100)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wikipedia2vec import Wikipedia2Vec\n",
    "w2v = Wikipedia2Vec.load(\"../embeddings/enwiki_20180420_100d.pkl\")\n",
    " \n",
    "vectors= np.array([ w2v.get_word_vector( idx2word[i] ) for i in range(2,5002) ])\n",
    "pretrained_vectors = np.concatenate( [np.random.randn( 200).reshape(2,100), vectors], axis=0).astype(np.float32)\n",
    "pretrained_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextCNN(\n",
       "  (embedding): Embedding(5002, 100)\n",
       "  (convs1): ModuleList(\n",
       "    (0): Conv2d(1, 3, kernel_size=(3, 100), stride=(1, 1))\n",
       "    (1): Conv2d(1, 3, kernel_size=(4, 100), stride=(1, 1))\n",
       "    (2): Conv2d(1, 3, kernel_size=(5, 100), stride=(1, 1))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (fc1): Linear(in_features=9, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_pretrained = TextCNN(embed_num=len(word2idx), embed_dim=100, pretrained_vectors=pretrained_vectors)\n",
    "\n",
    "optimizer = torch.optim.Adam(  net_pretrained.parameters())\n",
    "criterion = torch.nn.BCELoss()\n",
    "net_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/39\n",
      "----------\n",
      "epoch 0, mean loss = 0.647, validation loss=0.569, epoch acc=0.723\n",
      "Epoch 1/39\n",
      "----------\n",
      "epoch 1, mean loss = 0.577, validation loss=0.525, epoch acc=0.741\n",
      "Epoch 2/39\n",
      "----------\n",
      "epoch 2, mean loss = 0.551, validation loss=0.509, epoch acc=0.747\n",
      "Epoch 3/39\n",
      "----------\n",
      "epoch 3, mean loss = 0.539, validation loss=0.5, epoch acc=0.753\n",
      "Epoch 4/39\n",
      "----------\n",
      "epoch 4, mean loss = 0.533, validation loss=0.496, epoch acc=0.753\n",
      "Epoch 5/39\n",
      "----------\n",
      "epoch 5, mean loss = 0.529, validation loss=0.492, epoch acc=0.76\n",
      "Epoch 6/39\n",
      "----------\n",
      "epoch 6, mean loss = 0.526, validation loss=0.492, epoch acc=0.758\n",
      "Epoch 7/39\n",
      "----------\n",
      "epoch 7, mean loss = 0.524, validation loss=0.49, epoch acc=0.762\n",
      "Epoch 8/39\n",
      "----------\n",
      "epoch 8, mean loss = 0.518, validation loss=0.487, epoch acc=0.759\n",
      "Epoch 9/39\n",
      "----------\n",
      "epoch 9, mean loss = 0.515, validation loss=0.482, epoch acc=0.757\n",
      "Epoch 10/39\n",
      "----------\n",
      "epoch 10, mean loss = 0.513, validation loss=0.479, epoch acc=0.764\n",
      "Epoch 11/39\n",
      "----------\n",
      "epoch 11, mean loss = 0.513, validation loss=0.48, epoch acc=0.765\n",
      "Epoch 12/39\n",
      "----------\n",
      "epoch 12, mean loss = 0.507, validation loss=0.476, epoch acc=0.767\n",
      "Epoch 13/39\n",
      "----------\n",
      "epoch 13, mean loss = 0.507, validation loss=0.478, epoch acc=0.767\n",
      "Epoch 14/39\n",
      "----------\n",
      "epoch 14, mean loss = 0.501, validation loss=0.481, epoch acc=0.761\n",
      "Epoch 15/39\n",
      "----------\n",
      "epoch 15, mean loss = 0.506, validation loss=0.478, epoch acc=0.768\n",
      "Epoch 16/39\n",
      "----------\n",
      "epoch 16, mean loss = 0.499, validation loss=0.476, epoch acc=0.768\n",
      "Epoch 17/39\n",
      "----------\n",
      "epoch 17, mean loss = 0.5, validation loss=0.473, epoch acc=0.769\n",
      "Epoch 18/39\n",
      "----------\n",
      "epoch 18, mean loss = 0.499, validation loss=0.474, epoch acc=0.77\n",
      "Epoch 19/39\n",
      "----------\n",
      "epoch 19, mean loss = 0.498, validation loss=0.479, epoch acc=0.767\n",
      "Epoch 20/39\n",
      "----------\n",
      "epoch 20, mean loss = 0.496, validation loss=0.476, epoch acc=0.766\n",
      "Epoch 21/39\n",
      "----------\n",
      "epoch 21, mean loss = 0.503, validation loss=0.476, epoch acc=0.768\n",
      "Epoch 22/39\n",
      "----------\n",
      "epoch 22, mean loss = 0.498, validation loss=0.469, epoch acc=0.771\n",
      "Epoch 23/39\n",
      "----------\n",
      "epoch 23, mean loss = 0.491, validation loss=0.47, epoch acc=0.768\n",
      "Epoch 24/39\n",
      "----------\n",
      "epoch 24, mean loss = 0.496, validation loss=0.475, epoch acc=0.769\n",
      "Epoch 25/39\n",
      "----------\n",
      "epoch 25, mean loss = 0.492, validation loss=0.471, epoch acc=0.766\n",
      "Epoch 26/39\n",
      "----------\n",
      "epoch 26, mean loss = 0.494, validation loss=0.474, epoch acc=0.768\n",
      "Epoch 27/39\n",
      "----------\n",
      "epoch 27, mean loss = 0.489, validation loss=0.472, epoch acc=0.768\n",
      "Epoch 28/39\n",
      "----------\n",
      "epoch 28, mean loss = 0.491, validation loss=0.469, epoch acc=0.772\n",
      "Epoch 29/39\n",
      "----------\n",
      "epoch 29, mean loss = 0.486, validation loss=0.473, epoch acc=0.772\n",
      "Epoch 30/39\n",
      "----------\n",
      "epoch 30, mean loss = 0.487, validation loss=0.474, epoch acc=0.771\n",
      "Epoch 31/39\n",
      "----------\n",
      "epoch 31, mean loss = 0.484, validation loss=0.468, epoch acc=0.77\n",
      "Epoch 32/39\n",
      "----------\n",
      "epoch 32, mean loss = 0.487, validation loss=0.472, epoch acc=0.771\n",
      "Epoch 33/39\n",
      "----------\n",
      "epoch 33, mean loss = 0.488, validation loss=0.472, epoch acc=0.771\n",
      "Epoch 34/39\n",
      "----------\n",
      "epoch 34, mean loss = 0.485, validation loss=0.465, epoch acc=0.771\n",
      "Epoch 35/39\n",
      "----------\n",
      "epoch 35, mean loss = 0.484, validation loss=0.467, epoch acc=0.77\n",
      "Epoch 36/39\n",
      "----------\n",
      "epoch 36, mean loss = 0.49, validation loss=0.474, epoch acc=0.77\n",
      "Epoch 37/39\n",
      "----------\n",
      "epoch 37, mean loss = 0.48, validation loss=0.472, epoch acc=0.764\n",
      "Epoch 38/39\n",
      "----------\n",
      "epoch 38, mean loss = 0.485, validation loss=0.473, epoch acc=0.766\n",
      "Epoch 39/39\n",
      "----------\n",
      "epoch 39, mean loss = 0.484, validation loss=0.474, epoch acc=0.769\n",
      "Training complete in 158m 41s\n",
      "Best val Acc: 0.771720\n"
     ]
    }
   ],
   "source": [
    "p_model = train_model(net_pretrained, criterion, optimizer, dataloaders, scheduler=None, num_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
