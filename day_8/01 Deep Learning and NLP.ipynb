{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir ./data\n",
    "#!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz -O ./data/dataset.tar.gz\n",
    "#%cd data~\n",
    "#!tar xvfz dataset.tar.gz\n",
    "#%cd ..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a neural net to predict sentiment using embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "from gensim.utils import tokenize, deaccent, simple_preprocess\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "import copy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12500, 12500, 12500, 12500)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_train = glob.glob(\"./data/aclImdb/train/pos/*.txt\")\n",
    "negative_train = glob.glob(\"./data/aclImdb/train/neg/*.txt\")\n",
    "\n",
    "positive_test = glob.glob(\"./data/aclImdb/test/pos/*.txt\")\n",
    "negative_test = glob.glob(\"./data/aclImdb/test/neg/*.txt\")\n",
    "len(negative_test), len(positive_test), len(positive_train), len(negative_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus(object):\n",
    "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
    "    \n",
    "    def __init__(self, files):\n",
    "        self.files = files\n",
    "            \n",
    "    def __iter__(self):\n",
    "        for file in self.files:\n",
    "            \n",
    "            text = open( file ).read().lower()\n",
    "            \n",
    "            yield simple_preprocess(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model = Word2Vec( min_count=5, workers=5, size=200) \n",
    "    sentences = MyCorpus(positive_train + negative_train )\n",
    "    model.build_vocab(sentences)\n",
    "    sentences = MyCorpus(positive_test + positive_test)\n",
    "    model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(\"./data/wv2.model\")\n",
    "model = Word2Vec.load( \"./data/wv2.model\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28698"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx= { w:i for i, w in enumerate(model.wv.vocab.keys()) }\n",
    "idx2word = {i:w for w,i in word2idx.items()}\n",
    "\n",
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"Imbdb dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, positives, negatives, word2idx, wv_model):\n",
    "\n",
    "        self.dataset = positives + negatives\n",
    "        self.word2idx = word2idx\n",
    "     \n",
    "        self.labels = [1 for _ in range(len(positives))] + [0 for _ in range(len(negatives))]\n",
    "        self.w2v = wv_model\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        text = open(self.dataset[idx]).read()\n",
    "        s = simple_preprocess(text)\n",
    "              \n",
    "        vec = [self.w2v.wv[v] for v in s if v in word2idx]\n",
    "        \n",
    "        vec1 = np.mean( vec, axis =0)\n",
    "        assert( vec1.shape[0] == 200)\n",
    "        vec2 = np.max( vec, axis =0)\n",
    "        vec3 = np.mean( vec, axis =0)\n",
    "        \n",
    "        retvec = np.concatenate( [vec1, vec2,vec3])\n",
    "            \n",
    "        return retvec, torch.tensor( self.labels[idx], dtype=torch.float32)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.30463973,  0.42246544, -0.2912863 , -0.2783561 ,  0.42620432,\n",
       "        0.05244523, -0.06872007, -0.03618291, -0.2044119 , -0.4681223 ,\n",
       "       -0.03055123, -0.2830056 ,  0.26370284,  0.09876372,  0.09896849,\n",
       "       -0.027643  ,  0.21756378,  0.49818033, -0.06463417,  0.462838  ,\n",
       "        0.06896935,  0.22945541, -0.04939801, -0.6296438 ,  0.14896312,\n",
       "        0.1400828 ,  0.44591957,  0.00647832, -0.02713466,  0.24756284,\n",
       "        0.08056816, -0.05831452,  0.07209346,  0.03072605,  0.08622102,\n",
       "       -0.05628349,  0.06678642, -0.19799863,  0.46162486,  0.09927669,\n",
       "       -0.0160488 , -0.05710422, -0.10724166,  0.2518264 , -0.02858362,\n",
       "        0.22745375, -0.14321557,  0.02267244, -0.05551359,  0.23109427,\n",
       "       -0.10249305,  0.04312664,  0.18204601,  0.11684529, -0.16727039,\n",
       "       -0.06585053,  0.20509255,  0.21890089,  0.03161347, -0.17956921,\n",
       "       -0.08080181, -0.02933438, -0.41047508, -0.08545903,  0.22769296,\n",
       "       -0.19049394, -0.03005798, -0.24399011, -0.309684  , -0.25653243,\n",
       "       -0.09553224, -0.28479016,  0.42202088, -0.07389127,  0.36288026,\n",
       "        0.24334782,  0.16105713, -0.02338331,  0.14091648,  0.08810027,\n",
       "        0.03041753,  0.12931764,  0.2622103 ,  0.00984734,  0.08560334,\n",
       "       -0.00563881,  0.26625174,  0.16415673,  0.08957987, -0.07629694,\n",
       "       -0.08239349,  0.09395914, -0.18420517,  0.03575021, -0.10488398,\n",
       "       -0.19646893, -0.00483076,  0.1215027 , -0.20057884,  0.31074488,\n",
       "       -0.35244432,  0.20342714, -0.0980064 ,  0.60724455,  0.1863698 ,\n",
       "       -0.03715466,  0.39024323, -0.06562266, -0.11366525,  0.22278841,\n",
       "        0.3777724 ,  0.296325  ,  0.01691477,  0.4800846 ,  0.3224707 ,\n",
       "        0.10998867,  0.259433  , -0.30487484, -0.2359737 , -0.02876399,\n",
       "        0.17903072, -0.09314083,  0.16581991,  0.46436375, -0.09973808,\n",
       "        0.12544109, -0.4758445 ,  0.42590356,  0.4657917 , -0.05251175,\n",
       "        0.12113349,  0.09958795,  0.37374505, -0.09755526,  0.14058064,\n",
       "        0.18144472,  0.01633996,  0.22301096,  0.07731171,  0.06560192,\n",
       "       -0.27782682, -0.13646148, -0.03567378,  0.09341276, -0.0504809 ,\n",
       "        0.11615828,  0.02659938, -0.04978845,  0.06575811, -0.44031036,\n",
       "       -0.18876094, -0.43004942,  0.15483336,  0.15233788, -0.20379698,\n",
       "       -0.05378623, -0.14285627, -0.07131957, -0.04780588,  0.23078555,\n",
       "        0.50361854,  0.09246705, -0.47921184, -0.01050873, -0.07716036,\n",
       "       -0.17858964,  0.04937929, -0.20470361, -0.06817612,  0.140243  ,\n",
       "        0.25918463, -0.07157268,  0.06370649, -0.14791709, -0.14386731,\n",
       "        0.03418973, -0.03589372, -0.04767716,  0.1827246 ,  0.20309041,\n",
       "        0.05424798, -0.23402846,  0.21299207,  0.26472092, -0.22441566,\n",
       "       -0.21177825,  0.0560461 ,  0.18135612, -0.30880198, -0.00264294,\n",
       "        0.15217692, -0.01361884,  0.10168855,  0.06423044,  0.02222304,\n",
       "        0.51771945,  0.12832156, -0.22218516,  0.19786793,  0.10763095,\n",
       "        0.12795496,  0.31872642, -2.1467688 , -1.4217589 ,  1.1948596 ,\n",
       "       -0.18748116, -1.4780478 ,  2.1234236 ,  0.        ,  0.        ,\n",
       "        0.        , -0.26433158,  0.        , -0.7554016 , -0.19038153,\n",
       "        0.92880416, -0.7461721 ,  1.4042134 , -0.20028138,  0.36804998,\n",
       "       -0.12387323, -0.36148262,  1.3233268 ,  0.        ,  0.        ,\n",
       "       -0.19659686,  0.        , -0.66637135, -0.5380037 ,  1.1826181 ,\n",
       "        0.93951833,  0.        ,  0.        ,  0.82807493,  0.00864625,\n",
       "       -0.7884793 ,  0.        , -0.40647316,  0.41964543, -0.9470315 ,\n",
       "        0.13466334,  0.        , -0.6299293 , -0.18475628,  0.4270997 ,\n",
       "        1.1074741 ,  0.67877674,  0.14396214,  0.31397343, -0.32035875,\n",
       "        0.3963306 ,  0.30347395, -0.20917773,  0.28069162,  0.5221853 ,\n",
       "       -1.6571715 , -0.21153772,  0.08073008,  0.        , -0.28566456,\n",
       "        0.13792157, -0.4901507 , -0.03034616,  0.79088616,  0.15577424,\n",
       "       -0.5675659 , -0.19256973, -1.6853452 , -0.41739988,  0.        ,\n",
       "        0.0056355 ,  0.        ,  0.67092216,  0.        ,  1.6210833 ,\n",
       "       -0.64395   ,  0.73156   ,  0.        ,  0.        , -0.9488013 ,\n",
       "        1.5777688 , -0.40551233, -0.39915144, -0.7063134 ,  0.88285613,\n",
       "        0.5668669 ,  0.        ,  1.051264  , -0.1174531 , -0.6380789 ,\n",
       "        0.4604243 ,  0.4421847 , -0.23211217, -0.45682693, -0.04276466,\n",
       "        0.32809114,  0.6616223 , -1.1540229 , -0.9668658 ,  0.        ,\n",
       "       -0.45945096,  0.        ,  0.        ,  0.47934675,  0.401088  ,\n",
       "        0.54434276, -0.02882028,  1.0592592 ,  0.5690975 ,  0.0779388 ,\n",
       "       -0.8238654 ,  0.0880692 ,  0.11391234,  0.29673326,  0.24929357,\n",
       "        0.07212877, -1.0138521 , -0.8517196 , -0.28159404,  0.2758541 ,\n",
       "        0.        ,  0.        ,  0.9205675 ,  0.9541775 ,  0.        ,\n",
       "        0.3902645 , -0.3012247 ,  0.27462173,  0.7435694 , -0.3241408 ,\n",
       "        0.07467532,  0.12773299,  0.18254662,  0.2816999 , -0.63890684,\n",
       "        0.        ,  0.66549134, -1.238574  ,  0.11626768,  0.        ,\n",
       "        0.94410086, -0.21274328, -0.17827702,  0.        ,  0.05290055,\n",
       "       -0.06394339,  0.        ,  1.0252392 , -0.29406023, -0.19101739,\n",
       "       -0.6724088 , -0.82314444,  0.84558964, -0.87584114,  0.        ,\n",
       "       -0.690923  , -0.28887653, -0.23924637,  0.17669082,  0.        ,\n",
       "        0.        ,  1.0581563 , -1.4751484 ,  1.5523365 , -0.10102367,\n",
       "        0.27753377,  0.16826797, -0.4751892 , -0.33739614, -0.57025814,\n",
       "        0.        ,  0.22913194,  0.        , -0.18850875, -1.3712296 ,\n",
       "        0.        , -0.13552713, -0.09985852,  0.14009666, -0.3732133 ,\n",
       "        0.07476044,  0.30382586, -0.16107774,  1.0620489 , -0.6715634 ,\n",
       "       -0.35770798,  0.        ,  0.        ,  1.0438871 , -0.960745  ,\n",
       "        0.01319098,  0.        ,  0.5100012 ,  0.26801562,  0.5208771 ,\n",
       "        0.9575677 ,  0.57451904,  0.6894276 ,  0.        ,  0.43524075,\n",
       "       -0.30463973,  0.42246544, -0.2912863 , -0.2783561 ,  0.42620432,\n",
       "        0.05244523, -0.06872007, -0.03618291, -0.2044119 , -0.4681223 ,\n",
       "       -0.03055123, -0.2830056 ,  0.26370284,  0.09876372,  0.09896849,\n",
       "       -0.027643  ,  0.21756378,  0.49818033, -0.06463417,  0.462838  ,\n",
       "        0.06896935,  0.22945541, -0.04939801, -0.6296438 ,  0.14896312,\n",
       "        0.1400828 ,  0.44591957,  0.00647832, -0.02713466,  0.24756284,\n",
       "        0.08056816, -0.05831452,  0.07209346,  0.03072605,  0.08622102,\n",
       "       -0.05628349,  0.06678642, -0.19799863,  0.46162486,  0.09927669,\n",
       "       -0.0160488 , -0.05710422, -0.10724166,  0.2518264 , -0.02858362,\n",
       "        0.22745375, -0.14321557,  0.02267244, -0.05551359,  0.23109427,\n",
       "       -0.10249305,  0.04312664,  0.18204601,  0.11684529, -0.16727039,\n",
       "       -0.06585053,  0.20509255,  0.21890089,  0.03161347, -0.17956921,\n",
       "       -0.08080181, -0.02933438, -0.41047508, -0.08545903,  0.22769296,\n",
       "       -0.19049394, -0.03005798, -0.24399011, -0.309684  , -0.25653243,\n",
       "       -0.09553224, -0.28479016,  0.42202088, -0.07389127,  0.36288026,\n",
       "        0.24334782,  0.16105713, -0.02338331,  0.14091648,  0.08810027,\n",
       "        0.03041753,  0.12931764,  0.2622103 ,  0.00984734,  0.08560334,\n",
       "       -0.00563881,  0.26625174,  0.16415673,  0.08957987, -0.07629694,\n",
       "       -0.08239349,  0.09395914, -0.18420517,  0.03575021, -0.10488398,\n",
       "       -0.19646893, -0.00483076,  0.1215027 , -0.20057884,  0.31074488,\n",
       "       -0.35244432,  0.20342714, -0.0980064 ,  0.60724455,  0.1863698 ,\n",
       "       -0.03715466,  0.39024323, -0.06562266, -0.11366525,  0.22278841,\n",
       "        0.3777724 ,  0.296325  ,  0.01691477,  0.4800846 ,  0.3224707 ,\n",
       "        0.10998867,  0.259433  , -0.30487484, -0.2359737 , -0.02876399,\n",
       "        0.17903072, -0.09314083,  0.16581991,  0.46436375, -0.09973808,\n",
       "        0.12544109, -0.4758445 ,  0.42590356,  0.4657917 , -0.05251175,\n",
       "        0.12113349,  0.09958795,  0.37374505, -0.09755526,  0.14058064,\n",
       "        0.18144472,  0.01633996,  0.22301096,  0.07731171,  0.06560192,\n",
       "       -0.27782682, -0.13646148, -0.03567378,  0.09341276, -0.0504809 ,\n",
       "        0.11615828,  0.02659938, -0.04978845,  0.06575811, -0.44031036,\n",
       "       -0.18876094, -0.43004942,  0.15483336,  0.15233788, -0.20379698,\n",
       "       -0.05378623, -0.14285627, -0.07131957, -0.04780588,  0.23078555,\n",
       "        0.50361854,  0.09246705, -0.47921184, -0.01050873, -0.07716036,\n",
       "       -0.17858964,  0.04937929, -0.20470361, -0.06817612,  0.140243  ,\n",
       "        0.25918463, -0.07157268,  0.06370649, -0.14791709, -0.14386731,\n",
       "        0.03418973, -0.03589372, -0.04767716,  0.1827246 ,  0.20309041,\n",
       "        0.05424798, -0.23402846,  0.21299207,  0.26472092, -0.22441566,\n",
       "       -0.21177825,  0.0560461 ,  0.18135612, -0.30880198, -0.00264294,\n",
       "        0.15217692, -0.01361884,  0.10168855,  0.06423044,  0.02222304,\n",
       "        0.51771945,  0.12832156, -0.22218516,  0.19786793,  0.10763095],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_dataset = TextDataset(positive_train[0:500],\n",
    "                            negative_train[0:500],\n",
    "                            word2idx, wv_model=model)\n",
    "\n",
    "test_dataset = TextDataset(positive_test[0:500],\n",
    "                           negative_test[0:500],\n",
    "                           word2idx,  wv_model=model)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128,\n",
    "                        shuffle=True, num_workers=0)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128,\n",
    "                        shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "dataloaders = { \"train\": train_dataloader, \"val\":test_dataloader}\n",
    "\n",
    "dataset_sizes = { \"train\": len(train_dataset), \"val\":len(test_dataset)}\n",
    "\n",
    "train_dataset[0][0]-train_dataset[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 1000, 'val': 1000}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNClassifier(nn.Module):\n",
    "    def __init__(self, D, n_hidden):\n",
    "        super(NNClassifier, self).__init__()\n",
    "        \n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout()\n",
    "        self.fc1 = nn.Linear( 3*D, n_hidden )\n",
    "        self.fc2 = nn.Linear( n_hidden, 1)\n",
    "        \n",
    "    \n",
    "    def forward(self, X):\n",
    "        batch_size = X.shape[0]\n",
    "        out = self.fc1(X)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        ###\n",
    "        out = self.fc2(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, dataloaders, scheduler,  num_epochs=10):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "            # Iterate over data.\n",
    "        epoch_loss = []\n",
    "        for inputs, labels in dataloaders[\"train\"]:\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.reshape(-1,1).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "                    \n",
    "         \n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss.append( loss.item( ))\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        val_losses = []\n",
    "        targets = [] \n",
    "        predicted  = []\n",
    "        for inputs, labels in dataloaders[\"val\"]:\n",
    "        \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.reshape(-1,1).to(device)\n",
    "            \n",
    "            outputs  = model(inputs)\n",
    "            loss = criterion(outputs, labels) \n",
    "            \n",
    "            preds = outputs.clone().detach()\n",
    "            preds[preds<0.5]  = 0\n",
    "            preds[preds>=0.5] = 1 \n",
    "            \n",
    "             \n",
    "            \n",
    "            targets.extend( list(labels.view(-1).numpy()))\n",
    "            predicted.extend( list(preds.view(-1).numpy()))\n",
    "            \n",
    "            \n",
    "            val_losses.append(  loss.item() )\n",
    "        epoch_acc = accuracy_score( targets, predicted)\n",
    "            \n",
    "            \n",
    "        if epoch_acc> best_acc:\n",
    "            best_acc = epoch_acc\n",
    "        print( f\"epoch {epoch}, mean loss = {np.mean(epoch_loss):.3}, validation loss={np.mean(val_losses):.3}, epoch acc={epoch_acc:.3}\")\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    #model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n",
      "epoch 0, mean loss = 0.709, validation loss=0.685, epoch acc=0.509\n",
      "Epoch 1/49\n",
      "----------\n",
      "epoch 1, mean loss = 0.688, validation loss=0.677, epoch acc=0.548\n",
      "Epoch 2/49\n",
      "----------\n",
      "epoch 2, mean loss = 0.68, validation loss=0.667, epoch acc=0.649\n",
      "Epoch 3/49\n",
      "----------\n",
      "epoch 3, mean loss = 0.667, validation loss=0.661, epoch acc=0.604\n",
      "Epoch 4/49\n",
      "----------\n",
      "epoch 4, mean loss = 0.651, validation loss=0.652, epoch acc=0.638\n",
      "Epoch 5/49\n",
      "----------\n",
      "epoch 5, mean loss = 0.645, validation loss=0.64, epoch acc=0.665\n",
      "Epoch 6/49\n",
      "----------\n",
      "epoch 6, mean loss = 0.64, validation loss=0.639, epoch acc=0.644\n",
      "Epoch 7/49\n",
      "----------\n",
      "epoch 7, mean loss = 0.632, validation loss=0.639, epoch acc=0.622\n",
      "Epoch 8/49\n",
      "----------\n",
      "epoch 8, mean loss = 0.622, validation loss=0.64, epoch acc=0.619\n",
      "Epoch 9/49\n",
      "----------\n",
      "epoch 9, mean loss = 0.62, validation loss=0.615, epoch acc=0.682\n",
      "Epoch 10/49\n",
      "----------\n",
      "epoch 10, mean loss = 0.589, validation loss=0.603, epoch acc=0.686\n",
      "Epoch 11/49\n",
      "----------\n",
      "epoch 11, mean loss = 0.598, validation loss=0.61, epoch acc=0.681\n",
      "Epoch 12/49\n",
      "----------\n",
      "epoch 12, mean loss = 0.592, validation loss=0.591, epoch acc=0.699\n",
      "Epoch 13/49\n",
      "----------\n",
      "epoch 13, mean loss = 0.567, validation loss=0.6, epoch acc=0.675\n",
      "Epoch 14/49\n",
      "----------\n",
      "epoch 14, mean loss = 0.581, validation loss=0.578, epoch acc=0.704\n",
      "Epoch 15/49\n",
      "----------\n",
      "epoch 15, mean loss = 0.572, validation loss=0.634, epoch acc=0.635\n",
      "Epoch 16/49\n",
      "----------\n",
      "epoch 16, mean loss = 0.571, validation loss=0.569, epoch acc=0.714\n",
      "Epoch 17/49\n",
      "----------\n",
      "epoch 17, mean loss = 0.559, validation loss=0.563, epoch acc=0.719\n",
      "Epoch 18/49\n",
      "----------\n",
      "epoch 18, mean loss = 0.539, validation loss=0.56, epoch acc=0.725\n",
      "Epoch 19/49\n",
      "----------\n",
      "epoch 19, mean loss = 0.531, validation loss=0.579, epoch acc=0.673\n",
      "Epoch 20/49\n",
      "----------\n",
      "epoch 20, mean loss = 0.546, validation loss=0.557, epoch acc=0.716\n",
      "Epoch 21/49\n",
      "----------\n",
      "epoch 21, mean loss = 0.535, validation loss=0.549, epoch acc=0.722\n",
      "Epoch 22/49\n",
      "----------\n",
      "epoch 22, mean loss = 0.542, validation loss=0.582, epoch acc=0.691\n",
      "Epoch 23/49\n",
      "----------\n",
      "epoch 23, mean loss = 0.529, validation loss=0.542, epoch acc=0.73\n",
      "Epoch 24/49\n",
      "----------\n",
      "epoch 24, mean loss = 0.51, validation loss=0.541, epoch acc=0.725\n",
      "Epoch 25/49\n",
      "----------\n",
      "epoch 25, mean loss = 0.503, validation loss=0.529, epoch acc=0.735\n",
      "Epoch 26/49\n",
      "----------\n",
      "epoch 26, mean loss = 0.507, validation loss=0.548, epoch acc=0.72\n",
      "Epoch 27/49\n",
      "----------\n",
      "epoch 27, mean loss = 0.498, validation loss=0.527, epoch acc=0.732\n",
      "Epoch 28/49\n",
      "----------\n",
      "epoch 28, mean loss = 0.491, validation loss=0.527, epoch acc=0.734\n",
      "Epoch 29/49\n",
      "----------\n",
      "epoch 29, mean loss = 0.492, validation loss=0.522, epoch acc=0.735\n",
      "Epoch 30/49\n",
      "----------\n",
      "epoch 30, mean loss = 0.478, validation loss=0.516, epoch acc=0.735\n",
      "Epoch 31/49\n",
      "----------\n",
      "epoch 31, mean loss = 0.481, validation loss=0.518, epoch acc=0.735\n",
      "Epoch 32/49\n",
      "----------\n",
      "epoch 32, mean loss = 0.477, validation loss=0.531, epoch acc=0.726\n",
      "Epoch 33/49\n",
      "----------\n",
      "epoch 33, mean loss = 0.481, validation loss=0.516, epoch acc=0.737\n",
      "Epoch 34/49\n",
      "----------\n",
      "epoch 34, mean loss = 0.482, validation loss=0.526, epoch acc=0.727\n",
      "Epoch 35/49\n",
      "----------\n",
      "epoch 35, mean loss = 0.476, validation loss=0.571, epoch acc=0.702\n",
      "Epoch 36/49\n",
      "----------\n",
      "epoch 36, mean loss = 0.486, validation loss=0.513, epoch acc=0.736\n",
      "Epoch 37/49\n",
      "----------\n",
      "epoch 37, mean loss = 0.458, validation loss=0.514, epoch acc=0.747\n",
      "Epoch 38/49\n",
      "----------\n",
      "epoch 38, mean loss = 0.465, validation loss=0.52, epoch acc=0.745\n",
      "Epoch 39/49\n",
      "----------\n",
      "epoch 39, mean loss = 0.472, validation loss=0.51, epoch acc=0.746\n",
      "Epoch 40/49\n",
      "----------\n",
      "epoch 40, mean loss = 0.454, validation loss=0.509, epoch acc=0.737\n",
      "Epoch 41/49\n",
      "----------\n",
      "epoch 41, mean loss = 0.465, validation loss=0.511, epoch acc=0.74\n",
      "Epoch 42/49\n",
      "----------\n",
      "epoch 42, mean loss = 0.455, validation loss=0.5, epoch acc=0.747\n",
      "Epoch 43/49\n",
      "----------\n",
      "epoch 43, mean loss = 0.433, validation loss=0.508, epoch acc=0.75\n",
      "Epoch 44/49\n",
      "----------\n",
      "epoch 44, mean loss = 0.437, validation loss=0.503, epoch acc=0.743\n",
      "Epoch 45/49\n",
      "----------\n",
      "epoch 45, mean loss = 0.44, validation loss=0.501, epoch acc=0.748\n",
      "Epoch 46/49\n",
      "----------\n",
      "epoch 46, mean loss = 0.442, validation loss=0.522, epoch acc=0.741\n",
      "Epoch 47/49\n",
      "----------\n",
      "epoch 47, mean loss = 0.443, validation loss=0.544, epoch acc=0.717\n",
      "Epoch 48/49\n",
      "----------\n",
      "epoch 48, mean loss = 0.452, validation loss=0.522, epoch acc=0.747\n",
      "Epoch 49/49\n",
      "----------\n",
      "epoch 49, mean loss = 0.451, validation loss=0.514, epoch acc=0.748\n",
      "Training complete in 2m 37s\n",
      "Best val Acc: 0.750000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NNClassifier(\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (fc1): Linear(in_features=600, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = NNClassifier(D=200, n_hidden = 100)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam( net.parameters())# lr=0.1)\n",
    "\n",
    "train_model(net, criterion, optimizer, dataloaders, scheduler=None, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
