{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1X-N86Y207b8",
        "outputId": "ec57bfef-40d2-43fe-affd-73c534c8d954"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/84/7bc03215279f603125d844bf81c3fb3f2d50fe8e511546eb4897e4be2067/transformers-4.0.0-py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 7.8MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 29.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 47.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=2b97fe5528f249434528532317d642a3ac294d6bbcbf56e0acc10e13b82492cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRYOOzUS28xt"
      },
      "source": [
        "from pathlib import Path"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0zAVOW-16jI"
      },
      "source": [
        ""
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "vcM2zc-S3urO",
        "outputId": "da088b76-5b21-45d2-ebf1-85aa8c1bcc5f"
      },
      "source": [
        ""
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What could be more schlocky than the idea of p...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"Film noir\" is an overused expression when it ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I don't know how or why this film has a meager...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I used to watch this show when I was a little ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Its no surprise that Busey later developed a t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0  What could be more schlocky than the idea of p...      1\n",
              "1  \"Film noir\" is an overused expression when it ...      1\n",
              "2  I don't know how or why this film has a meager...      1\n",
              "3  I used to watch this show when I was a little ...      1\n",
              "4  Its no surprise that Busey later developed a t...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAenW7T_13XC"
      },
      "source": [
        "#!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar xfz aclImdb_v1.tar.gz"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YduK2NP20Ls5"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import unicodedata\n",
        "import torch\n",
        "from transformers import *\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertModel, BertConfig\n",
        "from torch import cuda\n",
        "\n",
        "from sklearn.svm import OneClassSVM\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "\n",
        "import json\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgtyJcQV0y8T"
      },
      "source": [
        "class BertEmbedding(torch.nn.Module):\n",
        "    def __init__( self ):\n",
        "        super(BertEmbedding, self).__init__()\n",
        "        self.l1 = AutoModel.from_pretrained(\"bert-base-uncased\",  output_hidden_states=True)\n",
        "\n",
        "    def forward(self, ids,mask, token_type_ids):\n",
        "        out_0, output_1, hidden_states = self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
        "        embd = torch.cat([hidden_states[i] for i in [-1,-2,-3,-4]], dim=-1).max(dim=1)[0]\n",
        "        return embd\n",
        "    \n",
        "\n",
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTClass, self).__init__()\n",
        "        self.l1 = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.l2 = torch.nn.Dropout(0.3)\n",
        "        self.l3 = torch.nn.Linear(768, 1)\n",
        "    \n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
        "        \n",
        "        output_2 = self.l2(output_1['pooler_output'])\n",
        "        output = self.l3(output_2)\n",
        "        return output\n"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtHklwos1Vr1"
      },
      "source": [
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "model = BERTClass()\n",
        "_ = model.to(device)\n",
        "_ = model.eval()\n"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA4hnorJ1qgL"
      },
      "source": [
        "class TextDataset(Dataset):\n",
        "    \"\"\"\n",
        "      Text dataset loader: Load data and feed 'em to BERT \n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe.text\n",
        "        self.targets = self.data.label\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True,\n",
        "            padding = \"max_length\"\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
        "        }\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r--bcor94eV8"
      },
      "source": [
        "pos_files = Path(\"/content/aclImdb/train/pos\").rglob(\"*.txt\")\n",
        "#len( list(pos_files))\n",
        "\n",
        "neg_files = Path(\"/content/aclImdb/train/neg\").rglob(\"*.txt\")\n",
        "#len( list(neg_files))\n",
        "\n",
        "data = [] \n",
        "for file in pos_files:\n",
        "  data.append( ( open(file).read(), 1))\n",
        "\n",
        "for file in neg_files:\n",
        "  data.append( ( open(file).read(), 0))\n",
        "train_dataset = pd.DataFrame( data=data, columns = [\"text\", \"label\"])\n",
        "\n",
        "\n",
        "\n",
        "data = [] \n",
        "pos_files = Path(\"/content/aclImdb/test/pos\").rglob(\"*.txt\")\n",
        "#len( list(pos_files))\n",
        "\n",
        "neg_files = Path(\"/content/aclImdb/test/neg\").rglob(\"*.txt\")\n",
        "#len( list(neg_files))\n",
        "\n",
        "data = [] \n",
        "for file in pos_files:\n",
        "  data.append( ( open(file).read(), 1))\n",
        "\n",
        "for file in neg_files:\n",
        "  data.append( ( open(file).read(), 0))\n",
        "test_dataset = pd.DataFrame( data=data, columns = [\"text\", \"label\"])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFvcIAHp545_",
        "outputId": "0068f24d-ca5c-40f0-8d94-b625cc7c6c04"
      },
      "source": [
        "training_set[0]"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ids': tensor([  101,  2054,  2071,  2022,  2062,  8040,  7317,  7432,  2100,  2084,\n",
              "          1996,  2801,  1997,  2797, 18145,  2893,  2920,  2007,  1996,  2308,\n",
              "          2027,  1005,  2128,  4011,  2000,  2022, 22624,  2006,  1029,  1998,\n",
              "          2087,  1997,  1996,  7982,  2004,  2517,  2003,  6669,  7221,  2389,\n",
              "          1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  2021,\n",
              "          1996,  5889,  2735,  1996, 13764,  8649,  2046,  2242,  2008,  3084,\n",
              "          3168,  1012,  2017,  2064,  2156,  2613,  2111,  2369,  1996,  4895,\n",
              "         22852,  3210,  1012,  1998,  1996,  9855,  2003,  6919,  1012,  2169,\n",
              "          3496,  2515,  2074,  2054,  2009,  2038,  2000,  1998,  4515,  2302,\n",
              "         11920,  2006,  2205,  2146,  1012,  1026,  7987,  1013,  1028,   102]),\n",
              " 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1]),\n",
              " 'targets': tensor(1.),\n",
              " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yclVXbm41sE"
      },
      "source": [
        "MAX_LEN = 100\n",
        "training_set = TextDataset(train_dataset, tokenizer, MAX_LEN)\n",
        "testing_set = TextDataset(test_dataset, tokenizer, MAX_LEN)"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvqxRugB1Wkt"
      },
      "source": [
        "TRAIN_BATCH_SIZE = 16\n",
        "VALID_BATCH_SIZE = 16\n",
        "\n",
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apXGnLot6Ubp",
        "outputId": "45be9dfb-a4c4-44e2-85f7-26850f2cf1d9"
      },
      "source": [
        "next(iter(training_loader))"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ids': tensor([[  101,  3398,  1012,  ..., 20773,  4009,   102],\n",
              "         [  101,  1045,  3728,  ...,  3185,  1998,   102],\n",
              "         [  101,  2202,  1000,  ...,  2008,  2038,   102],\n",
              "         ...,\n",
              "         [  101,  2195,  2086,  ..., 11615,  7330,   102],\n",
              "         [  101,  2017,  2064,  ...,  2143,  2000,   102],\n",
              "         [  101,  1037,  2843,  ..., 23277,  5654,   102]]),\n",
              " 'mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
              "         [1, 1, 1,  ..., 1, 1, 1],\n",
              "         [1, 1, 1,  ..., 1, 1, 1],\n",
              "         ...,\n",
              "         [1, 1, 1,  ..., 1, 1, 1],\n",
              "         [1, 1, 1,  ..., 1, 1, 1],\n",
              "         [1, 1, 1,  ..., 1, 1, 1]]),\n",
              " 'targets': tensor([0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.]),\n",
              " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0]])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6bKwa9z4R6P"
      },
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    for _,data in enumerate(training_loader, 0):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        targets = targets.reshape( targets.shape[0], 1 )\n",
        "        outputs = outputs.reshape( targets.shape[0], 1 )\n",
        "        loss = criterion(outputs, targets)\n",
        "        if _%5000==0:\n",
        "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "def validation(testing_loader):\n",
        "    model.eval()\n",
        "    fin_targets=[]\n",
        "    fin_outputs=[]\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(testing_loader, 0):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "    return fin_outputs, fin_targets\n"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bezVHNdw5Duy"
      },
      "source": [
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        " "
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccAMwt1p5KX8",
        "outputId": "4b21f352-1341-4b8b-9810-fc0bb6694380"
      },
      "source": [
        "LEARNING_RATE = 1e-05\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE) \n",
        "\n",
        "data = next(iter(training_loader))\n",
        "ids = data['ids'].to(device, dtype = torch.long)\n",
        "mask = data['mask'].to(device, dtype = torch.long)\n",
        "token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "\n",
        "m = AutoModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
        "\n",
        "outputs = model(ids, mask, token_type_ids)\n",
        "#\n",
        "#targets = targets.reshape( targets.shape[0] )\n",
        "#outputs = outputs.reshape( outputs.shape[0] )\n",
        "\n",
        "#outputs['pooler_output'].shape"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdtS7Tqx5QZM",
        "outputId": "f7ece2b6-d268-4ad3-892e-784983187c6d"
      },
      "source": [
        "EPOCHS = 2\n",
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)\n",
        "\n",
        "\n",
        "    outputs, targets = validation(testing_loader)\n",
        "    outputs = np.array(outputs) >= 0.5\n",
        "    accuracy = metrics.accuracy_score(targets, outputs)\n",
        "    f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
        "    f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
        "    print(f\"Accuracy Score = {accuracy}\")\n",
        "    print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
        "    print(f\"F1 Score (Macro) = {f1_score_macro}\")"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss:  0.7443519830703735\n",
            "Accuracy Score = 0.86444\n",
            "F1 Score (Micro) = 0.86444\n",
            "F1 Score (Macro) = 0.8642257919647056\n",
            "Epoch: 1, Loss:  0.1843758225440979\n",
            "Accuracy Score = 0.86796\n",
            "F1 Score (Micro) = 0.86796\n",
            "F1 Score (Macro) = 0.8677394018076157\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeD8-pHm5d5b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}