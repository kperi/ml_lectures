{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Scrum is awful for data science\n",
    "\n",
    "\n",
    "Scrum is a popular methodology for PM in software engineering and recently the trend has carried over to data science. While the utility of Scrum in standard software engineering may remain up for debate, here I will detail why it has unquestionably no place in data science (and data engineering as well). This is not to say that “Agile” as a whole is bad for data science, but rather that the specific principles of Scrum: sprints, single product owner, scrum master, daily stand-ups (and the litany of other meetings) fit poorly for data science teams and ultimately result in poorer products.\n",
    "The Sprint/Estimation\n",
    "\n",
    "\n",
    "Scrum prioritizes creating “deliverables” often in two-week sprints. While this might arguably work well for certain areas of software engineering, it fails spectacularly in the data science world. Data Science by its very nature is a scientific process and involves, research, experimentation, and analysis. Data Science projects are very difficult to estimate because many times they are asking the team to do something that hasn’t been done before. While it is true that data scientists may have designed similar models before they likely haven’t leveraged the dataset or utilized the specific technique required. This means that there is a lot of uncertainty in the process. Things like poorer than expected data quality, problems with hyper-parameter tuning, and/or a technique just not working can cause a failure to “deliver” by the end of the sprint. This means that point estimates are often less than worthless as they are based on prior projects that often don’t bear resemblance to the current project in progress.\n",
    "\n",
    "\n",
    "Moreover, even if data scientists “deliver” the required items for a sprint in many cases they have likely sacrificed code quality, model robustness, or documentation in order to meet the arbitrary end of the sprint. I have often heard management describe positively the benefits of “more urgency” in a two-week sprint. But remember this urgency also has drawbacks chiefly that data scientists are more likely to make mistakes and overlook things.\n",
    "\n",
    "\n",
    "On the opposite end of the spectrum, I’ve seen data scientists who finished their work early, hesitant to pull in new stories out of fear of not being able to complete them by the end of sprint. Therefore, they just sit idly for several days before the next sprint.\n",
    "\n",
    "\n",
    "But couldn’t we break up these big tasks? Proponents of Scrum will argue that the issue here is not Scrum, but just the need to better break up big tasks (likely with additional time consuming grooming sessions). However, even breaking up big tasks does not remove the uncertainty with data science. For instance, a task like train an XGBoost model and report results, might take much longer than a single sprint due to missing values in the code that need to be encoded or needed data not being present at all. Yes, this could be addressed by having prior story “explore the dataset and fill missing values” but as I will describe in a second most PO’s lack the expertise to prioritize these types of stories as it doesn’t fulfill an immediate deliverable.\n",
    "\n",
    "\n",
    "Constant Pivoting\n",
    "\n",
    "\n",
    "Related to the above point, Scrum often results in constant pivoting from one project to another. True, many consider pivoting a “desirable” trait, however this constant change in direction often results in nothing getting done and promising projects being shelved simply because they aren’t producing immediate deliverables. This is particularly true in data science where many projects require a long-term investment in both employee time and resources. I have seen many times where promising projects were discontinued because they didn’t deliver performance improvements fast enough or the product owner just saw something flashier, they wanted to focus on.\n",
    "Lack of cross team pollination\n",
    "\n",
    "\n",
    "Scrum often creates a horribly narrow focus on one’s own team’s sprint tickets to the exclusion of everything else. It discourages data scientists (or really anyone) from contributing to other initiatives around their company; other initiatives where their skills could potentially be of help. It also has the tendency to push off important issues that could affect other teams unless that team is in direct contact with the product owner.\n",
    "\n",
    "\n",
    "\n",
    "### The role of the product owner (PO)\n",
    "\n",
    "\n",
    "Another key problem of Scrum is that it places too much power in the hands of the PO. The PO is generally in charge of the backlog and determines which issues need to be prioritized. However, product owners generally have a poor understanding of the technical nuances of data science projects. Therefore, needed work such as refactoring of code or further analysis of model performance often gets pushed to the back. Additionally, lack of immediate “progress” might result in the product owner moving away from a project entirely. This isn’t to say that data scientists shouldn’t regularly communicate with stakeholders to determine the priority of tickets, but rather than having a dedicated product owner at all the meetings and deciding the priority of tasks is counterproductive both to the team and long term to the product itself.\n",
    "\n",
    "\n",
    "### Daily Standups, grooming and other wastes of time\n",
    "\n",
    "I’ve seen very few if any teams that need to meet on a daily basis. Communication between teammates is important, however, usually twice per week or three times will more than suffice. Likewise, teammates should be encouraged to reach out if they get blocked or need help. However, a daily standup often does nothing but micro-manage employees.\n",
    "\n",
    "Grooming (or refinement) is a meeting of the Scrum team in which the product backlog items are discussed, and the next sprint planning is prepared.\n",
    "Grooming is another session that needlessly wastes time. As I mentioned above, technical complexity in data science often means that sprint goals will often not be met or met with subpar results. This in turn often results in the justification for even more grooming meetings (or pre-grooming as we used to call them) in order to “break down those big issues.” In a never-ending cycle these meetings continue to eat up more and more data scientist time.\n",
    "\n",
    "The retro is one of the few scrum meetings that I like, however, suggestions at these meetings are often not taken seriously. For instance, at several sprint retros I’ve attended during my career, the majority of teammates recommended not having a daily stand-up, but the scrum master and management discounted these suggestions because “that would not be scrum.” However, in contrast, suggestions for adding more grooming sessions are almost always enacted without question.\n",
    "\n",
    "### The Scrum Master\n",
    "\n",
    "Another role that is essentially useless is the scrum master. the definition of a scrum master formally is:\n",
    "The scrum master is the team role responsible for ensuring the team lives agile values and principles and follows the processes and practices that the team agreed they would use.”- Agile Alliance\n",
    "\n",
    "\n",
    "What…? In practice, the scrum master acts as a non-technical busy body who coerces team members into attending the aforementioned pointless meetings and drags around JIRA cards, while preaching the canon of how Scrum will lead your team to salvation (e.g. more points delivered per sprint).\n",
    "\n",
    "False Dichotomy and the no true Scrum argument\n",
    "\n",
    "Finally, proponents of Scrum often create a straw man in comparing Scrum to waterfall and other older project management methods. Moreover, in many companies, management takes an all or nothing approach. It is possible to take aspects of Scrum, Agile or other forms of project management without adhering to them completely. For instance, you could utilize ideas like stories, epics, etc, without a product owner, sprints, or a scrum master.\n",
    "\n",
    "Another common trend I see frequently is for people to say “what you experienced was not true Scrum, blah blah is actually waterfall. If only you had a better product owner…” What this fails to realize is that Scrum as a system breeds these types of problems. Delegating a singular role as the product owner is bound to cause problems. Sure you could have an exceptionally good PO that has years of DS experience or understands the team, but that likely won’t be the case. Moreover, the sprint at its core encourages frantic rushing at the end of whatever arbitrarily decided duration in order to meet the “commitment.” It also fundamentally assumes that all work can be concretely estimated. Scrum could possibly work in areas of software engineering where there is very well defined problems that are only slight variations (though even then you have the issue with POs and the Scrum masters). However, when there is any uncertainty (like there is in data science, data engineering and Devops) Scrum breakdowns and results in both wasted time and resources.\n",
    "What should you use instead?\n",
    "\n",
    "This leads to a central question of how you should manage a data science team. There is no single answer. What I’ve found to work well is a Kanban based approach without a product owner but regular discussions with stakeholders (weekly or every other week). Additionally, work in progress limits seem to help streamline the process. Like I mentioned above, meetings twice per week (Tuesday/Friday) or some alternative often work well.\n",
    "\n",
    "However, this approach may not work well for all teams. That is why, particularly for data science, I’d recommend trying out many different approaches to determine what works well for your team. The key is to find the system that works well for your team and stakeholders rather than one that just placates upper management’s ideas of how a data scientist team should operate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structuring DS teams\n",
    "\n",
    "In a modern data driven enterprise there are three functions a data scientist can work on: data collection, data driven product or data driven prediction. Each of their tasks uniquely fall into one and only one of these categories. I will use the case of AlphaGo to describe why this is, because its pure structure provides a useful thought vehicle. \n",
    "\n",
    "AlphaGo implements a Mathematical concept called “Markov Decision Process” (MDP) to do its job and crush Go players around the world. Technical details are not relevant here, it is essentially the science of taking action to achieve one’s goals. An MDP like AlphaGo consists of three main components: State, Policy and Reward. These overlap with the three aforementioned parts of a data driven enterprise:\n",
    "\n",
    "State (data collection)\n",
    "This is the part that describes the world with data, anything that can be of any relevance downstream must be described here. AlphaGo has a pretty easy job here as its entire world is a 19x19 grid with some black and white stones. A company on the contrary faces an incredibly difficult task here describing the messiness of the real world with users, environment, competition and hard to describe human concepts (e.g.: sentiment analysis). \n",
    "\n",
    "Data Science in this role performs an explanatory function. What are the available and recordable aspects of the world that can help downstream functions. What logs should be recorded and how they should be transformed. If a KPI need to be calculated what it should be so it is clear and understandable and its changes actionable. What information about a user should be recorded in a user profile and what shouldn’t. If external data needs to be acquired, how it is harmonised with the company’s own data. The point is that this function is the gatekeeper, nothing downstream can depend on anything other than “State”.\n",
    "\n",
    "Policy (data driven product)\n",
    "This is the big one for both AlphaGo and a company. Policy is the tool to change the world (as described by State). It consists of a set of action that can be taken and a function (the “Policy”) of which action is best taken at each state. AlphaGo’s job is easy on the actions, there are 19x19 places and it can place a stone on an empty one and that’s it. Now where that should be in each turn is the million GPU question and indeed Deepmind’s tremendous achievement was to solve this problem. \n",
    "\n",
    "Data Scientists working in this area are focusing on automation. They are part of a product team and their tools are only used when the product require them, usually when traditional software engineering cannot solve it. Whether they work on internal or external user facing products, the job is to use the available data and select the best course of actions automatically. They are determining the State the company is in and the State it should be after the action. This action can be anything from a recommender system selecting recommended items, a model providing yes/no answers or a query selecting your friends’ friends in a social network. The point is that this function is automated and it must work without supervision taking the world from one state to a desired next one. \n",
    "\n",
    "Reward (data driven prediction)\n",
    "The final area is determining the value of being in one State or another. Just like “Policy” this works as well with data from “State” since you can only cook with what you recorded in your database. While the rules of Go provide a very easy way of measuring reward (the score), AlphaGo uses a clever prediction of future reward called the Value function. \n",
    "\n",
    "Value functions are much closer to what Data Scientists are working on in this field: the prediction of the business’s health in the future or alternatively the reward for the company in imagined “States” (scenarios). Anything to do with estimation of the KPIs in the future, financial prediction, estimation of the value of features to be implemented, risk scenarios. Focus is on prediction and business decision support and not automation, the primary concern is the accuracy of the prediction. \n",
    "\n",
    "A/B tests for example are not in this category, as to run an A/B test you need to be in production with both versions of the product, therefore you are going to influence the world with it. A/B tests are also inherently non-predictive and more of a measuring problem, therefore its data concerns belong to the “State” area.\n",
    "\n",
    "Differentiating between “Policy” and “Reward” areas are crucial as lot of the techniques DSes are using are shared (e.g.: Statistical Modelling) but the purpose is not. Differentiating between “State” and “Reward” areas is another important distinction: One is about being descriptive and the other is about being predictive.\n",
    "\n",
    "AlphaGo (and MDPs) are end-to-end systems just like data driven enterprises. Their respective three components map to each other without a gap and describe a complete classification of areas for Data Science.\n",
    "\n",
    "Conclusion\n",
    "We saw that the three areas in a data driven business overlap with the mathematical concept underpinning decision making and we identified key aspects of each of them:\n",
    "\n",
    "“State” area corresponds to “data collection” and is concerned with how to describe the world with numbers and also acts as a gatekeeper to the other two areas. If no data is recorded, no action or value can be calculated.\n",
    "\n",
    "“Policy” area corresponds to “data driven product” and is concerned with automation. If it cannot be reliably automated, it won’t be feasible. Also it might not be data science at all.\n",
    "\n",
    "“Reward” area corresponds to “data driven predictions” and is concerned with predictions and scenarios.  \n",
    "\n",
    "I hope the above provides a clear mental framework partitioning DS work and helps you in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
