{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import re\n",
    "import lime.lime_text\n",
    "import numpy as np\n",
    "import webbrowser\n",
    "from pathlib import Path\n",
    "from gensim.utils import simple_preprocess\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_train = glob.glob(\"../day_8/data/aclImdb/train/pos/*.txt\")\n",
    "negative_train = glob.glob(\"../day_8/data/aclImdb/train/neg/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/fasttext_train.txt\", \"w\") as fh:\n",
    "    for file in positive_train:\n",
    "        text = open(file).read()\n",
    "        text = simple_preprocess(text)\n",
    "        text = \" \".join(text)\n",
    "        text = \"__label__positive \" + text + \"\\n\"\n",
    "        \n",
    "        fh.write(text)\n",
    "        \n",
    "    for file in negative_train:\n",
    "        text = open(file).read()\n",
    "        text = simple_preprocess(text)\n",
    "        text = \" \".join(text)\n",
    "        text = \"__label__negative \" + text + \"\\n\"\n",
    "        \n",
    "        fh.write(text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 ./data/fasttext_train.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l ./data/fasttext_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised('./data/fasttext_train.txt', epoch=10, ws=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"./models/imbd_reviews.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# This function regularizes a piece of text so it's in the same format\n",
    "# that we used when training the FastText classifier.\n",
    "def strip_formatting(string):\n",
    "    string = string.lower()\n",
    "    string = re.sub(r\"([.!?,'/()])\", r\" \\1 \", string)\n",
    "    return string\n",
    "\n",
    "# LIME needs to be able to mimic how the classifier splits\n",
    "# the string into words. So we'll provide a function that\n",
    "# mimics how FastText works.\n",
    "def tokenize_string(string):\n",
    "    return string.split()\n",
    "\n",
    "# Load our trained FastText classifier model (created in Part 2)\n",
    "classifier = fasttext.load_model('./models/imbd_reviews.bin')\n",
    "\n",
    "# Create a LimeTextExplainer. This object knows how to explain a text-based\n",
    "# prediction by dropping words randomly.\n",
    "explainer = lime.lime_text.LimeTextExplainer(\n",
    "    # We need to tell LIME how to split the string into words. We can do this\n",
    "    # by giving it a function to call to split a string up the same way FastText does it.\n",
    "    split_expression=tokenize_string,\n",
    "    # Our FastText classifer uses bigrams (two-word pairs) to classify text. Setting\n",
    "    # bow=False tells LIME to not assume that our classifier is based on single words only.\n",
    "    bow=False,\n",
    "    # To make the output pretty, tell LIME what to call each possible prediction from our model.\n",
    "    class_names=[\"Negative\", \"Positive\"]\n",
    ")\n",
    "\n",
    "# LIME is designed to work with classifiers that generate predictions\n",
    "# in the same format as Scikit-Learn. It expects every prediction to have\n",
    "# a probability value for every possible label.\n",
    "# The default FastText python wrapper generates predictions in a different\n",
    "# format where it only returns the top N highest likelihood results. This\n",
    "# code just calls the FastText predict function and then massages it into\n",
    "# the format that LIME expects (so that LIME will work).\n",
    "def fasttext_prediction_in_sklearn_format(classifier, texts):\n",
    "    res = []\n",
    "    # Ask FastText for the top 10 most likely labels for each piece of text.\n",
    "    # This ensures we always get a probability score for every possible label in our model.\n",
    "    labels, probabilities = classifier.predict(texts, 10)\n",
    "\n",
    "    # For each prediction, sort the probabaility scores into the same order\n",
    "    # (I.e. no_stars, 1_star, 2_star, etc). This is needed because FastText\n",
    "    # returns predicitons sorted by most likely instead of in a fixed order.\n",
    "    for label, probs, text in zip(labels, probabilities, texts):\n",
    "        order = np.argsort(np.array(label))\n",
    "        res.append(probs[order])\n",
    "\n",
    "    return np.array(res)\n",
    "\n",
    "# Review to explain\n",
    "review = \"Enola Holmes is that rare gift that falls under the umbrella of 'family film' that still packs a great satisfying wallop for adult audiences too.\"\n",
    "\n",
    "# Pre-process the text of the review so it matches the training format\n",
    "preprocessed_review = strip_formatting(review)\n",
    "\n",
    "# Make a prediction and explain it!\n",
    "exp = explainer.explain_instance(\n",
    "    # The review to explain\n",
    "    preprocessed_review,\n",
    "    # The wrapper function that returns FastText predictions in scikit-learn format\n",
    "    classifier_fn=lambda x: fasttext_prediction_in_sklearn_format(classifier, x),\n",
    "    # How many labels to explain. We just want to explain the single most likely label.\n",
    "    top_labels=1,\n",
    "    # How many words in our sentence to include in the explanation. You can try different values.\n",
    "    num_features=20,\n",
    ")\n",
    "\n",
    "# Save the explanation to an HTML file so it's easy to view.\n",
    "# You can also get it to other formats: as_list(), as_map(), etc.\n",
    "# See https://lime-ml.readthedocs.io/en/latest/lime.html#lime.explanation.Explanation\n",
    "output_filename = Path(\"/disk1/projects/lectures/ml_lectures/day_9/outputs/out.html\")\n",
    "exp.save_to_file(output_filename)\n",
    "\n",
    "# Open the explanation html in our web browser.\n",
    "webbrowser.open(output_filename.as_uri())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'outputs/out.html'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/disk1/projects/lectures/ml_lectures/day_9\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
