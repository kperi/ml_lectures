{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fasttext lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import re\n",
    "import lime.lime_text\n",
    "import numpy as np\n",
    "import webbrowser\n",
    "from pathlib import Path\n",
    "from gensim.utils import simple_preprocess, simple_tokenize, deaccent\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_train = glob.glob(\"../day_7/data/aclImdb/train/pos/*.txt\")\n",
    "negative_train = glob.glob(\"../day_7/data/aclImdb/train/neg/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create fasttext format file to use for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/fasttext_train.txt\", \"w\") as fh:\n",
    "    for file in positive_train:\n",
    "        text = open(file).read()\n",
    "        text = simple_preprocess(text)\n",
    "        text = \" \".join(text)\n",
    "        text = \"__label__positive \" + text + \"\\n\"\n",
    "        \n",
    "        fh.write(text)\n",
    "        \n",
    "    for file in negative_train:\n",
    "        text = open(file).read()\n",
    "        text = simple_preprocess(text)\n",
    "        text = \" \".join(text)\n",
    "        text = \"__label__negative \" + text + \"\\n\"\n",
    "        \n",
    "        fh.write(text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   25000 ./data/fasttext_train.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l ./data/fasttext_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised('./data/fasttext_train.txt', epoch=10, ws=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"./models/imbd_reviews.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# This function regularizes a piece of text so it's in the same format\n",
    "# that we used when training the FastText classifier.\n",
    "def strip_formatting(string):\n",
    "    string = string.lower()\n",
    "    string = deaccent(string)\n",
    "    return string\n",
    "\n",
    "# LIME needs to be able to mimic how the classifier splits\n",
    "# the string into words. So we'll provide a function that\n",
    "# mimics how FastText works.\n",
    "def tokenize_string(string):\n",
    "    return simple_preprocess(string)\n",
    "\n",
    "# Load our trained FastText classifier model (created in Part 2)\n",
    "classifier = fasttext.load_model('./models/imbd_reviews.bin')\n",
    "\n",
    "# Create a LimeTextExplainer. This object knows how to explain a text-based\n",
    "# prediction by dropping words randomly.\n",
    "explainer = lime.lime_text.LimeTextExplainer(\n",
    "    # We need to tell LIME how to split the string into words. We can do this\n",
    "    # by giving it a function to call to split a string up the same way FastText does it.\n",
    "    split_expression=tokenize_string,\n",
    "    # Our FastText classifer uses bigrams (two-word pairs) to classify text. Setting\n",
    "    # bow=False tells LIME to not assume that our classifier is based on single words only.\n",
    "    bow=False,\n",
    "    # To make the output pretty, tell LIME what to call each possible prediction from our model.\n",
    "    class_names=[ \"Positive\", \"Negative\"]\n",
    ")\n",
    "\n",
    "# LIME is designed to work with classifiers that generate predictions\n",
    "# in the same format as Scikit-Learn. It expects every prediction to have\n",
    "# a probability value for every possible label.\n",
    "def fasttext_prediction_in_sklearn_format(classifier, texts):\n",
    "    res = []\n",
    "    # Ask FastText for the top  most likely labels for each piece of text.\n",
    "    # Here we can increase the number of the returned labels if we have multilabel classification\n",
    "    labels, probabilities = classifier.predict(texts, 2)\n",
    "\n",
    "    # For each prediction, sort the probabaility scores into the same order\n",
    "    # (I.e. POSITIVE, NEGATIVE etc). This is needed because FastText\n",
    "    # returns predicitons sorted by most likely instead of in a fixed order.\n",
    "    for label, probs, text in zip(labels, probabilities, texts):\n",
    "        order = np.argsort(np.array(label))\n",
    "        res.append(probs[order])\n",
    "\n",
    "    return np.array(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_review(review, output_filename):\n",
    "    \n",
    "    # Pre-process the text of the review so it matches the training format\n",
    "    preprocessed_review = strip_formatting(review)\n",
    "\n",
    "    # Make a prediction and explain it!\n",
    "    exp = explainer.explain_instance(\n",
    "        # The review to explain\n",
    "        preprocessed_review,\n",
    "        # The wrapper function that returns FastText predictions in scikit-learn format\n",
    "        classifier_fn=lambda x: fasttext_prediction_in_sklearn_format(classifier, x),\n",
    "        # How many labels to explain. We just want to explain the single most likely label.\n",
    "        top_labels=1,\n",
    "        # How many words in our sentence to include in the explanation. You can try different values.\n",
    "        num_features=20,\n",
    "    )\n",
    "\n",
    "    # Save the explanation to an HTML file so it's easy to view.\n",
    "    # You can also get it to other formats: as_list(), as_map(), etc.\n",
    "    # See https://lime-ml.readthedocs.io/en/latest/lime.html#lime.explanation.Explanation\n",
    "\n",
    "    full_path = os.path.dirname(os.path.realpath('__file__')) + \"/data/\" + output_filename\n",
    "\n",
    "    output_filename = Path( full_path )\n",
    "    exp.save_to_file(output_filename)\n",
    "\n",
    "    # Open the explanation html in our web browser.\n",
    "    webbrowser.open(output_filename.as_uri())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"If you can keep both eyes open through its whole three-hour length you're a better man than I am.\"\n",
    "explain_review(review, \"output_1.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"\"\"I love beautiful movies. If a film is eye-candy with carefully designed decorations, masterful camerawork, lighting, and architectural frames, I can forgive anything else in the movie. The lack or even absence of logic, weak dialogue and characters, cliche storylinesâ€“I can live with all that if I like what I see.\"\"\"\n",
    "explain_review(review, \"output_2.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
