{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where to go next with ML \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There several paths to take from this point in terms of ML applications.\n",
    "\n",
    "\n",
    "\n",
    "## Applications\n",
    "\n",
    "\n",
    "Everything we learned so far can be the first step (or even further) in real world applications.\n",
    "\n",
    "\n",
    "- Demand forecasting\n",
    "- Dynamic Prising \n",
    "- Recommender Engines\n",
    "- Personalisation \n",
    "- Churn prediction \n",
    "- Operational integrity \n",
    "- Anomaly & outlier detection both for machine generated data and human behaviour\n",
    "- Search and information retrieval, for internaly and externaly facing engines\n",
    "- Logistics and optimisation\n",
    "\n",
    "\n",
    "\n",
    "## Models and techniques\n",
    "\n",
    "In this course we covered the basics from a theoretical and practical point of view.\n",
    "The ML and AI space however is huge and this course has barely scratched the tip of the iceberg \n",
    "\n",
    "Potential next steps one can follow from here are, but not limited to, the following\n",
    "\n",
    "### Sequence models\n",
    "     \n",
    "All of the models we have been discussing so far are not sequencial, or their sequencial nature has been transformed to a non-sequential one (for example, text can be definitely thought as a sequencial process, however we treated text mostly with Bag of Words representation, disregarding -mostly- any sequencial information). \n",
    "\n",
    "There are various ways to deal with sequential problems, ranging from Markovian Process to Conditional Random Fields and Recurrent Neural Networks. \n",
    "\n",
    "In the last few years, the Encoder-Decoder architecture applied in RNNs have been proven extremely efficient in problems such as Machine Translation and generally transformation (transduction) from an input sequence to a terget sequence. \n",
    "\n",
    "### ConvNets for Object Detection \n",
    "\n",
    "\n",
    "So far we have seen Convolutional Neural networks for image classification and feature learning. \n",
    "From there one can go to object detection and localisation, where we are not interested only to classify an image based on its content, eg is there a cat or a dog or a plane. Insted we go one step further and answer the questions\n",
    "\n",
    "- What kinds of objects are in the image?\n",
    "- Where is the object(s) in the image?\n",
    "  \n",
    "For the last part we can either put a bounding box around the image or even do semantic segmentation on a pixel level and classify each pixel in the image to the class it belongs.  \n",
    "\n",
    "   \n",
    "\n",
    "### Advanced Natural Language Processing\n",
    "\n",
    "The state of the art in NLP involves several tasks under the umbrella name \"Natural Language Understanding\" \n",
    "It also combines NLP with Computer Vision for multimodal learning, for example detecting toxic content in images.\n",
    "\n",
    "The most successful models these days are variants of the Transformer architecture introduced by Google in 2017, and from there a bunch of models has been produced that pushed NLP to its boundaries\n",
    "\n",
    "Models such as BERT, we have seen examples as well as OpenAI's GPT-3 which recently attracted significant attention in the Media.\n",
    "\n",
    "\n",
    "### Probabilistic models\n",
    "\n",
    "We have been discussing so far models for prediction, eg we are feeding instances to a model and we were getting out a vector of predictions.\n",
    "\n",
    "There is a class of ML models where the approach is somewhat different.\n",
    "\n",
    "Probabilistic graphical models (PGMs) are a rich framework for encoding probability distributions over complex domains: joint (multivariate) distributions over large numbers of random variables that interact with each other. \n",
    "\n",
    "These representations sit at the intersection of statistics and computer science, relying on concepts from probability theory, graph algorithms, machine learning, and more. They are the basis for the state-of-the-art methods in a wide variety of applications, such as medical diagnosis, image understanding, speech recognition, natural language processing, and many, many more. They are also a foundational tool in formulating many machine learning problems. \n",
    "\n",
    "\n",
    "### Optimisation\n",
    "\n",
    "Optimisation on its own is a huge topic, ranging from training Neural Networks to finding optimal hyperparameters in Machine Learning models. We covered here Gradient Descent, Grid and Randomized Search.\n",
    "\n",
    "There are however many other directions one can take depending on the problem. From Gaussian Processes and Bayesian Optimisation to Genetic Algorithms Evolutionary Algorithms or even population based algorithms such as Ant Colony Optimisation.\n",
    "\n",
    "\n",
    "\n",
    "## ML Engineering\n",
    "\n",
    "We covered the very basics of ML Engineering here. Again the topic is vast and there are no yet optimal strategies and guidelines to consider the problem solved. Actually, every company with at least a dozen of ML models in production is facing the same problems and questions.\n",
    "\n",
    "\n",
    "There different strategies one can follow depending on the nature of the business. It is quite different to serve models in Facebook scale. \n",
    "\n",
    "For example, with  the FBLearner Flow, the internal tool for training and serving models in FB, more than a million models have been trained, and the prediction service has grown to make more than 6 million predictions per second.\n",
    "\n",
    "Depending on the scale, the available infrastructure, the required latency, one can choose to go from simple REST/Docker python solutions to RPC C/C++ based architectures and architectures such as Protocol Buffers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
