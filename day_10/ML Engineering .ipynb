{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install great_expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Engineering\n",
    "\n",
    "\n",
    "Here we will be discussing ML Engineering topics and how to get models into prod.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering\n",
    "\n",
    "Data Engineering is the corner stone on model buiding, deployment and serving.\n",
    "Except the traditional ETL pipeline however, ML Engineering needs to address several other issues before serving data to a model for prediction.\n",
    "\n",
    "This can be thought as a \"Data Unit Testing\" or in a sense, some form of anomaly detection in data.\n",
    "\n",
    "\n",
    "### Checks on input\n",
    "\n",
    "\n",
    "Every ML algorithm comes with certain assumptions on the form and the underlying distribution in the data it has been trained on. Sometimes, due to several reasons, data is broken or invalid. \n",
    "\n",
    "From a ML Engineering team perspective, we would like to have all the checkpoints in place to detect problems in the data **before** we serve them to the algorith.\n",
    "\n",
    "This process essentially acts as an anomaly detection filter on top of the data pipelines and it is typically a good practice in every ETL/Analytics pipeline, not just for ML problems.\n",
    "\n",
    "There are several checks one may want to apply on the data before served to the algo/transformation pipelines:\n",
    "\n",
    "\n",
    "- Data type consistency:\n",
    "    - Real numbers are real numbers\n",
    "    - Integers\n",
    "    - Strings\n",
    "    - Sets/Dictionaries\n",
    "    - Categorical data are fixed and identical to what the model has been trained on  etc \n",
    "    \n",
    "\n",
    "\n",
    "For numerical data, there is a full set of potenial checks, application-specific that one can apply before feeding the dataset to the algo:\n",
    "\n",
    "- Data type validation and concistency\n",
    "    - Are reals, reals?\n",
    "    - Are integers, integers?\n",
    "    \n",
    "- Statistical Checks\n",
    "    - Mean\n",
    "    - Median\n",
    "    - Standard deviation\n",
    "    - Percentiles \n",
    "    - Maximum values/minimum values\n",
    "    - Entropy\n",
    "    \n",
    "- Strings:\n",
    "    - Sanity checks on the input strings\n",
    "    - Vocabulary consistency \n",
    "    - Entropy \n",
    "- Categorical data\n",
    "    - Consistency checks on the input values \n",
    "    - Missing data \n",
    "\n",
    "\n",
    "There are a few libraries one can use in this process, namely `Dequee` from Amazon and `Great Expectations` (python).\n",
    "\n",
    "\n",
    "### Deequ\n",
    "\n",
    "Deequ is a library built on top of Apache Spark for defining \"unit tests for data\", which measure data quality in large datasets. \n",
    "\n",
    "\n",
    "Deequ's purpose is to \"unit-test\" data to find errors early, before the data gets fed to consuming systems or machine learning algorithms.\n",
    "\n",
    "\n",
    "Let's define a class of data\n",
    "\n",
    "```Scala\n",
    "case class Item(\n",
    "  id: Long,\n",
    "  productName: String,\n",
    "  description: String,\n",
    "  priority: String,\n",
    "  numViews: Long\n",
    ")\n",
    " \n",
    "val rdd = spark.sparkContext.parallelize(Seq(\n",
    "  Item(1, \"Thingy A\", \"awesome thing.\", \"high\", 0),\n",
    "  Item(2, \"Thingy B\", \"available at http://thingb.com\", null, 0),\n",
    "  Item(3, null, null, \"low\", 5),\n",
    "  Item(4, \"Thingy D\", \"checkout https://thingd.ca\", \"low\", 10),\n",
    "  Item(5, \"Thingy E\", null, \"high\", 12)))\n",
    "\n",
    "val data = spark.createDataFrame(rdd)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "most applications that work with data have implicit assumptions about that data, e.g., that attributes have certain types, do not contain NULL values, and so on. If these assumptions are violated, your application might crash or produce wrong outputs. The idea behind deequ is to explicitly state these assumptions in the form of a \"unit-test\" for data, which can be verified on a piece of data at hand. If the data has errors, we can \"quarantine\" and fix it, before we feed to an application.\n",
    "\n",
    "The main entry point for defining how you expect your data to look is the VerificationSuite from which you can add Checks that define constraints on attributes of the data. In this example, we test for the following properties of our data:\n",
    "\n",
    "- there are 5 rows in total\n",
    "- values of the id attribute are never NULL and unique\n",
    "- values of the productName attribute are never NULL\n",
    "- the priority attribute can only contain \"high\" or \"low\" as value\n",
    "- numViews should not contain negative values\n",
    "- at least half of the values in description should contain a url\n",
    "- the median of numViews should be less than or equal to 10\n",
    "\n",
    "\n",
    "In code this looks as follows:\n",
    "\n",
    "\n",
    "```Scala\n",
    "import com.amazon.deequ.VerificationSuite\n",
    "import com.amazon.deequ.checks.{Check, CheckLevel, CheckStatus}\n",
    "\n",
    "\n",
    "val verificationResult = VerificationSuite()\n",
    "  .onData(data)\n",
    "  .addCheck(\n",
    "    Check(CheckLevel.Error, \"unit testing my data\")\n",
    "      .hasSize(_ == 5) // we expect 5 rows\n",
    "      .isComplete(\"id\") // should never be NULL\n",
    "      .isUnique(\"id\") // should not contain duplicates\n",
    "      .isComplete(\"productName\") // should never be NULL\n",
    "      // should only contain the values \"high\" and \"low\"\n",
    "      .isContainedIn(\"priority\", Array(\"high\", \"low\"))\n",
    "      .isNonNegative(\"numViews\") // should not contain negative values\n",
    "      // at least half of the descriptions should contain a url\n",
    "      .containsURL(\"description\", _ >= 0.5)\n",
    "      // half of the items should have less than 10 views\n",
    "      .hasApproxQuantile(\"numViews\", 0.5, _ <= 10))\n",
    "    .run()\n",
    "    \n",
    "``` \n",
    "\n",
    "\n",
    "After calling run, deequ translates your test to a series of Spark jobs, which it executes to compute metrics on the data. Afterwards it invokes your assertion functions (e.g., _ == 5 for the size check) on these metrics to see if the constraints hold on the data. We can inspect the VerificationResult to see if the test found errors:\n",
    "\n",
    "```Scala\n",
    "import com.amazon.deequ.constraints.ConstraintStatus\n",
    "\n",
    "\n",
    "if (verificationResult.status == CheckStatus.Success) {\n",
    "  println(\"The data passed the test, everything is fine!\")\n",
    "} else {\n",
    "  println(\"We found errors in the data:\\n\")\n",
    "\n",
    "  val resultsForAllConstraints = verificationResult.checkResults\n",
    "    .flatMap { case (_, checkResult) => checkResult.constraintResults }\n",
    "\n",
    "  resultsForAllConstraints\n",
    "    .filter { _.status != ConstraintStatus.Success }\n",
    "    .foreach { result => println(s\"${result.constraint}: ${result.message.get}\") }\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "Which will produce the following output:\n",
    "\n",
    "\n",
    "```We found errors in the data:\n",
    "\n",
    "CompletenessConstraint(Completeness(productName)): Value: 0.8 does not meet the requirement!\n",
    "PatternConstraint(containsURL(description)): Value: 0.4 does not meet the requirement!\n",
    "```\n",
    "\n",
    "Deequ is also available in PySpark:\n",
    "\n",
    "```python\n",
    "from pydeequ.checks import *\n",
    "from pydeequ.verification import *\n",
    "\n",
    "check = Check(spark, CheckLevel.Warning, \"Review Check\")\n",
    "\n",
    "checkResult = VerificationSuite(spark) \\\n",
    "    .onData(df) \\\n",
    "    .addCheck(\n",
    "        check.hasSize(lambda x: x >= 3) \\\n",
    "        .hasMin(\"b\", lambda x: x == 0) \\\n",
    "        .isComplete(\"c\")  \\\n",
    "        .isUnique(\"a\")  \\\n",
    "        .isContainedIn(\"a\", [\"foo\", \"bar\", \"baz\"]) \\\n",
    "        .isNonNegative(\"b\")) \\\n",
    "    .run()\n",
    "    \n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.show()\n",
    "```\n",
    "\n",
    "\n",
    "### GreatExpectations\n",
    "\n",
    "\n",
    "Great Expectations [...] by offering a unique approach to automated testing: pipeline tests. Pipeline tests are applied to data (instead of code) and at batch time (instead of compile or deploy time). Pipeline tests are like unit tests for datasets: they help you guard against upstream data changes and monitor data quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"meta\": {},\n",
       "  \"result\": {\n",
       "    \"observed_value\": 541.0,\n",
       "    \"element_count\": 7,\n",
       "    \"missing_count\": null,\n",
       "    \"missing_percent\": null\n",
       "  },\n",
       "  \"success\": true,\n",
       "  \"exception_info\": {\n",
       "    \"raised_exception\": false,\n",
       "    \"exception_traceback\": null,\n",
       "    \"exception_message\": null\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import great_expectations as ge\n",
    "import pandas as pd\n",
    "\n",
    "data = [\n",
    "    (\"1\", \"male\", 1, 1334),\n",
    "    (\"2\", \"female\", 2, 643),\n",
    "    (\"3\", \"male\", 2, 645),\n",
    "    (\"4\", \"male\", 1, 234),\n",
    "    (\"5\", \"female\", 2, 23),\n",
    "    (\"6\", \"male\", 1, 454),\n",
    "    (\"6\", \"male\", 1, 454),\n",
    "\n",
    "]\n",
    "\n",
    "df = pd.DataFrame( data = data , columns = [\"id\", \"gender\", \"col1\", \"col2\"])\n",
    "\n",
    "ge_df = ge.from_pandas( df )\n",
    "\n",
    "\n",
    "\n",
    "ge_df.expect_column_distinct_values_to_be_in_set( \"gender\", [\"male\", \"female\"])\n",
    "ge_df.expect_column_mean_to_be_between( column=\"col1\", min_value=1,  max_value=3)\n",
    "ge_df.expect_column_max_to_be_between( column=\"col2\", min_value=100, max_value=2000)\n",
    "ge_df.expect_column_mean_to_be_between( column=\"col2\", min_value=100, max_value=1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"data_asset_type\": \"Dataset\",\n",
       "  \"expectation_suite_name\": \"default\",\n",
       "  \"meta\": {\n",
       "    \"great_expectations_version\": \"0.13.2\"\n",
       "  },\n",
       "  \"expectations\": [\n",
       "    {\n",
       "      \"meta\": {},\n",
       "      \"kwargs\": {\n",
       "        \"column\": \"gender\",\n",
       "        \"value_set\": [\n",
       "          \"male\",\n",
       "          \"female\"\n",
       "        ]\n",
       "      },\n",
       "      \"expectation_type\": \"expect_column_distinct_values_to_be_in_set\"\n",
       "    },\n",
       "    {\n",
       "      \"meta\": {},\n",
       "      \"kwargs\": {\n",
       "        \"column\": \"col1\",\n",
       "        \"min_value\": 1,\n",
       "        \"max_value\": 3\n",
       "      },\n",
       "      \"expectation_type\": \"expect_column_mean_to_be_between\"\n",
       "    },\n",
       "    {\n",
       "      \"meta\": {},\n",
       "      \"kwargs\": {\n",
       "        \"column\": \"col2\",\n",
       "        \"min_value\": 100,\n",
       "        \"max_value\": 2000\n",
       "      },\n",
       "      \"expectation_type\": \"expect_column_max_to_be_between\"\n",
       "    },\n",
       "    {\n",
       "      \"meta\": {},\n",
       "      \"kwargs\": {\n",
       "        \"column\": \"col2\",\n",
       "        \"min_value\": 100,\n",
       "        \"max_value\": 1000\n",
       "      },\n",
       "      \"expectation_type\": \"expect_column_mean_to_be_between\"\n",
       "    }\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ge_df.get_expectation_suite( discard_failed_expectations  = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Persistence and Versioning\n",
    "\n",
    "\n",
    "### sklearn \n",
    "In the sklearn world, the more common scenario is to use joblib to store the binary model after training and model evaluation and then, in serving time, load the model and serve accordingly.\n",
    "\n",
    "\n",
    "### pytorch \n",
    "In pytorch we have the following options:\n",
    "\n",
    "\n",
    "- torch.save: Saves a serialized object to disk. This function uses Python’s pickle utility for serialization. Models, tensors, and dictionaries of all kinds of objects can be saved using this function.\n",
    "\n",
    "- torch.load: Uses pickle’s unpickling facilities to deserialize pickled object files to memory. This function also facilitates the device to load the data into (see Saving & Loading Model Across Devices).\n",
    "\n",
    "- torch.nn.Module.load_state_dict: Loads a model’s parameter dictionary using a deserialized state_dict.\n",
    "\n",
    "\n",
    "In PyTorch, the learnable parameters (i.e. weights and biases) of an torch.nn.Module model are contained in the model’s parameters (accessed with model.parameters()). A state_dict is simply a Python dictionary object that maps each layer to its parameter tensor. Note that only layers with learnable parameters (convolutional layers, linear layers, etc.) and registered buffers (batchnorm’s running_mean) have entries in the model’s state_dict. Optimizer objects (torch.optim) also have a state_dict, which contains information about the optimizer’s state, as well as the hyperparameters used.\n",
    "\n",
    "\n",
    "\n",
    "### ONNX\n",
    "Open Neural Network Exchange is the open standard for machine learning interoperability. Pytorch has ONNX support and there is also some libraries that are offering ONNX support for sklearn as well, such as `sklearn-onnx` \n",
    "\n",
    "\n",
    "It is always a good idea to version models and their code altogether, using an internal versioning scheme. \n",
    "Of course this comes with a cost of disk space, however storage is relatively cheap and not much of an issue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving models\n",
    "\n",
    "There are two major patterns for serving models. The batch mode and the real time mode.\n",
    "These patterns typically obey business related rules and depend on the nature of the problem the ML model is solving.\n",
    "\n",
    "\n",
    "### Batch mode\n",
    "\n",
    "\n",
    "In the batch mode, the model runs in one-off scenarios in fixed time intervals. For example, a demand forecasting model could run every weekend to predict demand for next week. Similarily, a recommender system would update weekly or daily or a price optimisation system could run 2 or 3 times daily to update prices, depending on the business context. \n",
    "\n",
    "\n",
    "In this scenario, model would produce the entire output required for the business to operate, eg recommendations for all the products, forecasting for all the products etc.\n",
    "\n",
    "\n",
    "The primary steps are:-\n",
    "- Code runs at fixed time/interval\n",
    "- Code loads the model from the saved location\n",
    "- Code reads a batch of input data\n",
    "- Input data is new and unlabelled data that we want predictions for-\n",
    "- Input data might have data for multiple users/entities grouped together\n",
    "- Code runs model prediction over the Input batch and produces a Prediction batch\n",
    "- Prediction batch contains the predicted labels for each record in the input data\n",
    "- Predicted data is then saved in some new location\n",
    "\n",
    "\n",
    "\n",
    "### Real time mode. \n",
    "\n",
    "In this scenario, the machine learning pipeline is attached to an on-demand/streaming scenario, where the model is essentially a transformation or augmentation of the input data, such as for example image labelling and object detection in real time applications.\n",
    "\n",
    "\n",
    "We have a web service that wraps our code\n",
    "The web service exposes Rest endpoints for getting predictions\n",
    "Consumer application makes web service call and sends input data in Json format\n",
    "Input data contains all the feature required for prediction. It typically has only one record instead of a batch\n",
    "Code loads the model from the saved location\n",
    "Code gets input data when the web service endpoint is called\n",
    "Code runs model prediction over the Input data and produces Prediction data\n",
    "Prediction data is return back to the consumer application\n",
    "Consumer application can decide how to use the prediction data for a better user experience\n",
    "\n",
    "\n",
    "\n",
    "### Predictions via SQL\n",
    "\n",
    "This is a new trend that has caught off in the industry recently. This approach exposes the ML model as a SQL function which is applied as a transformation to the input data.\n",
    "\n",
    "This approach treats new input data as tables and allows ad-hoc analysis on the data by running our ML model as a function. The output is also viewed as table and can be saved for future if required.\n",
    "\n",
    "\n",
    "- ML model is wrapped in a SQL UDF\n",
    "- There is an SQL execution engine (like Spark or Google Big Table) that understands the UDF\n",
    "- SQL execution engine loads the code in a UDF\n",
    "- The user issues a SQL query to the execution engine, selecting the feature table in the SQL query\n",
    "- The execution engine runs the input features through the UDF to compute prediction\n",
    "- The prediction data is returned to the user\n",
    "- User might save the predicted data as a new table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Model Serving\n",
    "\n",
    "\n",
    "In the simplest case, we can setup a REST API to serve our models and also keep track of model versioning.\n",
    "\n",
    "This can be done by setting up a REST server which will listen to a certain port and will be structured to serve models in the form:\n",
    "\n",
    "```javascript\n",
    "/model_name/version/endpoint\n",
    "```\n",
    "\n",
    "for example \n",
    "\n",
    "```bash\n",
    "curl -XPOST \"data=@my_image.jpg\" http://localhost:5000/pet_recognition/1/endpoint \n",
    "```\n",
    "\n",
    "This is very practical for a small number of models even in large input scale as we can scale horizontally with docker images and load balancers. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airflow jobs\n",
    "\n",
    "For the batch mode, airflow scheduled jobs (or something similar) is a very good option. It allows to setup a DAG of tasks with the prediction phase included. It can combine ETL and feature engineering with the data validation/testing step above and finally the prediction step. \n",
    "\n",
    "\n",
    "### Docker Containers \n",
    "\n",
    "Docker containers can be used both for training and model serving jobs. Depending on the nature of the task, docker containers can be useful both in batch mode predictions as well as in real time/streaming scenarios, with the extra benefit of scaling.\n",
    "\n",
    "\n",
    "### MLFlow\n",
    "\n",
    "ML flow is open source and provides us capabilities for model tracking and serving. ML flow is also great for training lifecycle because it provides a web interface that shows model performance and accuracy over multiple runs. ML flow also lets us to group multiple runs as part of a single experiment.\n",
    "\n",
    "\n",
    "### SageMaker \n",
    "\n",
    "Amazon's SageMaker is a widely adopted solution for model serving in AWS. \n",
    "Pros: - Managed by Amazon, no in house infrastructure required\n",
    "Cons: - it can be expensive\n",
    "      - You may or may not want to lock yourself with Amazon\n",
    "      \n",
    "\n",
    "\n",
    "### Google BigQuery ML\n",
    "\n",
    "Google is also offering machine learning capabilities on top of BigQuery. It lets us use ML models as SQL functions on top of existing BigQuery tables.\n",
    "\n",
    "Pros: - Managed by Google, no in house infrastructure required\n",
    "Cons: - it can be expensive\n",
    "      - You may or may not want to lock yourself with google"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
