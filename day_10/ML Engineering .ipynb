{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting great_expectations\n",
      "  Downloading great_expectations-0.13.2-py3-none-any.whl (4.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.5 MB 290 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ipywidgets>=7.5.1\n",
      "  Using cached ipywidgets-7.5.1-py2.py3-none-any.whl (121 kB)\n",
      "Collecting tzlocal>=1.2\n",
      "  Downloading tzlocal-2.1-py2.py3-none-any.whl (16 kB)\n",
      "Collecting ruamel.yaml>=0.16\n",
      "  Downloading ruamel.yaml-0.16.12-py2.py3-none-any.whl (111 kB)\n",
      "\u001b[K     |████████████████████████████████| 111 kB 374 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2015.6 in /disk0/venvs/py3/lib/python3.6/site-packages (from great_expectations) (2020.4)\n",
      "Requirement already satisfied: pandas>=0.23.0 in /disk0/venvs/py3/lib/python3.6/site-packages (from great_expectations) (1.1.4)\n",
      "Requirement already satisfied: numpy>=1.14.1 in /disk0/venvs/py3/lib/python3.6/site-packages (from great_expectations) (1.19.4)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /disk0/venvs/py3/lib/python3.6/site-packages (from great_expectations) (1.1.0)\n",
      "Requirement already satisfied: pyparsing<3,>=2.4 in /disk0/venvs/py3/lib/python3.6/site-packages (from great_expectations) (2.4.7)\n",
      "Collecting Click>=7.1.2\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 229 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<2.24,>=2.20 in /disk0/venvs/py3/lib/python3.6/site-packages (from great_expectations) (2.21.0)\n",
      "Collecting black>=19.10b0\n",
      "  Downloading black-20.8b1.tar.gz (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 454 kB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: jsonschema>=2.5.1 in /disk0/venvs/py3/lib/python3.6/site-packages (from great_expectations) (3.2.0)\n",
      "Collecting altair<5,>=4.0.0\n",
      "  Downloading altair-4.1.0-py3-none-any.whl (727 kB)\n",
      "\u001b[K     |████████████████████████████████| 727 kB 343 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /disk0/venvs/py3/lib/python3.6/site-packages (from great_expectations) (2.8.1)\n",
      "Requirement already satisfied: mistune>=0.8.4 in /disk0/venvs/py3/lib/python3.6/site-packages (from great_expectations) (0.8.4)\n",
      "Requirement already satisfied: importlib-metadata>=1.7.0 in /disk0/venvs/py3/lib/python3.6/site-packages (from great_expectations) (2.0.0)\n",
      "Requirement already satisfied: jinja2>=2.10 in /disk0/venvs/py3/lib/python3.6/site-packages (from great_expectations) (2.11.2)\n",
      "Collecting jsonpatch>=1.22\n",
      "  Downloading jsonpatch-1.28-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /disk0/venvs/py3/lib/python3.6/site-packages (from great_expectations) (1.5.4)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /disk0/venvs/py3/lib/python3.6/site-packages (from ipywidgets>=7.5.1->great_expectations) (5.3.4)\n",
      "Collecting widgetsnbextension~=3.5.0\n",
      "  Using cached widgetsnbextension-3.5.1-py2.py3-none-any.whl (2.2 MB)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /disk0/venvs/py3/lib/python3.6/site-packages (from ipywidgets>=7.5.1->great_expectations) (5.0.8)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /disk0/venvs/py3/lib/python3.6/site-packages (from ipywidgets>=7.5.1->great_expectations) (7.16.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /disk0/venvs/py3/lib/python3.6/site-packages (from ipywidgets>=7.5.1->great_expectations) (4.3.3)\n",
      "Collecting ruamel.yaml.clib>=0.1.2; platform_python_implementation == \"CPython\" and python_version < \"3.9\"\n",
      "  Downloading ruamel.yaml.clib-0.2.2-cp36-cp36m-manylinux1_x86_64.whl (549 kB)\n",
      "\u001b[K     |████████████████████████████████| 549 kB 35 kB/s eta 0:00:012\n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.25,>=1.21.1 in /disk0/venvs/py3/lib/python3.6/site-packages (from requests<2.24,>=2.20->great_expectations) (1.23)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /disk0/venvs/py3/lib/python3.6/site-packages (from requests<2.24,>=2.20->great_expectations) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /disk0/venvs/py3/lib/python3.6/site-packages (from requests<2.24,>=2.20->great_expectations) (2.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /disk0/venvs/py3/lib/python3.6/site-packages (from requests<2.24,>=2.20->great_expectations) (2020.11.8)\n",
      "Collecting regex>=2020.1.8\n",
      "  Downloading regex-2020.11.13-cp36-cp36m-manylinux2014_x86_64.whl (723 kB)\n",
      "\u001b[K     |████████████████████████████████| 723 kB 36 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typed-ast>=1.4.0\n",
      "  Downloading typed_ast-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (737 kB)\n",
      "\u001b[K     |████████████████████████████████| 737 kB 628 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dataclasses>=0.6; python_version < \"3.7\"\n",
      "  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /disk0/venvs/py3/lib/python3.6/site-packages (from black>=19.10b0->great_expectations) (3.7.4.1)\n",
      "Collecting mypy-extensions>=0.4.3\n",
      "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
      "Requirement already satisfied: appdirs in /disk0/venvs/py3/lib/python3.6/site-packages (from black>=19.10b0->great_expectations) (1.4.4)\n",
      "Collecting pathspec<1,>=0.6\n",
      "  Downloading pathspec-0.8.1-py2.py3-none-any.whl (28 kB)\n",
      "Collecting toml>=0.10.1\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: setuptools in /disk0/venvs/py3/lib/python3.6/site-packages (from jsonschema>=2.5.1->great_expectations) (41.6.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /disk0/venvs/py3/lib/python3.6/site-packages (from jsonschema>=2.5.1->great_expectations) (0.17.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /disk0/venvs/py3/lib/python3.6/site-packages (from jsonschema>=2.5.1->great_expectations) (20.3.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /disk0/venvs/py3/lib/python3.6/site-packages (from jsonschema>=2.5.1->great_expectations) (1.15.0)\n",
      "Requirement already satisfied: entrypoints in /disk0/venvs/py3/lib/python3.6/site-packages (from altair<5,>=4.0.0->great_expectations) (0.3)\n",
      "Collecting toolz\n",
      "  Downloading toolz-0.11.1-py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 383 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /disk0/venvs/py3/lib/python3.6/site-packages (from importlib-metadata>=1.7.0->great_expectations) (3.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /disk0/venvs/py3/lib/python3.6/site-packages (from jinja2>=2.10->great_expectations) (1.1.1)\n",
      "Collecting jsonpointer>=1.9\n",
      "  Downloading jsonpointer-2.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: jupyter-client in /disk0/venvs/py3/lib/python3.6/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->great_expectations) (6.1.7)\n",
      "Requirement already satisfied: tornado>=4.2 in /disk0/venvs/py3/lib/python3.6/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->great_expectations) (6.1)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /disk0/venvs/py3/lib/python3.6/site-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations) (6.1.5)\n",
      "Requirement already satisfied: ipython-genutils in /disk0/venvs/py3/lib/python3.6/site-packages (from nbformat>=4.2.0->ipywidgets>=7.5.1->great_expectations) (0.2.0)\n",
      "Requirement already satisfied: jupyter-core in /disk0/venvs/py3/lib/python3.6/site-packages (from nbformat>=4.2.0->ipywidgets>=7.5.1->great_expectations) (4.6.3)\n",
      "Requirement already satisfied: pickleshare in /disk0/venvs/py3/lib/python3.6/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->great_expectations) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.10 in /disk0/venvs/py3/lib/python3.6/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->great_expectations) (0.17.2)\n",
      "Requirement already satisfied: decorator in /disk0/venvs/py3/lib/python3.6/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->great_expectations) (4.4.2)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /disk0/venvs/py3/lib/python3.6/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->great_expectations) (4.8.0)\n",
      "Requirement already satisfied: backcall in /disk0/venvs/py3/lib/python3.6/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->great_expectations) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /disk0/venvs/py3/lib/python3.6/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->great_expectations) (3.0.8)\n",
      "Requirement already satisfied: pygments in /disk0/venvs/py3/lib/python3.6/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->great_expectations) (2.7.2)\n",
      "Requirement already satisfied: pyzmq>=13 in /disk0/venvs/py3/lib/python3.6/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5.1->great_expectations) (19.0.2)\n",
      "Requirement already satisfied: Send2Trash in /disk0/venvs/py3/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations) (1.5.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /disk0/venvs/py3/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations) (0.9.1)\n",
      "Requirement already satisfied: argon2-cffi in /disk0/venvs/py3/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations) (20.1.0)\n",
      "Requirement already satisfied: prometheus-client in /disk0/venvs/py3/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations) (0.8.0)\n",
      "Requirement already satisfied: nbconvert in /disk0/venvs/py3/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations) (6.0.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /disk0/venvs/py3/lib/python3.6/site-packages (from jedi>=0.10->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->great_expectations) (0.7.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /disk0/venvs/py3/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->great_expectations) (0.6.0)\n",
      "Requirement already satisfied: wcwidth in /disk0/venvs/py3/lib/python3.6/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->great_expectations) (0.2.5)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /disk0/venvs/py3/lib/python3.6/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations) (1.14.3)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /disk0/venvs/py3/lib/python3.6/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations) (0.5.1)\n",
      "Requirement already satisfied: bleach in /disk0/venvs/py3/lib/python3.6/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations) (3.2.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /disk0/venvs/py3/lib/python3.6/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations) (1.4.3)\n",
      "Requirement already satisfied: defusedxml in /disk0/venvs/py3/lib/python3.6/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations) (0.6.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /disk0/venvs/py3/lib/python3.6/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations) (0.1.2)\n",
      "Requirement already satisfied: testpath in /disk0/venvs/py3/lib/python3.6/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations) (0.4.4)\n",
      "Requirement already satisfied: pycparser in /disk0/venvs/py3/lib/python3.6/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations) (2.20)\n",
      "Requirement already satisfied: nest-asyncio in /disk0/venvs/py3/lib/python3.6/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations) (1.4.2)\n",
      "Requirement already satisfied: async-generator in /disk0/venvs/py3/lib/python3.6/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations) (1.10)\n",
      "Requirement already satisfied: webencodings in /disk0/venvs/py3/lib/python3.6/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations) (0.5.1)\n",
      "Requirement already satisfied: packaging in /disk0/venvs/py3/lib/python3.6/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->great_expectations) (20.4)\n",
      "Building wheels for collected packages: black\n",
      "  Building wheel for black (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for black: filename=black-20.8b1-py3-none-any.whl size=124185 sha256=e7e1659aba588b4ef972bb5995ec91a6e34ba87e46d130a6e97d95288f7b3be0\n",
      "  Stored in directory: /home/kostas/.cache/pip/wheels/dd/c8/94/e774504958e4667180d81202d1d45cdc35d6222ae39229871b\n",
      "Successfully built black\n",
      "Installing collected packages: widgetsnbextension, ipywidgets, tzlocal, ruamel.yaml.clib, ruamel.yaml, Click, regex, typed-ast, dataclasses, mypy-extensions, pathspec, toml, black, toolz, altair, jsonpointer, jsonpatch, great-expectations\n",
      "  Attempting uninstall: widgetsnbextension\n",
      "    Found existing installation: widgetsnbextension 3.4.2\n",
      "    Uninstalling widgetsnbextension-3.4.2:\n",
      "      Successfully uninstalled widgetsnbextension-3.4.2\n",
      "  Attempting uninstall: ipywidgets\n",
      "    Found existing installation: ipywidgets 7.4.2\n",
      "    Uninstalling ipywidgets-7.4.2:\n",
      "      Successfully uninstalled ipywidgets-7.4.2\n",
      "  Attempting uninstall: Click\n",
      "    Found existing installation: Click 7.0\n",
      "    Uninstalling Click-7.0:\n",
      "      Successfully uninstalled Click-7.0\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2019.6.8\n",
      "    Uninstalling regex-2019.6.8:\n",
      "      Successfully uninstalled regex-2019.6.8\n",
      "Successfully installed Click-7.1.2 altair-4.1.0 black-20.8b1 dataclasses-0.8 great-expectations-0.13.2 ipywidgets-7.5.1 jsonpatch-1.28 jsonpointer-2.0 mypy-extensions-0.4.3 pathspec-0.8.1 regex-2020.11.13 ruamel.yaml-0.16.12 ruamel.yaml.clib-0.2.2 toml-0.10.2 toolz-0.11.1 typed-ast-1.4.1 tzlocal-2.1 widgetsnbextension-3.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install great_expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Engineering\n",
    "\n",
    "\n",
    "Here we will be discussing ML Engineering topics and how to get models into prod.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering\n",
    "\n",
    "Data Engineering is the corner stone on model buiding, deployment and serving.\n",
    "Except the traditional ETL pipeline however, ML Engineering needs to address several other issues before serving data to a model for prediction.\n",
    "\n",
    "This can be thought as a \"Data Unit Testing\" or in a sense, some form of anomaly detection in data.\n",
    "\n",
    "\n",
    "### Checks on input\n",
    "\n",
    "\n",
    "Every ML algorithm comes with certain assumptions on the form and the underlying distribution in the data it has been trained on. Sometimes, due to several reasons, data is broken or invalid. \n",
    "\n",
    "From a ML Engineering team perspective, we would like to have all the checkpoints in place to detect problems in the data **before** we serve them to the algorith.\n",
    "\n",
    "This process essentially acts as an anomaly detection filter on top of the data pipelines and it is typically a good practice in every ETL/Analytics pipeline, not just for ML problems.\n",
    "\n",
    "There are several checks one may want to apply on the data before served to the algo/transformation pipelines:\n",
    "\n",
    "\n",
    "- Data type consistency:\n",
    "    - Real numbers are real numbers\n",
    "    - Integers\n",
    "    - Strings\n",
    "    - Sets/Dictionaries\n",
    "    - Categorical data are fixed and identical to what the model has been trained on  etc \n",
    "    \n",
    "\n",
    "\n",
    "For numerical data, there is a full set of potenial checks, application-specific that one can apply before feeding the dataset to the algo:\n",
    "\n",
    "- Data type validation and concistency\n",
    "    - Are reals, reals?\n",
    "    - Are integers, integers?\n",
    "    \n",
    "- Statistical Checks\n",
    "    - Mean\n",
    "    - Median\n",
    "    - Standard deviation\n",
    "    - Percentiles \n",
    "    - Maximum values/minimum values\n",
    "    - Entropy\n",
    "    \n",
    "- Strings:\n",
    "    - Sanity checks on the input strings\n",
    "    - Vocabulary consistency \n",
    "    - Entropy \n",
    "- Categorical data\n",
    "    - Consistency checks on the input values \n",
    "    - Missing data \n",
    "\n",
    "\n",
    "There are a few libraries one can use in this process, namely `Dequee` from Amazon and `Great Expectations` (python).\n",
    "\n",
    "\n",
    "### Deequ\n",
    "\n",
    "Deequ is a library built on top of Apache Spark for defining \"unit tests for data\", which measure data quality in large datasets. \n",
    "\n",
    "\n",
    "Deequ's purpose is to \"unit-test\" data to find errors early, before the data gets fed to consuming systems or machine learning algorithms.\n",
    "\n",
    "\n",
    "Let's define a class of data\n",
    "\n",
    "```Scala\n",
    "case class Item(\n",
    "  id: Long,\n",
    "  productName: String,\n",
    "  description: String,\n",
    "  priority: String,\n",
    "  numViews: Long\n",
    ")\n",
    " \n",
    "val rdd = spark.sparkContext.parallelize(Seq(\n",
    "  Item(1, \"Thingy A\", \"awesome thing.\", \"high\", 0),\n",
    "  Item(2, \"Thingy B\", \"available at http://thingb.com\", null, 0),\n",
    "  Item(3, null, null, \"low\", 5),\n",
    "  Item(4, \"Thingy D\", \"checkout https://thingd.ca\", \"low\", 10),\n",
    "  Item(5, \"Thingy E\", null, \"high\", 12)))\n",
    "\n",
    "val data = spark.createDataFrame(rdd)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "most applications that work with data have implicit assumptions about that data, e.g., that attributes have certain types, do not contain NULL values, and so on. If these assumptions are violated, your application might crash or produce wrong outputs. The idea behind deequ is to explicitly state these assumptions in the form of a \"unit-test\" for data, which can be verified on a piece of data at hand. If the data has errors, we can \"quarantine\" and fix it, before we feed to an application.\n",
    "\n",
    "The main entry point for defining how you expect your data to look is the VerificationSuite from which you can add Checks that define constraints on attributes of the data. In this example, we test for the following properties of our data:\n",
    "\n",
    "- there are 5 rows in total\n",
    "- values of the id attribute are never NULL and unique\n",
    "- values of the productName attribute are never NULL\n",
    "- the priority attribute can only contain \"high\" or \"low\" as value\n",
    "- numViews should not contain negative values\n",
    "- at least half of the values in description should contain a url\n",
    "- the median of numViews should be less than or equal to 10\n",
    "\n",
    "\n",
    "In code this looks as follows:\n",
    "\n",
    "\n",
    "```Scala\n",
    "import com.amazon.deequ.VerificationSuite\n",
    "import com.amazon.deequ.checks.{Check, CheckLevel, CheckStatus}\n",
    "\n",
    "\n",
    "val verificationResult = VerificationSuite()\n",
    "  .onData(data)\n",
    "  .addCheck(\n",
    "    Check(CheckLevel.Error, \"unit testing my data\")\n",
    "      .hasSize(_ == 5) // we expect 5 rows\n",
    "      .isComplete(\"id\") // should never be NULL\n",
    "      .isUnique(\"id\") // should not contain duplicates\n",
    "      .isComplete(\"productName\") // should never be NULL\n",
    "      // should only contain the values \"high\" and \"low\"\n",
    "      .isContainedIn(\"priority\", Array(\"high\", \"low\"))\n",
    "      .isNonNegative(\"numViews\") // should not contain negative values\n",
    "      // at least half of the descriptions should contain a url\n",
    "      .containsURL(\"description\", _ >= 0.5)\n",
    "      // half of the items should have less than 10 views\n",
    "      .hasApproxQuantile(\"numViews\", 0.5, _ <= 10))\n",
    "    .run()\n",
    "    \n",
    "``` \n",
    "\n",
    "\n",
    "After calling run, deequ translates your test to a series of Spark jobs, which it executes to compute metrics on the data. Afterwards it invokes your assertion functions (e.g., _ == 5 for the size check) on these metrics to see if the constraints hold on the data. We can inspect the VerificationResult to see if the test found errors:\n",
    "\n",
    "```Scala\n",
    "import com.amazon.deequ.constraints.ConstraintStatus\n",
    "\n",
    "\n",
    "if (verificationResult.status == CheckStatus.Success) {\n",
    "  println(\"The data passed the test, everything is fine!\")\n",
    "} else {\n",
    "  println(\"We found errors in the data:\\n\")\n",
    "\n",
    "  val resultsForAllConstraints = verificationResult.checkResults\n",
    "    .flatMap { case (_, checkResult) => checkResult.constraintResults }\n",
    "\n",
    "  resultsForAllConstraints\n",
    "    .filter { _.status != ConstraintStatus.Success }\n",
    "    .foreach { result => println(s\"${result.constraint}: ${result.message.get}\") }\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "Which will produce the following output:\n",
    "\n",
    "\n",
    "```We found errors in the data:\n",
    "\n",
    "CompletenessConstraint(Completeness(productName)): Value: 0.8 does not meet the requirement!\n",
    "PatternConstraint(containsURL(description)): Value: 0.4 does not meet the requirement!\n",
    "```\n",
    "\n",
    "Deequ is also available in PySpark:\n",
    "\n",
    "```python\n",
    "from pydeequ.checks import *\n",
    "from pydeequ.verification import *\n",
    "\n",
    "check = Check(spark, CheckLevel.Warning, \"Review Check\")\n",
    "\n",
    "checkResult = VerificationSuite(spark) \\\n",
    "    .onData(df) \\\n",
    "    .addCheck(\n",
    "        check.hasSize(lambda x: x >= 3) \\\n",
    "        .hasMin(\"b\", lambda x: x == 0) \\\n",
    "        .isComplete(\"c\")  \\\n",
    "        .isUnique(\"a\")  \\\n",
    "        .isContainedIn(\"a\", [\"foo\", \"bar\", \"baz\"]) \\\n",
    "        .isNonNegative(\"b\")) \\\n",
    "    .run()\n",
    "    \n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.show()\n",
    "```\n",
    "\n",
    "\n",
    "### GreatExpectations\n",
    "\n",
    "\n",
    "Great Expectations [...] by offering a unique approach to automated testing: pipeline tests. Pipeline tests are applied to data (instead of code) and at batch time (instead of compile or deploy time). Pipeline tests are like unit tests for datasets: they help you guard against upstream data changes and monitor data quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"meta\": {},\n",
       "  \"result\": {\n",
       "    \"observed_value\": 541.0,\n",
       "    \"element_count\": 7,\n",
       "    \"missing_count\": null,\n",
       "    \"missing_percent\": null\n",
       "  },\n",
       "  \"success\": true,\n",
       "  \"exception_info\": {\n",
       "    \"raised_exception\": false,\n",
       "    \"exception_traceback\": null,\n",
       "    \"exception_message\": null\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import great_expectations as ge\n",
    "import pandas as pd\n",
    "\n",
    "data = [\n",
    "    (\"1\", \"male\", 1, 1334),\n",
    "    (\"2\", \"female\", 2, 643),\n",
    "    (\"3\", \"male\", 2, 645),\n",
    "    (\"4\", \"male\", 1, 234),\n",
    "    (\"5\", \"female\", 2, 23),\n",
    "    (\"6\", \"male\", 1, 454),\n",
    "    (\"6\", \"male\", 1, 454),\n",
    "\n",
    "]\n",
    "\n",
    "df = pd.DataFrame( data = data , columns = [\"id\", \"gender\", \"col1\", \"col2\"])\n",
    "\n",
    "ge_df = ge.from_pandas( df )\n",
    "\n",
    "\n",
    "\n",
    "ge_df.expect_column_distinct_values_to_be_in_set( \"gender\", [\"male\", \"female\"])\n",
    "ge_df.expect_column_mean_to_be_between( column=\"col1\", min_value=1,  max_value=3)\n",
    "ge_df.expect_column_max_to_be_between( column=\"col2\", min_value=100, max_value=2000)\n",
    "ge_df.expect_column_mean_to_be_between( column=\"col2\", min_value=100, max_value=1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"data_asset_type\": \"Dataset\",\n",
       "  \"expectation_suite_name\": \"default\",\n",
       "  \"meta\": {\n",
       "    \"great_expectations_version\": \"0.13.2\"\n",
       "  },\n",
       "  \"expectations\": [\n",
       "    {\n",
       "      \"meta\": {},\n",
       "      \"kwargs\": {\n",
       "        \"column\": \"gender\",\n",
       "        \"value_set\": [\n",
       "          \"male\",\n",
       "          \"female\"\n",
       "        ]\n",
       "      },\n",
       "      \"expectation_type\": \"expect_column_distinct_values_to_be_in_set\"\n",
       "    },\n",
       "    {\n",
       "      \"meta\": {},\n",
       "      \"kwargs\": {\n",
       "        \"column\": \"col1\",\n",
       "        \"min_value\": 1,\n",
       "        \"max_value\": 3\n",
       "      },\n",
       "      \"expectation_type\": \"expect_column_mean_to_be_between\"\n",
       "    },\n",
       "    {\n",
       "      \"meta\": {},\n",
       "      \"kwargs\": {\n",
       "        \"column\": \"col2\",\n",
       "        \"min_value\": 100,\n",
       "        \"max_value\": 2000\n",
       "      },\n",
       "      \"expectation_type\": \"expect_column_max_to_be_between\"\n",
       "    },\n",
       "    {\n",
       "      \"meta\": {},\n",
       "      \"kwargs\": {\n",
       "        \"column\": \"col2\",\n",
       "        \"min_value\": 100,\n",
       "        \"max_value\": 1000\n",
       "      },\n",
       "      \"expectation_type\": \"expect_column_mean_to_be_between\"\n",
       "    }\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ge_df.get_expectation_suite( discard_failed_expectations  = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '-f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-13c629f30df5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"quality\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0ml1_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '-f'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving models\n",
    "\n",
    "There are two major patterns for serving models. The batch mode and the real time mode.\n",
    "These patterns typically obey business related rules and depend on the nature of the problem the ML model is solving.\n",
    "\n",
    "\n",
    "### Batch mode\n",
    "\n",
    "\n",
    "In the batch mode, the model runs in one-off scenarios in fixed time intervals. For example, a demand forecasting model could run every weekend to predict demand for next week. Similarily, a recommender system would update weekly or daily or a price optimisation system could run 2 or 3 times daily to update prices, depending on the business context. \n",
    "\n",
    "\n",
    "In this scenario, model would produce the entire output required for the business to operate, eg recommendations for all the products, forecasting for all the products etc.\n",
    "\n",
    "\n",
    "The primary steps are:-\n",
    "- Code runs at fixed time/interval\n",
    "- Code loads the model from the saved location\n",
    "- Code reads a batch of input data\n",
    "- Input data is new and unlabelled data that we want predictions for-\n",
    "- Input data might have data for multiple users/entities grouped together\n",
    "- Code runs model prediction over the Input batch and produces a Prediction batch\n",
    "- Prediction batch contains the predicted labels for each record in the input data\n",
    "- Predicted data is then saved in some new location\n",
    "\n",
    "\n",
    "\n",
    "### Real time mode. \n",
    "\n",
    "In this scenario, the machine learning pipeline is attached to an on-demand/streaming scenario, where the model is essentially a transformation or augmentation of the input data, such as for example image labelling and object detection in real time applications.\n",
    "\n",
    "\n",
    "We have a web service that wraps our code\n",
    "The web service exposes Rest endpoints for getting predictions\n",
    "Consumer application makes web service call and sends input data in Json format\n",
    "Input data contains all the feature required for prediction. It typically has only one record instead of a batch\n",
    "Code loads the model from the saved location\n",
    "Code gets input data when the web service endpoint is called\n",
    "Code runs model prediction over the Input data and produces Prediction data\n",
    "Prediction data is return back to the consumer application\n",
    "Consumer application can decide how to use the prediction data for a better user experience\n",
    "\n",
    "\n",
    "\n",
    "### Predictions via SQL\n",
    "\n",
    "This is a new trend that has caught off in the industry recently. This approach exposes the ML model as a SQL function which is applied as a transformation to the input data.\n",
    "\n",
    "This approach treats new input data as tables and allows ad-hoc analysis on the data by running our ML model as a function. The output is also viewed as table and can be saved for future if required.\n",
    "\n",
    "\n",
    "- ML model is wrapped in a SQL UDF\n",
    "- There is an SQL execution engine (like Spark or Google Big Table) that understands the UDF\n",
    "- SQL execution engine loads the code in a UDF\n",
    "- The user issues a SQL query to the execution engine, selecting the feature table in the SQL query\n",
    "- The execution engine runs the input features through the UDF to compute prediction\n",
    "- The prediction data is returned to the user\n",
    "- User might save the predicted data as a new table\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
